{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg25MkDgcADy"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/10-gen-ai/Week_10_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLbG_J_EdcyH",
        "outputId": "286f13a8-316f-4ffc-8ba5-556368e6d57c"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install wikipedia\n",
        "!pip install chromadb\n",
        "!pip install tiktoken\n",
        "!pip install pypdf\n",
        "!pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xeaa6cWTeWOI",
        "outputId": "65e0b94c-7883-4bc2-ae94-45a82a64f670"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1NCfM18ZRl8L"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wikipedia\n",
        "import urllib.request\n",
        "import bs4 as bs\n",
        "import os\n",
        "\n",
        "#import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# import Langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.chains import create_tagging_chain, create_tagging_chain_pydantic\n",
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5WgfVqRRl8N"
      },
      "source": [
        "# Generative AI\n",
        "\n",
        "<img src='https://images.unsplash.com/photo-1686191568035-db49125b65c6?auto=format&fit=crop&q=80&w=2940&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D' width=\"450\">\n",
        "\n",
        "Credit: [Mojahid Mottakin](https://unsplash.com/@iammottakin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onO46LysT2dh"
      },
      "source": [
        "## Content\n",
        "\n",
        "The goal of this walkthrough is to provide you with insights on [Generative AI](https://en.wikipedia.org/wiki/Generative_artificial_intelligence). Generative AI refers to artificial intelligence systems that can create new and original content, such as text, images, or music, without direct human input.\n",
        "\n",
        "We will first see some applications that we can have with GenAI and the text. We will also see that using GenAI can be costly and we will finally try to fine tune a model.\n",
        "\n",
        "- [First steps with LangChain and OpenAI](#First-steps)\n",
        "    - [Claculate the cost](#Cost)\n",
        "- [Text Summarization](#Text-Summarization)\n",
        "- [Sentiment analysis](#Sentiment-analysis)\n",
        "- [Text embedding](#Text-embedding)\n",
        "- [Fine-tuning](#Fine-tuning)\n",
        "- [Your turn!](#exercices)\n",
        "    - [Prompt](#prompting)\n",
        "    - [Summarizing](#Summarizing)\n",
        "    - [Sentiment analysis](#Sentiment-analysis-ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv1T75j9YpS4"
      },
      "source": [
        "## First steps with LangChain and OpenAI\n",
        "During this exercise session, we will be using the [OpenAI](https://platform.openai.com/docs/api-reference) API with different libraries.\n",
        "First we will need to setup the apikey and some of the librairies.\n",
        "\n",
        "Due to recent changes in the openAI library, there are incompatibilities with the Langchain library. We will therfore use a previous version by using this command:\n",
        "\n",
        "```\n",
        "!pip install openai==0.28.1\n",
        "```\n",
        "We will also use the [Langchain](https://python.langchain.com/docs/get_started/introduction) library for the prompting and the text applications.\n",
        "\n",
        "First, we will define the API key. You can find how to create an API key with your account in this document : [Guide-API-Key-OpenAi](https://github.com/michalis0/DataScience_and_MachineLearning/blob/master/10-gen-ai/Guide-API-Key-OpenAI.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNtXBIJGbB1Q",
        "outputId": "24e38e64-a8b8-4fdd-8ebf-c62fe65a73a7"
      },
      "outputs": [],
      "source": [
        "# define the openAI API key\n",
        "os.environ['OPENAI_API_KEY'] = 'YOUR OPENAI API KEY'\n",
        "print(os.getenv('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_A3akTUhHCH"
      },
      "source": [
        "To try the key, let's use ChatOpenAI like we would use ChatGPT. To do so, we will need to import the ChatOpenAI model:\n",
        "\n",
        "\n",
        "```\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aFDzxmgYsdD",
        "outputId": "ba4df36f-ed75-4224-ac15-5b19e36227b2"
      },
      "outputs": [],
      "source": [
        "# define the chat and try it with a simple message\n",
        "chat_model  = ChatOpenAI()\n",
        "llm = OpenAI()\n",
        "\n",
        "# print(llm.predict(\"Hi! How are you?\")) -> find another one\n",
        "print(chat_model.predict(\"Hi! How are you?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmORZnjrl15m"
      },
      "source": [
        "You can also use it differently as it is shown in the two following examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTeJiGm6iNtl",
        "outputId": "4b776444-ef8f-4015-b1f7-8f65ffeeedf5"
      },
      "outputs": [],
      "source": [
        "text = \"What would be a good startup name for startup from a student who just graduated from a degree in Information Systems and Digital Innovation?\"\n",
        "\n",
        "#print(llm.predict(text))\n",
        "print(chat_model.predict(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXde7hOAiaVw",
        "outputId": "e2f7f03e-0ca4-404d-9125-c1a8dc2cf563"
      },
      "outputs": [],
      "source": [
        "text = \"What would be a good startup name for startup from a student who just graduated from a degree in Information Systems and Digital Innovation?\"\n",
        "messages = [HumanMessage(content=text)]\n",
        "\n",
        "#print(llm.predict_messages(messages).content)\n",
        "print(chat_model.predict_messages(messages).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YqxMiZcmOxL"
      },
      "source": [
        "With Langchain, you can directly pass specifications for a prompt by using the ```PromptTemplate``` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8bxWPF9mkoyI",
        "outputId": "d02c9e1a-3923-4bac-b938-508e14adb498"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate.from_template(\"What is a good name for a startup that works in {field}?\")\n",
        "prompt.format(field=\"Information Systems and Digital Innovation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti0RqVq8_Fzc"
      },
      "source": [
        "### Calculate the cost\n",
        "Everything has a cost and the chatGPT API is not free. It depends on how many tokens we are giving to the API. You can have the pricing [here](https://openai.com/pricing).\n",
        "\n",
        "To give you an idea of the costs, calculate the price of using the GPT-4 model with a document that contains 86'000 tokens and you want a summary of 1'000 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPrLNLh69NNw",
        "outputId": "a0d42bbd-d2b8-4742-8a13-d7f82aadbb22"
      },
      "outputs": [],
      "source": [
        "# Your turn!\n",
        "tokens_input = 86000\n",
        "tokens_output = 1000\n",
        "cost_per_token_input = 0.00003\n",
        "cost_per_token_output = 0.00006\n",
        "total_cost = tokens_input*cost_per_token_input + tokens_output*cost_per_token_output\n",
        "print(f'The total cost is equal to {total_cost}$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pI-k5OMvn95"
      },
      "source": [
        "## Text Summarization\n",
        "GenerativeAI can be very useful to summarize data. We will continue to use [Langchain](https://python.langchain.com/docs/use_cases/summarization) for our implementation. We will try to summarize the Wikipedia page of [*Information System*](https://en.wikipedia.org/wiki/Information_system). We will first load the text, the model [gpt-3.5-turbo-16k](https://platform.openai.com/docs/models/gpt-3-5), which allow us to have 16k tokens, and the chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XyaUfkbNvpvp"
      },
      "outputs": [],
      "source": [
        "# load the text\n",
        "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Information_system\")\n",
        "docs = loader.load()\n",
        "\n",
        "# load the model and the chain\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl9w98ej_Nhj"
      },
      "source": [
        "### Calculate the cost\n",
        "Using openAI API is not free. You can calculate the cost of your model like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "yBCYdOpk_gIT",
        "outputId": "161f88ea-054d-4f62-b693-b0090b69ec9d"
      },
      "outputs": [],
      "source": [
        "#get the costs\n",
        "with get_openai_callback() as cb:\n",
        "  result = chain.run(docs)\n",
        "  print(cb)\n",
        "  display(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsQoyYkH1lwM"
      },
      "source": [
        "## Sentiment analysis\n",
        "\n",
        "By using Langchain, we can also classify the text. You can give different categories to the model and it will try to classify the sentence. For example, we will do a sentiment analysis and we will try to recognize in which language it is written.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DI6x1BhRJPHb"
      },
      "outputs": [],
      "source": [
        "# Schema\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"sentiment\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\":[\"positive\", \"neutral\", \"negative\"],\n",
        "        },\n",
        "        \"language\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"spanish\", \"english\", \"french\", \"german\", \"italian\"],\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"sentiment\", \"language\"],\n",
        "}\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\") \n",
        "chain = create_tagging_chain(schema, llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlb1g9BWO5KS",
        "outputId": "10d5fa76-4aa5-4f61-ed84-640992691b27"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "input = \"I love dogs!\"\n",
        "\n",
        "chain.run(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_SUKEgOSnOD"
      },
      "source": [
        "## Text embedding\n",
        "LangChain is also useful for text embedding. You can see the different models [here](https://python.langchain.com/docs/integrations/text_embedding/). You can have a look at the natural language processing class if you don't remeber why text embedding is important.\n",
        "\n",
        "You can either do the text embedding for a list of texts or a single piece of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3AwioZxzZz4W"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55i2zcA2YW3s",
        "outputId": "49878615-a225-46cd-fd59-d688c204af3e"
      },
      "outputs": [],
      "source": [
        "#embeding a single query\n",
        "text = \"This is a test document.\"\n",
        "query_result = embeddings.embed_query(text)\n",
        "query_result[:5]\n",
        "len(query_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w89IGJ2IZ2eo",
        "outputId": "174620cf-0996-4008-d607-97684c8182d5"
      },
      "outputs": [],
      "source": [
        "#embeding a list of texts\n",
        "embedds = embeddings.embed_documents(\n",
        "    [\n",
        "        \"Hi there!\",\n",
        "        \"Oh, hello!\",\n",
        "        \"What's your name?\",\n",
        "        \"My friends call me World\",\n",
        "        \"Hello World!\"\n",
        "    ]\n",
        ")\n",
        "len(embedds), len(embedds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ-TviJxmMsS"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Fine-tuning refers to the process of taking a pre-trained model (like chatGPT) and further training it on a specific task or dataset to improve its performance on that particular task. Instead of training a model from scratch, which can be computationally expensive and time-consuming, fine-tuning leverages the knowledge and features learned by a model on a large and diverse dataset.\n",
        "\n",
        "You can have a more detailed article [here](https://medium.com/@dataoilst.info/fine-tuning-langchain-llm-applications-a-technical-perspective-part-1-4b4c552ab557).\n",
        "\n",
        "For now, we will use the previous text and try to ask the model a question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F82FIQoZatwK",
        "outputId": "69095d99-a995-4a0e-d20f-23427f21f08a"
      },
      "outputs": [],
      "source": [
        "#processing the document\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZimOCk4a9dD",
        "outputId": "1ce62203-b84a-457b-a770-f89fd9107b08"
      },
      "outputs": [],
      "source": [
        "#asking the question\n",
        "retrieved_docs = retriever.invoke(\n",
        "    \"What is an information system?\"\n",
        ")\n",
        "print(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU4dqJmMeRYO"
      },
      "source": [
        "We can see that the model is simply giving us the parts where information system is mentionned. Therfore, the model is not very useful. You can however go a bit further if you wish to integrate it better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9owDlwBaLXp0"
      },
      "source": [
        "## Your turn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDbF60XZevym"
      },
      "source": [
        "### Prompt\n",
        "Create a prompt to ask chatGPT some project ideas for a Data Science and Machine Learning class. Try to apply some rules of [prompt engineering](https://www.datacamp.com/tutorial/a-beginners-guide-to-chatgpt-prompt-engineering)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFZ5t75WfMXr",
        "outputId": "be88d514-d254-4639-b67c-cc21fa6e400b"
      },
      "outputs": [],
      "source": [
        "# Your code\n",
        "chat_model  = ChatOpenAI()\n",
        "\n",
        "text = \"As a student in a class of Data Science and Machine learning that is part of a Master in Information Systems and Digital Innovation, list 10 project innovative ideas to do in Python\"\n",
        "\n",
        "print(chat_model.predict(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAG2jjy_wGhh"
      },
      "source": [
        "### Summarizing\n",
        "\n",
        "You will now try to summarize one of your lesson and print how much it costed by using the model ```gpt-3.5-turbo-16k-0613```. You might want to look at the package ```PyPDFLoader``` in order to load your pdf.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VAQQxguTyI9i"
      },
      "outputs": [],
      "source": [
        "# Your code\n",
        "pdf_loader = PyPDFLoader(\"./data/IoT.pdf\")\n",
        "pages = pdf_loader.load_and_split()\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "uu_5TJS2zFxC",
        "outputId": "06482d62-233e-49d3-8787-07a4f85d24a7"
      },
      "outputs": [],
      "source": [
        "with get_openai_callback() as cb:\n",
        "  result = chain.run(pages)\n",
        "  print(cb)\n",
        "  display(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPPhytTFRQNz"
      },
      "source": [
        "### Sentiment analysis\n",
        "Try to classify this sentence: ```J'aime l'intelligence artificielle.```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "OpKnsJ1BRhR7",
        "outputId": "7ad079fa-41cb-4476-8033-f3e07e0a5575"
      },
      "outputs": [],
      "source": [
        "# Your code\n",
        "# Schema\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"sentiment\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\":[\"positive\", \"neutral\", \"negative\"],\n",
        "        },\n",
        "        \"language\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"spanish\", \"english\", \"french\", \"german\", \"italian\"],\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"sentiment\", \"language\"],\n",
        "}\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "chain = create_tagging_chain(schema, llm)\n",
        "input = \"J'aime l'intelligence artificielle.\"\n",
        "display(chain.run(input))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
