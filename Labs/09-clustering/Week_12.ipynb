{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Labs/09-clustering/Week_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw22Uv7KoBZb"
      },
      "source": [
        "# Data Science and Machine Learning - Week 12\n",
        "# Clustering\n",
        "\n",
        "This is an unsupervised learning algorithm (i.e., we have unlabelled data). The objectives of clustering are:\n",
        "* to organise the data into classes so that there is a high degree of intra-class similarity and a low degree of inter-class similarity\n",
        "* to find the class labels and the number of classes\n",
        "directly from the data (in contrast to\n",
        "classification where the algortihm learned used labelled data).\n",
        "* to find natural groupings between objects.\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:4800/format:webp/0*ZxLMBwq9rmW9ZFuZ.jpg' width=\"800\">\n",
        "\n",
        "Source: [The difference between supervised and unsupervised learning](https://twitter.com/athena_schools/status/1063013435779223553), illustrated by [@Ciaraioch](https://twitter.com/Ciaraioch) \n",
        "\n",
        "\n",
        "There are two types of clustering:\n",
        "* Partitioning algorithms (for example, k-means)\n",
        "* Hierarchical clustering\n",
        "\n",
        "In this lab session we implement and compare them.\n",
        "\n",
        "### **Table of Contents**\n",
        "\n",
        "#### **1. Basic Examples**\n",
        "- **1.1**: K-means in 1 Dimension  \n",
        "- **1.2**: Hierarchical Clustering  \n",
        "\n",
        "#### **2. Runtime Complexity Comparison Exercise**  \n",
        "- **2.1**: Generate 3 Clusters for Different Numbers of Points  \n",
        "- **2.2**: Compare Runtime of K-means and Hierarchical Clustering  \n",
        "- **2.3**: Plot Execution Time vs. Number of Points  \n",
        "\n",
        "#### **3. Customer Segmentation**  \n",
        "- **3.1**: Load, Prepare, and Explore Data  \n",
        "- **3.2**: K-means Algorithms  \n",
        "- **3.3**: Hierarchical Clustering  \n",
        "\n",
        "#### **4. Clustering vs. Classification**  \n",
        "- **4.1**: Load Data  \n",
        "- **4.2**: Algorithms Comparison: K-means vs. Logistic Regression  \n",
        "\n",
        "#### **5. Feedbacks from the Previous Year**  \n",
        "- **5.1**: Can You Spot Clusters in the Data?  \n",
        "- **5.2**: What Linkages Do the Clusters Have?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQP1xZ8p1i_7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import cluster\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.datasets import make_blobs\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "# Customize plots\n",
        "%matplotlib inline\n",
        "try:\n",
        "    plt.style.use(\"seaborn-deep\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# What other styles are available\n",
        "#plt.style.available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjB9fn6A8TB8"
      },
      "source": [
        "## 1. Basic Examples\n",
        "As always, we first go through basic examples to visualize and understand what happens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnBprgbU9UYI"
      },
      "source": [
        "### 1.1 K-means in 1 dimension\n",
        "\n",
        "We first look at a sample data set of students grades.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "9NUKDjkz1oF3",
        "outputId": "86197ca5-a31b-4f02-cc96-35551b49ed5f"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "grades = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/refs/heads/master/Labs/09-clustering/data/student_grades.csv\")\n",
        "print(f\"There are {len(grades)} observations.\")\n",
        "grades.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "uK1LuZtP4Uz7",
        "outputId": "bbc6cac4-7d05-43fd-8a88-fecc7a744f94"
      },
      "outputs": [],
      "source": [
        "sns.histplot(grades['Course total (Real)'], kde=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbgB-QD8XV25"
      },
      "source": [
        "There are observations are between 72 and 92 approximately. One way to visualise the clusters is by plotting them in a one dimension scatterplot (one dimension because we only ave one feature=the grades)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "J6QxAAAN1ub1",
        "outputId": "8426310b-b7bc-431b-e56f-eef32b9ed8b6"
      },
      "outputs": [],
      "source": [
        "# Plot the grades\n",
        "plt.figure(figsize=(20,2))\n",
        "plt.scatter(grades, np.zeros_like(grades), marker='x')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQVEX7Gb0JhU"
      },
      "source": [
        "We clearly see 4 clusters in our sample dataset:\n",
        "* around 72.5 are the lowest grades\n",
        "* around 80 are the normal grades\n",
        "* around 87.5 are the good grades\n",
        "* around 92.5 are the highest grades\n",
        "\n",
        "We now fit a K-Means algortim with different k's (i.e. number of clusters). The documentation file can be found [at this link](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "q3zKL6oe12yg",
        "outputId": "8d179c38-710a-42e4-e5f2-e77ddc32c7f7"
      },
      "outputs": [],
      "source": [
        "# Fit k-means algorithm for different k\n",
        "fig, ax = plt.subplots(4, 1, figsize=(18, 10))\n",
        "\n",
        "for i, k in enumerate([2, 3, 4, 5]):\n",
        "    # Create instance of class with n_init explicitly set\n",
        "    model = KMeans(n_clusters=k, n_init=10)\n",
        "    # Fit model\n",
        "    model.fit(grades)\n",
        "    # Get centers of clusters\n",
        "    centers = model.cluster_centers_\n",
        "    # Get predictions (we use only the grades)\n",
        "    pred = model.predict(grades)\n",
        "    # Plot the points, colored by associated cluster\n",
        "    ax[i].scatter(grades, np.zeros_like(grades), marker='x', c=pred, cmap='tab10')\n",
        "    # Plot the center of each cluster\n",
        "    ax[i].scatter(centers, np.zeros_like(centers), color='red')\n",
        "    ax[i].set_title('k-means algorithm with k = ' + str(k))\n",
        "\n",
        "plt.subplots_adjust(hspace=2)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PySrrVKyYkg7"
      },
      "source": [
        "With 2 clusters, it already works well. We could then label students as `good` and `not so good` for example.   \n",
        "However, at least visually, it seems it works best with 4 clusters.  \n",
        "In order to find out the optimal number of groups, we can plot the the objective/cost function of the algorithm vs the values of `k`. The point where we see an __\"elbow\"__, i.e, the cost value almost remains the same by further increasing the `k`, could be an appropriate choice for the optimal value of `k`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "S6WXdTnq5GfW",
        "outputId": "f1a423d7-2b95-423e-b5f6-f722766e5756"
      },
      "outputs": [],
      "source": [
        "# Initialize inertia list\n",
        "inertia = []\n",
        "\n",
        "# Define range of k values\n",
        "k_values = range(2, 20)\n",
        "\n",
        "# Fit k-means algorithm for each k\n",
        "for k in k_values:\n",
        "    # Set n_init explicitly to suppress the warning\n",
        "    model = KMeans(n_clusters=k, n_init=10)\n",
        "    # Fit the model\n",
        "    model.fit(grades)\n",
        "    # Append inertia\n",
        "    inertia.append(model.inertia_)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, inertia, marker='o', linestyle='solid')\n",
        "plt.title('Inertia for different k values')\n",
        "plt.ylabel('Inertia')\n",
        "plt.xlabel('k')\n",
        "plt.xticks(np.arange(min(k_values), max(k_values) + 1, step=1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FVR7WxnaMWe"
      },
      "source": [
        "\n",
        "In this case, 4 is the optimal number of clusters. The `inertia` is defined as the sum of squared distances of each sample to their closest cluster center. This is the cost function that the algorithm minimizes. From k=5 we see that the reduction in the cost function is much lower than for example for k=1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j3xiR1Y2Yy3"
      },
      "source": [
        "### 1.2 Hierarchical clustering\n",
        "In order to show how it works, we use a small set of 9 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "gdKFuVFO_UjI",
        "outputId": "bddbb7c9-8e9b-4fe4-f198-e1b6b325bc8d"
      },
      "outputs": [],
      "source": [
        "# Create Data\n",
        "data = {\n",
        "    \"p\":[\"p0\", \"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\", \"p8\"],\n",
        "    \"x1\":[0.84, 0.15, 0.87, 0.90, 0.93, 0.88, 0.87, 0.12, 0.91],\n",
        "    \"x2\":[0.91, 0.15, 0.47, 0.54, 0.84, 0.51, 0.86, 0.18, 0.73]}\n",
        "\n",
        "data = pd.DataFrame(data)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "676LpTjH_UhF",
        "outputId": "d6ae90ac-2f76-4945-bd8b-1d40cd0c1b85"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "data.plot.scatter(\"x1\", \"x2\", figsize=(8, 5))\n",
        "plt.title(\"Our dataset of 9 points in two dimensions\")\n",
        "for point in data.values:\n",
        "  plt.text(point[1]+0.01, point[2], point[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tZYsxv7a4hw"
      },
      "source": [
        "In this case, there are 2 or 3 clusters. We can further compute the euclidean distances between the points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "Sf5VmKxIbFbF",
        "outputId": "0b44c937-e610-4f54-9a38-2cb1057c21d6"
      },
      "outputs": [],
      "source": [
        "# Show distances\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(pd.DataFrame(pairwise_distances(data[[\"x1\", \"x2\"]], metric='euclidean')), annot=True, cmap='coolwarm_r', fmt='.2g')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8KfUGtdbaAs"
      },
      "source": [
        "For example `p0` is:\n",
        "* close to p6, p4, p8 (distance close to 0, here red)\n",
        "* a little further away from p3, p5, p2\n",
        "* far from p1, p7 (distance close to 1, here blue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjmpXYclMFqp"
      },
      "source": [
        "Now we plot our dendrograms using the [Scipy hierarchical clustering functions](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html).\n",
        "\n",
        "The first dendrogram uses as a metric the Euclidean distance between observations and the signle method linkage (considers the minimum distance between observations belonging to the clusters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "DCiwVmL02FBx",
        "outputId": "c7d244b8-44c6-443c-ac8e-d30b13ff9bca"
      },
      "outputs": [],
      "source": [
        "# Plot dendrogram, first with linkage = 'single'\n",
        "plt.figure(figsize=(10,4))\n",
        "dendrogram = sch.dendrogram(sch.linkage(data[[\"x1\", \"x2\"]], method = 'single', metric='euclidean'))\n",
        "plt.title('Dendrogram - single linkage')\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('Euclidean distances')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZaeYzyLcQFv"
      },
      "source": [
        "With `single linkage`, we can see that `p0` is first attached to `p6`, then both of them are attached to `p4`, and finally the these three points are attached to `p8`. This is the first cluster. The resulting number of clusters depends on where we cut the dendrogram, as shown below. With 2 clusters we have: {p1, p7} and {p2, p3, p5, p8, p4, p0, p6} and with 3 clusters we have: {p1, p7}, {p2, p3, p5} and {p8, p4, p0, p6}."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17_JiWkDND6F"
      },
      "source": [
        "For the second dendrogram we use the `complete` linkage which looks at the maximum distance between observations belonging to the clusters. This forces clusters to be more separated. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "MyqAwbnI2Vc_",
        "outputId": "2f0dec4b-ec9a-4456-8dc9-aa3929900f32"
      },
      "outputs": [],
      "source": [
        "# Plot dendrogram, now with linkage = 'complete'\n",
        "plt.figure(figsize=(10,4))\n",
        "dendrogram = sch.dendrogram(sch.linkage(data[[\"x1\", \"x2\"]], method = 'complete', metric='euclidean'))\n",
        "plt.title('Dendrogram - complete linkage')\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('Euclidean distances')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azg8N3zsfpU0"
      },
      "source": [
        "With `complete linkage`, p0 is first attached to p6 and p4 to p8. Then the two groups are combined to form a superset. This is because p4 and p8 are closer to each other than to the max distance to p0 or p6.\n",
        "\n",
        "For more documentation on the other linkage methods that you can use with the Scipy functions, you can see the documentation [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orKu7BsIP6ez"
      },
      "source": [
        "From sklearn we can use the [Agglomerative Clustering function](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html), where we can define the number of clusers, the metric to be used for distances and the linkage method as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "NFXk-Lal4IwD",
        "outputId": "08142213-db57-49cf-9c3b-7c1c8fa57e8f"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot with different number of clusters\n",
        "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for i, k in enumerate([2, 3, 4]):\n",
        "    # Update to use 'metric' instead of 'affinity'\n",
        "    clustering = AgglomerativeClustering(n_clusters=k, linkage='single', metric='euclidean')\n",
        "    assignment = clustering.fit_predict(data[[\"x1\", \"x2\"]])\n",
        "    ax[i].scatter(data[\"x1\"], data[\"x2\"], marker='x', c=assignment, cmap='brg')\n",
        "    ax[i].set_title(f'Hierarchical Clustering with k = {k}')\n",
        "    ax[i].set_xlabel(\"x1\")\n",
        "    ax[i].set_ylabel(\"x2\")\n",
        "\n",
        "plt.subplots_adjust(hspace=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsR4PC-F75w0"
      },
      "source": [
        "We see that k = 2 and k = 3 is fine. However, k = 4 might be overdoing it in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuPuIV_O08ws"
      },
      "source": [
        "## 2. Runtime complexity comparison exercise\n",
        "\n",
        "\n",
        "We want to compare the computation time needed between **Kmeans** and **Hierarchial clustering** for different numbers of points.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "wQAO7w4V1KVJ",
        "outputId": "2f50c73e-2509-4104-b987-a3cbcf47ac90"
      },
      "outputs": [],
      "source": [
        "# We create a function that generates 3 clusters\n",
        "def generate_three_clusters(num_points):\n",
        "  centers = [(-15, -15), (0, 0), (15, 15)]\n",
        "  cluster_std = [2, 3, 2]\n",
        "  X, y = make_blobs(n_samples=num_points, cluster_std=cluster_std, centers=centers, n_features=3, random_state=1)\n",
        "  return X, y\n",
        "\n",
        "# Example with 100 points\n",
        "X, y = generate_three_clusters(100)\n",
        "# Plot clusters\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"red\", s=10)\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"blue\", s=10)\n",
        "plt.scatter(X[y == 2, 0], X[y == 2, 1], color=\"green\", s=10)\n",
        "plt.title('Number of points: 100')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjccGsJD2zrx"
      },
      "source": [
        "### 2.1 Generating 3 Clusters for Different Numbers of Points\n",
        "\n",
        "We can use the previously defined clustering function to generate datasets with 3 clusters for various sample sizes. Specifically, we'll generate clusters for `n = 100`, `1000`, `2500`, `5000`, `7500`, `10000`, and `25000` points. Then, we store the resulting datasets in a list called `X_list`. This allows us to compare clustering performance across datasets of increasing size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQAGGEEU1Np2"
      },
      "outputs": [],
      "source": [
        "# Define list\n",
        "X_list = []\n",
        "# Define numbers of points\n",
        "num_points = [100, 1000, 2500, 5000, 7500, 10000, 25000]\n",
        "\n",
        "for n in num_points:\n",
        "  # Generate three clusters\n",
        "  X, y = generate_three_clusters(n)\n",
        "\n",
        "  # Append X to X_list\n",
        "  X_list.append(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJP8S1YJQ3zX",
        "outputId": "8dc41b2f-af7e-4552-93da-8ecf5bc75a53"
      },
      "outputs": [],
      "source": [
        "X_list[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QBRI8cg4YMz"
      },
      "source": [
        "### 2.2 For all samples of points, fit a k-means algorithm and a hierarchical clustering algorithm and record the completion time. \n",
        "\n",
        "We use the [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and [`AgglomerativeClustering`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) classes, both configured with 3 clusters. Then we record the time it takes to fit each algorithm using the `time` package, as shown in the example below. This will help compare the efficiency of the two clustering methods across datasets of different sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi8PiVGp4XrF",
        "outputId": "c13e0194-3697-4e67-ad19-a394e8e120bc"
      },
      "outputs": [],
      "source": [
        "# Example with the time package\n",
        "import time\n",
        "start = time.time()\n",
        "for i in range(10000000):\n",
        "  a = 1 + 1 # do something\n",
        "end = time.time()\n",
        "print('Start: ' + str(start))\n",
        "print('End: ' + str(end))\n",
        "print('Time: ' + str(end-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0umAoaOB1TJI"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "\n",
        "# Record time in lists\n",
        "k_means_time = []\n",
        "hc_time = []\n",
        "\n",
        "for X in X_list:\n",
        "    # Create instance of KMeans class (with 3 clusters) and set `n_init`\n",
        "    model = KMeans(n_clusters=3, n_init=10)\n",
        "\n",
        "    # Fit the model on X and record the time to fit\n",
        "    start = time.time()\n",
        "    model.fit(X)\n",
        "    end = time.time()\n",
        "\n",
        "    # Store the execution time in k_means_time\n",
        "    k_means_time.append(end - start)\n",
        "\n",
        "    # Create instance of AgglomerativeClustering class (with 3 clusters)\n",
        "    model = AgglomerativeClustering(n_clusters=3)\n",
        "\n",
        "    # Fit the model on X and record the time to fit\n",
        "    start = time.time()\n",
        "    model.fit(X)\n",
        "    end = time.time()\n",
        "\n",
        "    # Store the execution time in hc_time\n",
        "    hc_time.append(end - start)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNqVDKxS9PlG"
      },
      "source": [
        "### 2.3 Plot the execution time vs. number of points for k-means and hierarchical clustering\n",
        "We plot the execution time against the number of points for both k-means and hierarchical clustering. This visualization allows us to clearly observe how the runtime of each algorithm scales with the size of the dataset and compare their computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "n6i05g-D1UNu",
        "outputId": "69cbc881-2ffb-422e-9c8d-ae510ba9d286"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(num_points, k_means_time, label='k-means')\n",
        "plt.scatter(num_points, hc_time, label='hierarchical clustering')\n",
        "plt.ylabel('Execution time')\n",
        "plt.xlabel('Number of observations')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYW9a6qgEPnj"
      },
      "source": [
        "## 3. Customer segmentation\n",
        "We now implement customer segmentation with the two algorithms. The data set can be found in Kaggle [here](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcsUYpNfGLBI"
      },
      "source": [
        "### 3.1 Load, prepare, and explore data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "MbBoV9aywVOq",
        "outputId": "f9b7552c-0d7c-410b-b8bb-c89291b1fa26"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/refs/heads/master/Labs/09-clustering/data/Mall_Customers.csv', index_col=0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIcKdZghbkN"
      },
      "source": [
        "We have 200 customers with their gender, age, annual income, and spending score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgFn2utoEafN",
        "outputId": "5097a81b-4e8c-44eb-9021-91b2990a381c"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjaYIlDfGc7J",
        "outputId": "58b5bac6-432f-4c84-ae80-1c75e83ba18e"
      },
      "outputs": [],
      "source": [
        "# Convert Gender to numerical\n",
        "df.Gender = df.Gender.astype('category').cat.codes\n",
        "df.Gender.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-O3Mf3CTii5"
      },
      "source": [
        "In this case 0 corresponds to female customers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "f7IVdFIqGJVC",
        "outputId": "6789237e-0afa-468a-9d78-5b724619f6e7"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwvV0lEqAw1U",
        "outputId": "41ea4c2e-3739-4487-d68d-0d8e6d6ff599"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "nDYZXr2QAwxr",
        "outputId": "6e474f09-9bd3-4684-9b0c-8faa56ec8244"
      },
      "outputs": [],
      "source": [
        "# Univariate Analysis\n",
        "fig, ax = plt.subplots(2, 2, figsize=(10, 6))\n",
        "i = 0\n",
        "j = 0\n",
        "for var in df:\n",
        "    if var == \"Gender\":\n",
        "        # Use color instead of palette, or assign hue\n",
        "        sns.countplot(x=df[var], ax=ax[i, j], color=\"teal\")  # Replace 'palette' with 'color'\n",
        "    else:\n",
        "        sns.histplot(df[var], ax=ax[i, j], color=(0.4, 0.3, 0.7))\n",
        "    i += 1\n",
        "    if i == 2:\n",
        "        i = 0\n",
        "        j += 1\n",
        "\n",
        "plt.tight_layout()  # Improve layout for better visualization\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "cnm_nwREAwt0",
        "outputId": "5a154496-714e-4d7a-fbd0-5f688aeaa3dc"
      },
      "outputs": [],
      "source": [
        "# Bivariate Analysis\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.pairplot(df, hue='Gender', palette=\"coolwarm_r\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkye7zJWh0jt"
      },
      "source": [
        "Using `Annual Income` and `Spending Score`, we can observe 5 clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blRRuPmcIkWV"
      },
      "source": [
        "### 3.2 k-means algorithm\n",
        "Again, we first want to find out the good number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setautyAIcV_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to plot inertia\n",
        "def k_means_inertia_plot(variables):\n",
        "    # Create new dataframe\n",
        "    new_df = df[variables]\n",
        "\n",
        "    # Fit k-means algorithm for different k\n",
        "    inertia = []\n",
        "    k_values = range(2, 20)\n",
        "    for k in k_values:\n",
        "        model = KMeans(n_clusters=k, n_init=10)  # Set n_init explicitly\n",
        "        model.fit(new_df)\n",
        "        inertia.append(model.inertia_)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_values, inertia, marker='o', linestyle='solid')\n",
        "    plt.title('Inertia for different k values')\n",
        "    plt.ylabel('Inertia')\n",
        "    plt.xlabel('k')\n",
        "    plt.xticks(np.arange(min(k_values), max(k_values) + 1, step=1))\n",
        "    plt.show()\n",
        "\n",
        "# Use function - two variables\n",
        "k_means_inertia_plot([\"Annual Income (k$)\", \"Spending Score (1-100)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W993tofPZuUI"
      },
      "source": [
        "This confirms what we saw in the EDA. Using these two variables, 5 clusters are optimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "dZC881HnLpvs",
        "outputId": "a0572c41-ab66-4fd7-c6a9-92efd81f862e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use function - three variables\n",
        "k_means_inertia_plot([\"Annual Income (k$)\", \"Spending Score (1-100)\", \"Age\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAsaX63aZwIM"
      },
      "source": [
        "With three variables, this is less clear. Maybe 6 clusters would be great."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "vRNLUrrhLpqX",
        "outputId": "5dbacddc-1c87-4c18-deda-d7ad53a61dc3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use function - four variables\n",
        "k_means_inertia_plot([\"Annual Income (k$)\", \"Spending Score (1-100)\", \"Age\", \"Gender\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa6XBN2HZzDB"
      },
      "source": [
        "Again, not clear with all four variables. Maybe 6 clusters would be good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "wnE6QtftGEaR",
        "outputId": "b33db963-96fd-4aee-cb62-fa205765f3b6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot clusters for k = 2,3,4,5,6 and two variables\n",
        "new_df = df[['Annual Income (k$)', 'Spending Score (1-100)']]\n",
        "\n",
        "fig, ax = plt.subplots(1,5, figsize=(23,4))\n",
        "for k in [2, 3, 4, 5, 6]:\n",
        "  model = KMeans(n_clusters=k, n_init=10) \n",
        "  model.fit(new_df)\n",
        "  centers = model.cluster_centers_\n",
        "  pred = model.predict(new_df)\n",
        "  ax[k-2].scatter(new_df['Annual Income (k$)'], new_df['Spending Score (1-100)'], marker='x', c=pred)\n",
        "  for c in range(k):\n",
        "    ax[k-2].scatter(centers[c][0], centers[c][1], color='red', marker=',')\n",
        "    ax[k-2].set_title('k-means algorithm with k = ' + str(k))\n",
        "    ax[k-2].set_xlabel(\"Annual Income (k$)\")\n",
        "    ax[k-2].set_ylabel(\"Spending Score (1-100)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-HD7FGwZAkt"
      },
      "source": [
        "As already mentioned, the optimal number of cluster is 5 in this case. Once clustering is done, we may then use the result to label (i.e. classifiy) the customers, for example:\n",
        "* Customers who earn little and spend little (the `reasonable poor`)\n",
        "* Customers who earn little and spend a lot (the `senseless poor`)\n",
        "* Customers who earn average and spend average (the `middle-income`)\n",
        "* Customers who earn a lot and spend a lot (the `rich`). The marketing effort should focus mainly on them.\n",
        "* Customers who earn a lot and spend little (the `stingy`).\n",
        "\n",
        "With three variables and 6 clusters: [link](https://www.kaggle.com/kushal1996/customer-segmentation-k-means-analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVwswjoLI-nI",
        "outputId": "3aaa02af-298a-4b96-8997-65fe864083ab"
      },
      "outputs": [],
      "source": [
        "pd.Series(pred).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZGL9MvNKmqc",
        "outputId": "0174bcf9-20f6-49be-9223-1c6960cdb99f"
      },
      "outputs": [],
      "source": [
        "centers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PhcxeNOLka8"
      },
      "source": [
        "We can see that the first cluster center corresponds to the middle income group of 80 consumers, the biggest cluster in our example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4fN-neQU-jM"
      },
      "source": [
        "### 3.3 Hierarchical clustering\n",
        "We now do the same with hierarchical clustering. This yields similar results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "w7_I7zFfjmMz",
        "outputId": "53e427cc-020c-482a-df41-b60fb3b63627"
      },
      "outputs": [],
      "source": [
        "# Plot with different k and two variables\n",
        "fig, ax = plt.subplots(1, 5, figsize=(23, 4))\n",
        "for i, k in enumerate([2, 3, 4, 5, 6]):\n",
        "    # Replace affinity with metric\n",
        "    clustering = AgglomerativeClustering(n_clusters=k, linkage='complete', metric='euclidean')\n",
        "    assignment = clustering.fit_predict(new_df)\n",
        "    ax[i].scatter(new_df['Annual Income (k$)'], new_df['Spending Score (1-100)'], marker='x', c=assignment, cmap='viridis')\n",
        "    ax[i].set_title(f'Hierarchical Clustering with k = {k}')\n",
        "    ax[i].set_xlabel(\"Annual Income (k$)\")\n",
        "    ax[i].set_ylabel(\"Spending Score (1-100)\")\n",
        "\n",
        "plt.tight_layout()  # Adjust layout for better spacing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Af2-marEGzMV",
        "outputId": "54768bf7-9f86-4220-865b-32f07569b3b3"
      },
      "outputs": [],
      "source": [
        "# Plot dendrogram - complete\n",
        "plt.figure(figsize=(12,4))\n",
        "dendrogram = sch.dendrogram(sch.linkage(new_df, method = 'complete'))\n",
        "plt.title('Dendrogram - complete linkage')\n",
        "plt.xlabel('Customer')\n",
        "plt.ylabel('Euclidean distances')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "b_ODqCCuGPwH",
        "outputId": "86271257-f1cb-4f61-a081-871aec4150d0"
      },
      "outputs": [],
      "source": [
        "# Plot dendrogram - single\n",
        "plt.figure(figsize=(12,4))\n",
        "dendrogram = sch.dendrogram(sch.linkage(new_df, method = 'single'))\n",
        "plt.title('Dendrogram - single linkage')\n",
        "plt.xlabel('Customer')\n",
        "plt.ylabel('Euclidean distances')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IWskeLWF7u7"
      },
      "source": [
        "It does not work with single linkage. With complete linkage, we can see the 5 clusters on the dendrogram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1KnnOL8-dLb"
      },
      "source": [
        "## 4. Clustering vs. classification\n",
        "We compare classification and clustering algortihms, using a labelled data set of emails."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAer-XqDJK-S"
      },
      "source": [
        "### 4.1 Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "iFU81S6hJKnb",
        "outputId": "e6f5dd5f-3e11-497a-d622-0dd1de003872"
      },
      "outputs": [],
      "source": [
        "emails = pd.read_csv('https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/refs/heads/master/Labs/09-clustering/data/spambase.csv')\n",
        "emails.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-IsycCeM2Vz"
      },
      "source": [
        "We see the emails were already vectorised and we have 57 features and a `class` feature, which classifies email into SPAM or NOT SPAM (2 classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JruxcThGd9S",
        "outputId": "95c6a73a-a368-4b00-cbaa-92a842c443e7"
      },
      "outputs": [],
      "source": [
        "emails.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DNnEWJ0L4ly",
        "outputId": "bc90a367-7805-4cc8-c7e6-4319e9b2d582"
      },
      "outputs": [],
      "source": [
        "# Base rate simplified version\n",
        "print(emails[\"class\"].value_counts())\n",
        "len(emails[emails[\"class\"] == 0]) / len(emails)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S4FPIR5Jpxd"
      },
      "source": [
        "### 4.2 Algorithms comparison: k-means vs. logistic regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Ce3W1bLzl7",
        "outputId": "7b6ceb55-b798-43a4-fc94-59566488630f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Suppress tokenizers parallelism warning\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(emails.drop(\"class\", axis=1))\n",
        "y = emails[\"class\"]\n",
        "\n",
        "# k-means\n",
        "kmeans = KMeans(n_clusters=2, n_init=10)  # Explicitly set n_init\n",
        "\n",
        "# Logistic regression\n",
        "logit = LogisticRegression(max_iter=500)  # Increase max_iter\n",
        "\n",
        "algorithms = [kmeans, logit]\n",
        "\n",
        "for algo in algorithms:\n",
        "    # K-fold cross-validation\n",
        "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "    # Conduct k-fold cross-validation\n",
        "    cv_results = cross_val_score(algo,  # Algorithm\n",
        "                                  X_scaled,  # Scaled feature matrix\n",
        "                                  y,  # Target vector\n",
        "                                  cv=kf,  # Performance metric\n",
        "                                  scoring=\"accuracy\",  # Loss function\n",
        "                                  n_jobs=-1)  # Use all CPU cores\n",
        "\n",
        "    # Print mean\n",
        "    print(f\"The Mean Score is {cv_results.mean():.2f} for {algo}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(emails.drop(\"class\", axis=1))\n",
        "y = emails[\"class\"]\n",
        "\n",
        "# Dimensionality reduction for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Models\n",
        "kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
        "logit = LogisticRegression(max_iter=500, random_state=42)\n",
        "\n",
        "# Fit models\n",
        "kmeans.fit(X_scaled)\n",
        "logit.fit(X_scaled, y)\n",
        "\n",
        "# Predict labels\n",
        "kmeans_labels = kmeans.predict(X_scaled)\n",
        "logit_labels = logit.predict(X_scaled)\n",
        "\n",
        "# Create subplots\n",
        "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot true labels\n",
        "ax[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=30, marker='o', alpha=0.8)\n",
        "ax[0].set_title(\"True Labels\")\n",
        "\n",
        "# Plot KMeans predictions\n",
        "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='viridis', s=30, marker='o', alpha=0.8)\n",
        "ax[1].set_title(\"KMeans Clustering\")\n",
        "\n",
        "# Plot Logistic Regression predictions\n",
        "ax[2].scatter(X_pca[:, 0], X_pca[:, 1], c=logit_labels, cmap='viridis', s=30, marker='o', alpha=0.8)\n",
        "ax[2].set_title(\"Logistic Regression Classification\")\n",
        "\n",
        "# Add axis labels\n",
        "for a in ax:\n",
        "    a.set_xlabel(\"PCA Component 1\")\n",
        "    a.set_ylabel(\"PCA Component 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJMZtYRtP4Hn"
      },
      "source": [
        "**Conclusion:** In general, classification algorithms are more precise than clustering for classification problems. This means as soon as you have labelled data, you should use classification. When there are no label or if you want to label unlabelled data, you can use clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Feedbacks from the previous year\n",
        "You will find bellow the dataset from last years students for the mid semester\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree  # Added cut_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess feedback data\n",
        "feedbacks = pd.read_csv('https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/refs/heads/master/Labs/09-clustering/data/master_survey_mid_2023.csv', index_col=0).drop_duplicates()\n",
        "\n",
        "# Display a random sample\n",
        "print(feedbacks.sample(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Can you spot clusters in the data?\n",
        "\n",
        "In this section of the notebook, we first load and preprocess survey feedback data, focusing on textual recommendations for improvement. We then transform this text into numerical representations using two approaches: TF-IDF, which captures word frequency and importance, and SentenceTransformer embeddings, which encode the semantic meaning of the sentences. After converting the text into numerical data, we scale the features to normalize their range and prepare them for clustering.\n",
        "\n",
        "Next, we perform hierarchical clustering on the scaled data using different linkage methods and visualize the results with dendrograms:\n",
        "- **Ward linkage**: Minimizes the variance within clusters, producing compact clusters.\n",
        "- **Single linkage**: Uses the minimum distance between points of different clusters, leading to elongated, chain-like clusters.\n",
        "- **Complete linkage**: Considers the maximum distance between points of different clusters, resulting in tighter, spherical clusters.\n",
        "\n",
        "This allows us to understand the hierarchical structure of the clusters and identify patterns or themes within the feedback, providing insights into common recommendations or areas for improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert column to list for compatibility\n",
        "recommendations = feedbacks[\"What are your recommendations for things to improve?\"].dropna().tolist()\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
        "features1 = tfidf.fit_transform(recommendations)\n",
        "\n",
        "# Sentence embedding using SentenceTransformer\n",
        "model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
        "features2 = model.encode(recommendations)\n",
        "\n",
        "# Choose which features to use for clustering (TF-IDF or embeddings)\n",
        "# Uncomment one of the following:\n",
        "# features = features1.toarray()  # Use TF-IDF\n",
        "features = features2  # Use sentence embeddings\n",
        "\n",
        "# Scale the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Perform hierarchical clustering and plot dendrograms\n",
        "methods = ['ward', 'single', 'complete']\n",
        "for method in methods:\n",
        "    Z = linkage(scaled_features, method)\n",
        "    print(f\"Z shape ({method}):\", Z.shape)\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    plt.title(method.capitalize())\n",
        "    dendrogram(Z, orientation='left')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we focus on applying hierarchical clustering using the **Ward method**, which minimizes variance within clusters and produces compact, well-separated groups. Once the clustering is computed, we use the `cut_tree` function to segment the hierarchical tree into a specified number of clusters. Here, we define `n_clusters=12` to divide the data into 12 distinct groups.\n",
        "\n",
        "Next, we iterate through the clusters to display a few sample feedback responses from each group. This approach allows us to qualitatively evaluate the clustering results by examining representative recommendations in each cluster. By reviewing these samples, we can better understand the common themes or patterns within each cluster, which may highlight actionable insights or distinct areas for improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We will use the ward method from now on\n",
        "# The cut_tree function cuts the tree at a certain height\n",
        "# The n_clusters parameter is the number of clusters to find\n",
        "Z = linkage(scaled_features, 'ward') # Compute the hierarchical clustering\n",
        "cutree = cut_tree(Z, n_clusters=[12])\n",
        "\n",
        "i=1\n",
        "while feedbacks[cutree==[i]][\"\"\"What are your recommendations for things to improve?\"\"\"].to_list() != []:\n",
        "    display(feedbacks[cutree==[i]][\"\"\"What are your recommendations for things to improve?\"\"\"].to_list()[0:3])\n",
        "    i+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDtMJ-jEv_pm"
      },
      "source": [
        "## References\n",
        "* https://www.kaggle.com/kushal1996/customer-segmentation-k-means-analysis\n",
        "* https://www.kaggle.com/morecoding/hierarchical-clustering\n",
        "* https://www.geeksforgeeks.org/cutting-hierarchical-dendrogram-into-clusters-using-scipy-in-python/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
