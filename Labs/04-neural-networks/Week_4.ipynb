{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "gD1Vv4GS8TwY",
      "metadata": {
        "id": "gD1Vv4GS8TwY"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Labs/04-neural-networks/Week_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "id": "15dc4475-7f7d-4457-8762-bb031a0629b2",
      "metadata": {
        "id": "15dc4475-7f7d-4457-8762-bb031a0629b2"
      },
      "outputs": [],
      "source": [
        "# Import standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "# ML import\n",
        "from sklearn.model_selection import train_test_split # Splitting the data set\n",
        "from sklearn.preprocessing import MinMaxScaler       # Normalization\n",
        "from sklearn.preprocessing import LabelEncoder       # Encoder\n",
        "import torch                                   # PyTorch\n",
        "import torch.nn as nn                          # PyTorch building blocks\n",
        "from IPython.display import YouTubeVideo\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a844a8c0-078c-4946-8927-e570469c601c",
      "metadata": {
        "id": "a844a8c0-078c-4946-8927-e570469c601c"
      },
      "source": [
        "# Neural Nets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9984e553-36af-42c8-9522-dd090686c913",
      "metadata": {
        "id": "9984e553-36af-42c8-9522-dd090686c913"
      },
      "source": [
        "<img src='https://imgs.xkcd.com/comics/trained_a_neural_net.png' width=\"270\">\n",
        "\n",
        "Source: [xqcd 2173](https://xkcd.com/2173/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4392fab0-671b-45ea-b27a-61a797c0e090",
      "metadata": {
        "id": "4392fab0-671b-45ea-b27a-61a797c0e090"
      },
      "source": [
        "## Content\n",
        "\n",
        "The goal of this walkthrough is to provide you with insights on Neural Nets. After presenting the main concepts, you will be introduced to the techniques to implement your own neural network in Python, with PyTorch and using a pre-build Neural Net.\n",
        "\n",
        "This notebook is organized as follows:\n",
        "- [Background](#Background)\n",
        "    - [Neurons](#Neurons)\n",
        "        - [Biological Neuron](#Biological-Neuron)\n",
        "        - [Artificial neuron and perceptron](#Artificial-neuron-and-perceptron)\n",
        "            - [Activation function](#Activation-function)\n",
        "    - [Some Neural Nets](#Some-Neural-Nets)\n",
        "        - [Multilayer Perceptron (MLP)](#Multilayer-Perceptron-(MLP))\n",
        "        - [Convolutional Neural Nerwork (CNN)](#Convolutional-Neural-Nerwork-(CNN))\n",
        "        - [Recurrent Neural Network (RNN)](#Recurrent-Neural-Network-(RNN))\n",
        "- [How to build your own Neural Net?](#How-to-build-your-own-Neural-Net?)\n",
        "    - [Application: Predicting house prices](#Application:-Predicting-house-prices)\n",
        "        - [Data](#Data)\n",
        "            - [Preprocessing](#Preprocessing)\n",
        "            - [Creating training and test set](#Creating-training-and-test-set)\n",
        "            - [Normalizing-the-data](#Normalizing-the-data)\n",
        "        - [Building a Linear Regression model with PyTorch](#Building-a-Linear-Regression-model-with-PyTorch)\n",
        "            - [Create tensors](#Create-tensors)\n",
        "            - [Define and train a model with PyTorch](#Define-and-train-a-model-with-PyTorch)\n",
        "        - [Your turn](#Your-turn)\n",
        "- [Implement a pre-built Neural Net](#Implement-a-pre-built-Neural-Net)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7763984b-c9d6-43b0-ba06-fd749431aa32",
      "metadata": {
        "id": "7763984b-c9d6-43b0-ba06-fd749431aa32"
      },
      "source": [
        "# 1. Background\n",
        "\n",
        "[Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network) - simply called Neural Nets (NNs) - are computing systems inspired by the biological neural networks that constitute animal brains. They are used to approximate functions that are generally unknown.\n",
        "\n",
        "NNs are based on a collection of connected nodes, called **artificial neurons**. In short, an [artificial neuron](https://en.wikipedia.org/wiki/Artificial_neuron) is a mathematical function conceived as a model of biological neurons.\n",
        "\n",
        "These artificial neurons are aggregated into **layers**. Signals travel from the first layer (the *input layer*), to the last layer (the *output layer*), traversing one or several *hidden layers*, which perform different transformations on their inputs. The output of one layer is the input of the next one: this is called **forward propagation**. A neural network with multiple layers between the input and output layers is called [Deep Neural Network](https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks) (DNN).\n",
        "\n",
        "Below is an illustration of a simple NN. Each circle represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another:\n",
        "\n",
        "<center>\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/444px-Colored_neural_network.svg.png?20130228185515' width=\"270\"></center>\n",
        "\n",
        "Source: [Glosser.ca](https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg), Wikipedia, [Artificial Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c63fa865-07da-4109-b61b-cd4d155fb33d",
      "metadata": {
        "id": "c63fa865-07da-4109-b61b-cd4d155fb33d"
      },
      "source": [
        "Depending on the structure of the network, we differentiate between feedforward and recurrent NNs:\n",
        "- [Feedforward Neural Network](https://en.wikipedia.org/wiki/Feedforward_neural_network) (FFNN)  was the first and simplest type of artificial neural network devised. In FFNNs, connections between the nodes do not form a cycle. FFNN are trained by backward propagation ([**backpropagation**](https://en.wikipedia.org/wiki/Backpropagation)).\n",
        "- [Recurrent Neural Networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) are networks wherein connections between nodes can create a cycle, allowing output from some nodes to affect subsequent input to the same nodes. RNNs are trained by [**backpropagation through time**](https://en.wikipedia.org/wiki/Backpropagation_through_time) (BPTT).\n",
        "\n",
        "In the following, we will explore the concept of (artificial) neurons and present a few types of neural nets.\n",
        "\n",
        "For a nice visual introduction to Neural Nets and to backpropagation, you can view the series of videos by [3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) on the topic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a1828da-c74c-44b3-a0a3-352c02ff9079",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a1828da-c74c-44b3-a0a3-352c02ff9079",
        "outputId": "f21f7c9d-d299-46ca-f74c-c9fcacc01ee3"
      },
      "outputs": [],
      "source": [
        "YouTubeVideo(\"aircAruvnKk\", width=\"560\", height=\"315\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27b2f794-7e6b-4438-968d-87cf15802f91",
      "metadata": {
        "id": "27b2f794-7e6b-4438-968d-87cf15802f91"
      },
      "source": [
        "Further reading:\n",
        "- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), by Aurélien Géron\n",
        "- [Everything you need to know about Neural Networks and Backpropagation — Machine Learning Easy and Fun](https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a), by Gavril Ognjanovski\n",
        "- [Neural Network from scratch in Python](https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65), by Omar Aflak"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d74b223-5b65-4bb3-b6a9-4ec89d491649",
      "metadata": {
        "id": "5d74b223-5b65-4bb3-b6a9-4ec89d491649"
      },
      "source": [
        "## 1.1 Neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a1b7d97-3e8c-4ffc-bc2e-67116d23a889",
      "metadata": {
        "id": "7a1b7d97-3e8c-4ffc-bc2e-67116d23a889"
      },
      "source": [
        "### Biological Neuron\n",
        "\n",
        "The nervous system is composed of more than 100 billion cells known as [neurons](https://en.wikipedia.org/wiki/Neuron), which process and transmit the information received from our senses. Neurons are arranged together in our brain to form a network of nerves. These nerves pass electrical impulses from one neuron to the other.\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*vBIWWCFLLzZzGes1HcCJOw.jpeg' width=\"450\">\n",
        "\n",
        "Neurons are made up of three major parts:\n",
        "- the *dendrites* collect information from other cells and send this information to the soma;\n",
        "- the *soma* is the cell body containing the nucleus of the cell and keeping it alive;\n",
        "- the *axon* transmits information away from the cell body towards other neurons or to the muscles and glands;\n",
        "    - when the axon is stimulated by an electrical signal from the dendrites, the impulse is transmitted if the electrical signal is strong enough that it passes a certain level or threshold.\n",
        "    - the axon terminal, located at the end of the axon farthest from the soma, contains [*synapses*](https://en.wikipedia.org/wiki/Synapse), which are structure that permits the transmission of information to another cell. Synapses have the ability to strengthen or weaken over time in responses to increases or decreases in their activity: this is called [*synaptic plasticity*](https://en.wikipedia.org/wiki/Synaptic_plasticity).\n",
        "\n",
        "<center>\n",
        "<img src='https://opentextbc.ca/introductiontopsychology/wp-content/uploads/sites/9/2013/11/6a3f0732c22683476ea201ffc5e428ad.jpg' width=\"400\"></center>\n",
        "\n",
        "Source: Jennifer Walinga, [The Neuron Is the Building Block of the Nervous System](https://opentextbc.ca/introductiontopsychology/chapter/3-1-the-neuron-is-the-building-block-of-the-nervous-system/), Chapter 4.1 of *Introduction to Psychology*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11fe51c9-40ab-4ad4-a54f-19563870533d",
      "metadata": {
        "id": "11fe51c9-40ab-4ad4-a54f-19563870533d"
      },
      "source": [
        "### Artificial neuron and perceptron\n",
        "\n",
        "An [artificial neuron](https://en.wikipedia.org/wiki/Artificial_neuron) is inspired by its biological counterpart. It receives some inputs, transforms them, and transmits the output.  \n",
        "\n",
        "An artificial neuron typically: 1) performs a weighted sum of the inputs; 2) passes this sum through an [activation function](https://en.wikipedia.org/wiki/Activation_function). We'll discuss what is an activation function below. For now, let's formalize what is happening in an artificial neuron. Let:\n",
        "- $\\boldsymbol{x}=(x_1, ..., x_d)$ be some input signals\n",
        "    - the input signals can come from artificial neurons from the input layer (input data) or from artificial neurons from the hidden layers\n",
        "- $y_k$ be the output signal of neuron $k$\n",
        "\n",
        "The operation performed by an artificial neuron is:\n",
        "$$y_k = \\varphi(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) = \\varphi(w_{k1} x_{1} +  w_{k2} x_{2} + ... +  w_{kd} x_{d} + b_k) $$\n",
        "\n",
        "Where:\n",
        "- $\\boldsymbol{w}=(w_{k1}, ..., w_{kd})$ are the **weights**\n",
        "    -  the weights have the same role than the synapses. Their value determines the strength of the link between one artificial neuron and another. When we learn the weights, this link can strengthen or weaken, similarly than with synaptic plasticity\n",
        "- $b_k$ is the **bias**\n",
        "- $\\varphi$ is an **activation function**\n",
        "    - the activation function models what is happening in the axon of a biological neuron: the electrical impulses are only transmitted if they are strong enough to reach a given threshold\n",
        "\n",
        "<center>\n",
        "<img src='https://www.gabormelli.com/RKB/images/thumb/3/31/artificial-neuron-model.png/600px-artificial-neuron-model.png' width=\"450\"></center>\n",
        "\n",
        "Source: [Artificial Neuron](https://www.gabormelli.com/RKB/Artificial_Neuron), Gabor Melli's Research Knowledge Base\n",
        "\n",
        "Note that, oftentimes, we write $b_k=w_{k0}$ and $x_{0}=1$, so that the expression simplifies to $y_k = \\varphi(\\boldsymbol{w} \\cdot \\boldsymbol{x})$.\n",
        "\n",
        "The first artificial neuron was the [perceptron](https://en.wikipedia.org/wiki/Perceptron), invented in 1943 by Warren McCulloch and Walter Pitts. It was a binary classifier, i.e., the activation function was the unit step function:\n",
        "- $\\varphi(\\boldsymbol{x}) = 1$ if $\\boldsymbol{w} \\cdot \\boldsymbol{x} +b>0$\n",
        "- $\\varphi(\\boldsymbol{x}) = 0$ otherwise\n",
        "\n",
        "The perceptron, also called the single-layer perceptron, was the first and simplest (feedforward) neural network!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be768579-7a57-43d8-b26b-b6abc9efeb92",
      "metadata": {
        "id": "be768579-7a57-43d8-b26b-b6abc9efeb92"
      },
      "source": [
        "#### Activation function\n",
        "\n",
        "As mentioned above, the [activation function](https://en.wikipedia.org/wiki/Activation_function) represents what is happening in the axon of a biological neuron: the electrical impulses are only transmitted if they are strong enough to reach a given threshold. There are many possible activation functions:\n",
        "- unit step function ([Heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function)), as used in the perceptron;\n",
        "- sigmoid, e.g., the [logistic function](https://en.wikipedia.org/wiki/Logistic_function), previously seen for logistic regression\n",
        "    - $\\varphi(x)=\\frac{1}{1 + e^{-x}}$\n",
        "- Rectified linear unit ([ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))), one of the most popular because of its good performance and fast gradient computation\n",
        "    - $\\varphi(x) = \\max(0,x) $\n",
        "- Parametric rectified linear unit ([PReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Parametric_ReLU)), a variant of ReLU that avoids the *dead* neurons problem, i.e., neurons becoming inactive\n",
        "    - $\\varphi(x) = \\max(\\alpha x,x) $ with $0<\\alpha<1$\n",
        "    - when $\\alpha=0.01$, the function is called Leaky ReLU\n",
        "\n",
        "<center>\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ypsvQH7kvtI2BhzR2eT_Sw.png' width=\"400\"> </center>\n",
        "\n",
        "Source: Danqing Liu, [A Practical Guide to ReLU](https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7)\n",
        "\n",
        "There are numerous other activation functions. Explore them [here](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)!\n",
        "\n",
        "You can also play and visualize more esay how neural networs are build with this [Web app](http://playground.tensorflow.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5654a34-b6ea-4513-968d-a4af75e4efd1",
      "metadata": {
        "id": "f5654a34-b6ea-4513-968d-a4af75e4efd1",
        "tags": []
      },
      "source": [
        "## 1.2 Examples of Neural Nets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ca625a0-b1b7-4d5d-8660-ea33f7a22b1e",
      "metadata": {
        "id": "4ca625a0-b1b7-4d5d-8660-ea33f7a22b1e"
      },
      "source": [
        "### Multilayer Perceptron (MLP)\n",
        "\n",
        "A [Multilayer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) is a *fully-connected* feedforward neural network. Here is an illustration, with 3 hidden layers:\n",
        "\n",
        "<center>\n",
        "<img src='https://miro.medium.com/v2/resize:fit:569/0*03elfV9p5nZTnRKe.png' width=\"500\"> </center>\n",
        "\n",
        "Each circle represents one perceptron. Historically, the perceptrons used a sigmoid activation function, but nowadays, ReLU and its variants are more frequently used.\n",
        "\n",
        "Originally developed in the 1960s, these models had applications in diverse fields in the 1980s, such as speech recognition and machine translation software. Recently, interest in these networks was renewed due to the successes of deep learning.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab5a330-3cac-42a5-9079-c7b564993c0b",
      "metadata": {
        "id": "1ab5a330-3cac-42a5-9079-c7b564993c0b"
      },
      "source": [
        "### Convolutional Neural Network (CNN)\n",
        "\n",
        "A [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN) is a class of neural network most commonly applied in processing data that has a grid-like topology, such as an image. They have proven very effective in analyzing visual imagery, e.g., image recognition and classification.\n",
        "\n",
        "CNNs use a mathematical operation called [convolution](https://en.wikipedia.org/wiki/Convolution) in place of general matrix multiplication in at least one of their layers. This layer performs a dot product between two matrices, where one matrix is the set of learnable parameters otherwise known as a [kernel](https://en.wikipedia.org/wiki/Kernel_(image_processing)), and the other matrix is the restricted portion of the receptive field. In other words, a convolution is the process of adding each element of the image to its local neighbors, weighted by the kernel:\n",
        "\n",
        "<center>\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif' width=\"300\"> </center>\n",
        "\n",
        "Source: Michael Plotke, [Kernel (image processing)](https://en.wikipedia.org/wiki/Kernel_(image_processing))\n",
        "\n",
        "The kernel is a filter that applies a transformation to the original image, e.g., blurring, sharpening, embossing, edge detection, etc.\n",
        "\n",
        "For a more detailed introduction of CNN, you can read:\n",
        "- [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/), by Ujjwal Karn\n",
        "- [Convolutional Neural Networks, Explained](https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939), by Mayank Mishra"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d9ede07-61c4-42bc-85d5-191bdb70e9af",
      "metadata": {
        "id": "2d9ede07-61c4-42bc-85d5-191bdb70e9af"
      },
      "source": [
        "### Recurrent Neural Network (RNN)\n",
        "\n",
        "A [Recurrent Neural Networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) is a class of artificial neural networks where connections between nodes can create a cycle, allowing output from some nodes to affect subsequent input to the same nodes. RNNs are useful when dealing with sequential data like natural language and time series. They are mostly used in the fields of natural language processing and speech recognition.\n",
        "\n",
        "A RNN is very much alike feedforward neural nerworks, except it also has connections pointing backforward: at each time step $t$, a *recurrent neuron* receives the input $x_t$ as well as its own output from the previous step $y_{t-1}$:\n",
        "$$y_{t} = \\varphi(\\boldsymbol{w_t} \\cdot \\boldsymbol{x_t} + \\boldsymbol{w_y} \\cdot \\boldsymbol{y_{t-1}} + b)$$\n",
        "\n",
        "We can represent a recurrent neuron graphically:\n",
        "<center>\n",
        "<img src='https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1401.png' width=\"400\"> </center>\n",
        "\n",
        "\n",
        "More generally, a recurrent unit may store more information than the previous predictions. A part of a neural network that preserves some state across time steps is called a *memory cell*. Indeed, since the output at time $t$ is a function of the inputs from previous time steps, the recurrent neuron has a form of memory! The memory cell is a hidden layer that flows through time:\n",
        "\n",
        "<center>\n",
        "<img src='https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1403.png' width=\"400\"> </center>\n",
        "\n",
        "\n",
        "RNNs have the obvious advantage of taking into account historical information. However, computation can be slow and it might be difficult to access information from a long time ago (issue of *fading memory*). To solve this issue, various memory cell architectures with long-term memory have been developed to save information, including:  \n",
        "- The [Long Short-Term Memory](https://en.wikipedia.org/wiki/Long_short-term_memory) (LSTM) with the idea to store a short-term state $h_t$ and a long-term state $c_t$. LTSM rely on *gates*: when the gate is open, the output is multiplied by 1 (information preserved); when the gate is closed, the outputs is multiplied by 0 (information erased). Gates are a way to update and reset persistent information:\n",
        "    - the *forget gate* controls which parts of the long-term state should be erased,\n",
        "    - the *input gate* controls which parts of the output should be added to the long-term state,\n",
        "    - the *output gate* controls which parts should be output at this time step\n",
        "- The [Gated Recurrent Unit](https://en.wikipedia.org/wiki/Gated_recurrent_unit) (GRU), a simplified version of the LTSM.\n",
        "\n",
        "<center>\n",
        "<img src='https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1413.png' width=\"400\"> </center>\n",
        "\n",
        "For a more detailed introduction of RNN, you can read:\n",
        "- [Source behind all the figures in this section]: [Chapter 4. Recurrent Neural Networks](https://www.oreilly.com/library/view/neural-networks-and/9781492037354/ch04.html), *Neural networks and deep learning*, by Aurélien Géron\n",
        "- [Recurrent Neural Networks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks), by Afshine Amidi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce878d70-296b-4ce2-9a05-a5d0410d79e1",
      "metadata": {
        "id": "ce878d70-296b-4ce2-9a05-a5d0410d79e1"
      },
      "source": [
        "# 2. Implmentation\n",
        "\n",
        "There exists several libraries to implement Neural Nets in Python, the most popular being PyTorch and TensorFlow/Keras:\n",
        "- [PyTorch](https://pytorch.org/) is a deep learning framework based on [Torch](https://en.wikipedia.org/wiki/Torch_(machine_learning)). It was developed by Meta AI (Facebook) and open-sourced in 2017.\n",
        "- [TensorFlow](https://www.tensorflow.org/) is an end-to-end open-source platform for machine learning. It was developed by Google and released as open source in 2015. Its name comes from the basic data structure used, namely [tensors](https://en.wikipedia.org/wiki/Tensor).\n",
        "- [Keras](https://keras.io/) is a high-level neural networks library that is running on the top of TensorFlow, among others.\n",
        "\n",
        "For a comparison of TensorFlow, Keras, and Pytorch, you can refer to:\n",
        "- [PyTorch vs TensorFlow for Your Python Deep Learning Project](https://realpython.com/pytorch-vs-tensorflow/), by Ray Johns\n",
        "- [Keras vs Tensorflow vs Pytorch: Key Differences Among Deep Learning](https://www.simplilearn.com/keras-vs-tensorflow-vs-pytorch-article), by John Terra\n",
        "\n",
        "In this notebook, we are going to implement a Neural Net with PyTorch, displaying the simplicity, ease of use, and flexibility of this library. For a walkthrough on Neural Nets with TensorFlow/Keras, you can for instance refer to [Deep Learning with Python](https://github.com/fchollet/deep-learning-with-python-notebooks), by François Chollet (Chapter 7)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d33d277-2861-493d-8fad-63baec052f8e",
      "metadata": {
        "id": "1d33d277-2861-493d-8fad-63baec052f8e"
      },
      "source": [
        "## 2.1 Application: Predicting house prices\n",
        "\n",
        "In this application, we will implement a Linear Regression model with PyTorch to predict house prices using the [Ames Housing dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e439b086-55d0-442f-ab05-0e9399e15659",
      "metadata": {
        "id": "e439b086-55d0-442f-ab05-0e9399e15659"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "240c89a5-84f5-4229-a987-1dbeaa878682",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "240c89a5-84f5-4229-a987-1dbeaa878682",
        "outputId": "a181ca6f-3760-4d06-f85c-181564db581c"
      },
      "outputs": [],
      "source": [
        "raw_data = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/04-neural-networks/data/house_price.csv\")\n",
        "raw_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf6af7a8-2aad-45ec-af77-7b00a60cfbac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf6af7a8-2aad-45ec-af77-7b00a60cfbac",
        "outputId": "06241ca6-d9b5-4ee2-8d89-f9c16d8c8682"
      },
      "outputs": [],
      "source": [
        "raw_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfabbcb8-ef34-46d2-b156-6787824e3a85",
      "metadata": {
        "id": "bfabbcb8-ef34-46d2-b156-6787824e3a85"
      },
      "source": [
        "The dataset contains 81 columns. A description of the features is available in the file \"[house_price_data_description](https://github.com/michalis0/DataScience_and_MachineLearning/blob/master/Week_5/data/house_price_data_description.txt)\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72baf826-a96e-49d7-a6df-57cd349781f3",
      "metadata": {
        "id": "72baf826-a96e-49d7-a6df-57cd349781f3"
      },
      "source": [
        "##### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59cd5835-e7e1-451b-8e5e-64968834756a",
      "metadata": {
        "id": "59cd5835-e7e1-451b-8e5e-64968834756a"
      },
      "source": [
        "Let's first extract the features of interest. We will use the numeric columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dadf3888-4c1a-406f-a199-05145d017ff7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dadf3888-4c1a-406f-a199-05145d017ff7",
        "outputId": "e2c57621-2715-40fb-b490-997aa215844f"
      },
      "outputs": [],
      "source": [
        "# Data types\n",
        "raw_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc199722-8f76-4ad7-9b33-65f76a6bb2bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc199722-8f76-4ad7-9b33-65f76a6bb2bb",
        "outputId": "5c2ec611-64dd-4760-efda-1c279a94e06b"
      },
      "outputs": [],
      "source": [
        "# Display numeric features (integer and floats)\n",
        "numeric_columns = list(raw_data.columns[(raw_data.dtypes==np.int64) |\n",
        "                 (raw_data.dtypes==np.float64)])\n",
        "print(numeric_columns, \"\\n\", len(numeric_columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9354882-1469-42ee-8fd6-38301f89ed0f",
      "metadata": {
        "id": "f9354882-1469-42ee-8fd6-38301f89ed0f"
      },
      "source": [
        "`SalePrice` is the value we want to predict. We set it as the last column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "28dd7159-374a-47b0-ae56-5864dbac3cf8",
      "metadata": {
        "id": "28dd7159-374a-47b0-ae56-5864dbac3cf8"
      },
      "outputs": [],
      "source": [
        "# Output SalePrice as last column\n",
        "numeric_columns.remove('SalePrice')\n",
        "numeric_columns.append('SalePrice')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd2153a-be33-48b1-b191-31f95b4734aa",
      "metadata": {
        "id": "dfd2153a-be33-48b1-b191-31f95b4734aa"
      },
      "source": [
        "We also remove the `Id` column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "8e372a0d-4c5c-444d-bd1e-7fd8d411d84c",
      "metadata": {
        "id": "8e372a0d-4c5c-444d-bd1e-7fd8d411d84c"
      },
      "outputs": [],
      "source": [
        "# Remove Id\n",
        "numeric_columns.remove('Id')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c160a2-ec7f-4690-9eba-464cfffe2e7a",
      "metadata": {
        "id": "05c160a2-ec7f-4690-9eba-464cfffe2e7a"
      },
      "source": [
        "Now we extract the numeric data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a410bfa5-0140-4d42-a766-c3a553d67683",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "a410bfa5-0140-4d42-a766-c3a553d67683",
        "outputId": "f50524ac-4193-4da0-ca7f-079ba6c76233"
      },
      "outputs": [],
      "source": [
        "# Extract numeric data\n",
        "numeric_data = raw_data[numeric_columns]\n",
        "numeric_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a51ca5a4-e780-4b42-a304-01c5b6a915b2",
      "metadata": {
        "id": "a51ca5a4-e780-4b42-a304-01c5b6a915b2"
      },
      "source": [
        "Now let's deal with the missing values in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9084d6e-2a1f-4e81-ab29-a575cf340478",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9084d6e-2a1f-4e81-ab29-a575cf340478",
        "outputId": "39cef36c-83ca-47fd-d7fa-3f1096733442"
      },
      "outputs": [],
      "source": [
        "# Display features with missing values\n",
        "nan_columns = np.any(pd.isna(numeric_data), axis = 0)\n",
        "nan_columns = list(nan_columns[nan_columns == True].index)\n",
        "nan_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "748bbb00-d6db-47a8-b295-8652159c69c1",
      "metadata": {
        "id": "748bbb00-d6db-47a8-b295-8652159c69c1"
      },
      "source": [
        "We simply replace them with zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "id": "45c3bbd9-c641-4864-b657-b193d1659f67",
      "metadata": {
        "id": "45c3bbd9-c641-4864-b657-b193d1659f67"
      },
      "outputs": [],
      "source": [
        "# Replace NAN with 0\n",
        "numeric_data = numeric_data.fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3244053-ee49-4f29-b4c1-aefe674bdb33",
      "metadata": {
        "id": "a3244053-ee49-4f29-b4c1-aefe674bdb33"
      },
      "source": [
        "##### Creating training and test set\n",
        "\n",
        "Let's split the data for training and test. We use the `train_test_split` module of `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "id": "91354d50-6616-4454-a8cf-3aa8a615d9e6",
      "metadata": {
        "id": "91354d50-6616-4454-a8cf-3aa8a615d9e6"
      },
      "outputs": [],
      "source": [
        "# Splitting training/test set\n",
        "numeric_data_train, numeric_data_test = train_test_split(numeric_data, test_size=0.1, random_state=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac2ae80-d076-489e-af05-251d81820da3",
      "metadata": {
        "id": "0ac2ae80-d076-489e-af05-251d81820da3"
      },
      "source": [
        "##### Normalizing the data\n",
        "\n",
        "Before training our linear regression model, we have to normalize the data. We do this by subtracting each column from its minimum value and then dividing it by the difference between maximum and minimum. We use the `MinMaxScaler` of `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "id": "ea634322-702e-42d7-aaa3-722b144ac6f9",
      "metadata": {
        "id": "ea634322-702e-42d7-aaa3-722b144ac6f9"
      },
      "outputs": [],
      "source": [
        "#Define the scaler\n",
        "scaler = MinMaxScaler()\n",
        "#Fit the scaler\n",
        "scaler.fit(numeric_data_train)\n",
        "#Transform the train and the test set\n",
        "numeric_data_train.loc[:,:] = scaler.transform(numeric_data_train)\n",
        "numeric_data_test.loc[:,:] = scaler.transform(numeric_data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dce94995-d1aa-4efc-9bc9-727c32d23291",
      "metadata": {
        "id": "dce94995-d1aa-4efc-9bc9-727c32d23291"
      },
      "source": [
        "Finally, we split the column we want to predict (\"SalePrice\") to our features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "id": "14d211f0-ee9d-4ded-bfb0-c870e91ff0a9",
      "metadata": {
        "id": "14d211f0-ee9d-4ded-bfb0-c870e91ff0a9"
      },
      "outputs": [],
      "source": [
        "# Extract features and output\n",
        "numeric_x_columns = list(numeric_data_train.columns)\n",
        "numeric_x_columns.remove(\"SalePrice\")\n",
        "X_train_df = numeric_data_train[numeric_x_columns]\n",
        "y_train_df = pd.DataFrame(numeric_data_train[\"SalePrice\"])\n",
        "X_test_df = numeric_data_test[numeric_x_columns]\n",
        "y_test_df = pd.DataFrame(numeric_data_test[\"SalePrice\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b91b5e4-c62a-48eb-9ded-7b3799c69f85",
      "metadata": {
        "id": "6b91b5e4-c62a-48eb-9ded-7b3799c69f85"
      },
      "source": [
        "Ok, all set, we can start building our Neural Net!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e69796e1-d7d7-4cf4-9950-395082f2084d",
      "metadata": {
        "id": "e69796e1-d7d7-4cf4-9950-395082f2084d"
      },
      "source": [
        "### Building a Linear Regression model with PyTorch\n",
        "\n",
        "We use the `PyTorch` library ([Documentation](https://pytorch.org/), imported at the beginning of this notebook with the following lines of codes:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "```\n",
        "\n",
        "`torch.nn` contains the building blocks to build Neural Nets, e.g., the layers ([Documentation](https://pytorch.org/docs/stable/nn.html))."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b18b7976-ea1e-4a95-9e53-c29d816e615d",
      "metadata": {
        "id": "b18b7976-ea1e-4a95-9e53-c29d816e615d"
      },
      "source": [
        "#### Create tensors\n",
        "\n",
        "The first step is to convert the data into torch tensors. A `torch.Tensor` is a multi-dimensional matrix containing elements of a single data type. It's very similar to arrays in `NumPy`.\n",
        "\n",
        "We rely on `torch.tensor()` for the conversion ([Documentation](https://pytorch.org/docs/stable/tensors.html))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "id": "ad283baf-f652-4d6f-a6fa-441c53731ea2",
      "metadata": {
        "id": "ad283baf-f652-4d6f-a6fa-441c53731ea2"
      },
      "outputs": [],
      "source": [
        "X_train = torch.tensor(X_train_df.values, dtype=torch.float)\n",
        "y_train = torch.tensor(y_train_df.values, dtype=torch.float)\n",
        "X_test = torch.tensor(X_test_df.values, dtype=torch.float)\n",
        "y_test = torch.tensor(y_test_df.values, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2abafe50-c9cf-4275-8d08-3aca51122e0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2abafe50-c9cf-4275-8d08-3aca51122e0f",
        "outputId": "571bddd2-c990-4e92-ac19-2668f0acbbd1"
      },
      "outputs": [],
      "source": [
        "print(X_train.size(), y_train.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aae66a72-8269-4917-9ac4-0c42f8dcd249",
      "metadata": {
        "id": "aae66a72-8269-4917-9ac4-0c42f8dcd249"
      },
      "source": [
        "#### Define and train a model with PyTorch\n",
        "\n",
        "A model is defined as a `class` in PyTorch. Classes are a means of bundling data and functionality together, allowing to create a new type of Python object. You can read the Python documentation on [Classes](https://docs.python.org/3/tutorial/classes.html) to learn more about them.\n",
        "\n",
        "When you create your Neural Net, you should define:\n",
        "- a `__init__` function in which you define the layers of your network.\n",
        "- a `forward` function (method) that defines the forward pass on the network.\n",
        "\n",
        "For the beginning, let's start with a single layer network:\n",
        "- The layer `nn.Linear()` performs a linear transformation ([Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)). The input and output are the number of neurons\n",
        "- `nn.ReLU()` applies the Rectified Linear Unit function: $ReLU(x)=\\max(0,x)$ ([Documentation](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "id": "27e00f35-13ce-4e0b-946c-a2f0ad780e79",
      "metadata": {
        "id": "27e00f35-13ce-4e0b-946c-a2f0ad780e79"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, D_in, H1, D_out):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(D_in, H1)        # Linear transformation for hidden layer\n",
        "        self.linear2 = nn.Linear(H1, D_out)       # Linear transformation for output layer\n",
        "        self.activation = nn.ReLU()               # Activation function for hidden layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = self.activation(self.linear1(x))   # Hidden layer: linear transformation + ReLU\n",
        "        y_pred = self.linear2(y_pred)               # Output layer: linear transformation\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ccb4e35-61d3-49fe-aa50-a0c9e7590be9",
      "metadata": {
        "id": "3ccb4e35-61d3-49fe-aa50-a0c9e7590be9"
      },
      "source": [
        "`D_in` is the input dimension, i.e., the number of features. Similarly, `D_out` is the output dimension, i.e., 1 (we only predict the \"SalePrice\"):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc3176e-1cab-4b11-88c6-f3547752aca3",
      "metadata": {
        "id": "7dc3176e-1cab-4b11-88c6-f3547752aca3"
      },
      "outputs": [],
      "source": [
        "D_in, D_out = X_train.shape[1], y_train.shape[1]\n",
        "\n",
        "print(D_in, D_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1be40cbf-4811-4338-afab-2de1798c6257",
      "metadata": {
        "id": "1be40cbf-4811-4338-afab-2de1798c6257"
      },
      "source": [
        "Ok, let's define our first model. It is an instance of our newly-created class \"Net\". We are going to use 500 neurons for the hidden layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "id": "1c315b3f-951b-4b8b-8e5b-12594dcc265f",
      "metadata": {
        "id": "1c315b3f-951b-4b8b-8e5b-12594dcc265f"
      },
      "outputs": [],
      "source": [
        "# Model with 500 neurons\n",
        "model1 = Net(D_in, 500, D_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WfBZxU5fmHsg",
      "metadata": {
        "id": "WfBZxU5fmHsg"
      },
      "source": [
        "Let's calculate now how many parameters we have in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2_pUo4LGl7Us",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_pUo4LGl7Us",
        "outputId": "67991f1f-90b4-4cda-d99c-5b8a7ad957b3"
      },
      "outputs": [],
      "source": [
        "# calculate how many parameters are in the model\n",
        "pytorch_total_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae8c9515-d808-4838-92d1-a56deb34ff08",
      "metadata": {
        "id": "ae8c9515-d808-4838-92d1-a56deb34ff08"
      },
      "source": [
        "The next steps is to define the **loss criterion** and the **optimizer** for the network. That is, we have to define the loss function we want to optimize during training and also the optimization method. We use:\n",
        "- `MSELoss()` as loss criterion, i.e., the mean square error ([Documentation](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html))\n",
        "- `SGD()`as optimizer, i.e., stochastic gradient descent ([Documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "id": "0efef490-e70e-445e-be89-3f206912d03e",
      "metadata": {
        "id": "0efef490-e70e-445e-be89-3f206912d03e"
      },
      "outputs": [],
      "source": [
        "# MSE loss\n",
        "criterion = nn.MSELoss(reduction='sum')\n",
        "# SGD optimizer for finding the weights of the network\n",
        "optimizer = torch.optim.SGD(model1.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4268da0-fa53-43e2-8ee8-e8f3bb952d22",
      "metadata": {
        "id": "a4268da0-fa53-43e2-8ee8-e8f3bb952d22"
      },
      "source": [
        "Wonderful, we are ready to do the training! We can simply by looping over the number of iterations. The training has 3 main steps:\n",
        "- A forward pass to compute the prediction for the current data point (batch).\n",
        "- Computing the loss for the current prediction with the previously defined criterion.\n",
        "- A backward pass to compute the gradient of the loss with respect to the weight of the network (`backward()`)\n",
        "- Finally, updating the weights of the network (`optimizer.step()`).\n",
        "\n",
        "Note that in each backward pass PyTorch saves the gradient for all of the parameters. Therefore it is important to replace the old gradient values with zero in the beginning of each iteration (`optimizer.zero_grad()`), otherwise the gradients will be accumulated during the iterations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ec6c24-99e9-4a0f-bebd-25ea390edd0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42ec6c24-99e9-4a0f-bebd-25ea390edd0b",
        "outputId": "294c7f13-d28c-4c29-d795-bde338593709"
      },
      "outputs": [],
      "source": [
        "losses1 = []\n",
        "losses1_test = []\n",
        "\n",
        "for t in range(500):                # 500 iterations\n",
        "\n",
        "    # Forward pass: compute prediction on training set\n",
        "    y_pred = model1(X_train)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(y_pred, y_train)\n",
        "    print(t, loss.item())\n",
        "    losses1.append(loss.item())\n",
        "    if torch.isnan(loss):\n",
        "        break\n",
        "\n",
        "    # Compute gradient\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Update\n",
        "    optimizer.step()\n",
        "\n",
        "    # Compute loss on test set\n",
        "    losses1_test.append(criterion(model1(X_test), y_test).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09d02e07-bd34-45ac-867d-8982849127a0",
      "metadata": {
        "id": "09d02e07-bd34-45ac-867d-8982849127a0"
      },
      "source": [
        "Let's visualize the evolution of the MSE on the training set and test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f20a67-f7f3-457b-be47-fa0884f43921",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "72f20a67-f7f3-457b-be47-fa0884f43921",
        "outputId": "d7b55cca-a4a0-4cc9-cbe3-047c668386f4"
      },
      "outputs": [],
      "source": [
        "# Plot training and test loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(losses1, label=\"Training loss\")\n",
        "plt.plot(losses1_test, label=\"Test loss\")\n",
        "plt.title('Evolution of training and test loss - 500 neurons')\n",
        "plt.ylim(top=70, bottom=0.0)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a0d9df0-6281-4f7e-bc24-de4043050d9f",
      "metadata": {
        "id": "1a0d9df0-6281-4f7e-bc24-de4043050d9f"
      },
      "source": [
        "Now let's try a new model with more neurons in the hidden layer. We use 1000 neurons, and follow the same steps as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "id": "9e01fdf3-d136-4186-9031-5afa987dcff6",
      "metadata": {
        "id": "9e01fdf3-d136-4186-9031-5afa987dcff6"
      },
      "outputs": [],
      "source": [
        "model2 = Net(D_in, 1000, D_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "id": "4bff1abe-a077-4aae-ac9b-3f691384d92d",
      "metadata": {
        "id": "4bff1abe-a077-4aae-ac9b-3f691384d92d"
      },
      "outputs": [],
      "source": [
        "# MSE loss\n",
        "criterion = nn.MSELoss(reduction='sum')\n",
        "# SGD optimizer for finding the weights of the network\n",
        "optimizer = torch.optim.SGD(model2.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "id": "6a66ea8a-0024-4a15-a610-b2a48bffb53f",
      "metadata": {
        "id": "6a66ea8a-0024-4a15-a610-b2a48bffb53f"
      },
      "outputs": [],
      "source": [
        "losses2 = []\n",
        "\n",
        "for t in range(500):\n",
        "    y_pred = model2(X_train)\n",
        "\n",
        "    loss = criterion(y_pred, y_train)\n",
        "    losses2.append(loss.item())\n",
        "\n",
        "    if torch.isnan(loss):\n",
        "        break\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a867ebe-fc63-48e5-bf2c-4c0f04c4f507",
      "metadata": {
        "id": "9a867ebe-fc63-48e5-bf2c-4c0f04c4f507"
      },
      "source": [
        "Let's visualize the evolution of the training loss for the two models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e65c835d-cadf-4149-83f0-36356f48a67a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "e65c835d-cadf-4149-83f0-36356f48a67a",
        "outputId": "53473407-cda2-4563-c49e-1bbacd88b7b5"
      },
      "outputs": [],
      "source": [
        "# Plot training and test loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(losses1, label=\"Model 1: 500 neurons\")\n",
        "plt.plot(losses2, label=\"Model 2: 1000 neurons\")\n",
        "plt.title('Evolution of training loss')\n",
        "plt.ylim(top=70, bottom=0.0)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da286f3-4298-4bac-901a-a553314f7201",
      "metadata": {
        "id": "2da286f3-4298-4bac-901a-a553314f7201"
      },
      "source": [
        "Let's compare the MSE loss on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7035489-609d-423f-a36c-0158923fb064",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7035489-609d-423f-a36c-0158923fb064",
        "outputId": "170e511e-db8a-4696-c8f8-f752c9869d74"
      },
      "outputs": [],
      "source": [
        "# prediction for model 1\n",
        "model1_pred = model1(X_test)\n",
        "print(\"MSE loss for model 1: \", criterion(model1_pred, y_test))\n",
        "# prediction for model 2\n",
        "model2_pred = model2(X_test)\n",
        "print(\"MSE loss for model 2: \", criterion(model2_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f7f0b8d",
      "metadata": {},
      "source": [
        "Then, we can switch the model to evaluation mode with `.eval()`, then predict both the training and test data. After making predictions, calculate and print the training and test accuracy to evaluate the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a27475c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the model to evaluation mode\n",
        "model1.eval()\n",
        "\n",
        "# Predict and calculate training accuracy\n",
        "y_tr_pred = model1(X_train)\n",
        "y_tr_pred = torch.argmax(y_tr_pred, dim=1)\n",
        "# train_labels = y_train.argmax(dim=1)\n",
        "train_acc = (y_train == y_tr_pred).sum() / len(y_train)\n",
        "print(f\"train accuracy: {train_acc}\")\n",
        "\n",
        "# Predict and calculate test accuracy\n",
        "y_te_pred = model1(X_test)\n",
        "y_te_pred = torch.argmax(y_te_pred, dim=1)\n",
        "# test_labels = y_test.argmax(dim=1)\n",
        "test_acc = (y_test == y_te_pred).sum() / len(y_test)\n",
        "print(f\"test accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fd45a42-8bdc-45e6-be68-7e18b7253a63",
      "metadata": {
        "id": "4fd45a42-8bdc-45e6-be68-7e18b7253a63"
      },
      "source": [
        "What do you think?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dorz99y4EToc",
      "metadata": {
        "id": "dorz99y4EToc"
      },
      "source": [
        "## Your turn !"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CuT4YoaXEV-A",
      "metadata": {
        "id": "CuT4YoaXEV-A"
      },
      "source": [
        "Based on this dataset ([link](https://github.com/michalis0/DataScience_and_MachineLearning/blob/master/04-neural-networks/data/spotify.csv)) imported from kaggle, create a Neural Network that will predict the streams of a song based on its bpm, key, danceability, valence, energy, acousticness, instrumentalness, liveness, and speechiness. (Hint: if there are some errors, try to copy/paste them in Google to see some solutions.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CUKC2qaLETBV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "CUKC2qaLETBV",
        "outputId": "30c357b2-e6f7-48bf-e5e6-fc459db71706"
      },
      "outputs": [],
      "source": [
        "#import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c1303a-7a43-4bcc-a930-b91d16acf1af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "11c1303a-7a43-4bcc-a930-b91d16acf1af",
        "outputId": "d585eaf5-c179-479e-8a54-0d714a97776e"
      },
      "outputs": [],
      "source": [
        "#select only the columns that are interesting for you and display the type of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2qjJ53TKH24I",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "2qjJ53TKH24I",
        "outputId": "dfad7782-05ad-4add-955a-811436c212b0"
      },
      "outputs": [],
      "source": [
        "# keep only the numeric data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "id": "iPg_l8aOID0E",
      "metadata": {
        "id": "iPg_l8aOID0E"
      },
      "outputs": [],
      "source": [
        "# Splitting training/test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "id": "yohti5cUXn7I",
      "metadata": {
        "id": "yohti5cUXn7I"
      },
      "outputs": [],
      "source": [
        "# encode 'key' feature using LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "id": "_EiU-bHLOO_R",
      "metadata": {
        "id": "_EiU-bHLOO_R"
      },
      "outputs": [],
      "source": [
        "# scale your data using MinMaxScaler\n",
        "#Define the scaler\n",
        "\n",
        "#Fit the scaler\n",
        "\n",
        "#Transform the train and the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gaosh24gOOqz",
      "metadata": {
        "id": "Gaosh24gOOqz"
      },
      "outputs": [],
      "source": [
        "# Extract features and output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "id": "kVXb22yzOrFD",
      "metadata": {
        "id": "kVXb22yzOrFD"
      },
      "outputs": [],
      "source": [
        "# Transform the data into tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oJBlJsBYO6Qa",
      "metadata": {
        "id": "oJBlJsBYO6Qa"
      },
      "source": [
        "Now create a Neural Net class with two hidden layer using the ReLU activation layer for both of them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "id": "gw_4_CVsO1yL",
      "metadata": {
        "id": "gw_4_CVsO1yL"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "id": "YlpMENjKO2dl",
      "metadata": {
        "id": "YlpMENjKO2dl"
      },
      "outputs": [],
      "source": [
        "# give the right dimension to the input and output layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "id": "MPvresr1O2bR",
      "metadata": {
        "id": "MPvresr1O2bR"
      },
      "outputs": [],
      "source": [
        "# Create a model with 100 neurons for both hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gjsYOgwDmU2C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "gjsYOgwDmU2C",
        "outputId": "47984157-19a7-44e5-d897-da74ce97bf0b"
      },
      "outputs": [],
      "source": [
        "# calculate how many parameters the model has"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "id": "y8FdT7BDQSBF",
      "metadata": {
        "id": "y8FdT7BDQSBF"
      },
      "outputs": [],
      "source": [
        "# use the MSELoss() as the loss criterion and Adam() as the optimizer (search online the optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3QvTMKNoZSes",
      "metadata": {
        "id": "3QvTMKNoZSes"
      },
      "source": [
        "Now create a loop to train your model (with 500 iterations), don't forget to save your loss criterion in order to plot it later !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WHghL05IQR3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHghL05IQR3d",
        "outputId": "c034806d-4847-4781-be3d-30b0731bdf0d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-OYcTnB2Q8ke",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "-OYcTnB2Q8ke",
        "outputId": "b47285bf-bac3-4298-caad-e921492971ca"
      },
      "outputs": [],
      "source": [
        "# Plot training and test loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aa9461b",
      "metadata": {},
      "source": [
        "## 3. Automating Hyperparameter Tuning\n",
        "\n",
        "When training a model, there are often many hyperparameters to adjust, leading to countless possible combinations. To determine the optimal configuration, experimentation is crucial. However, manually testing each combination can be highly time-consuming. In this section, we’ll automate the hyperparameter tuning process to efficiently find the best-performing model.\n",
        "\n",
        "Since we are working on a regression problem with a numerical target, accuracy is not an appropriate metric for evaluation. Instead, we will use **RMSE** (Root Mean Squared Error) to assess model performance and identify the best hyperparameter combination."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21624c6c",
      "metadata": {},
      "source": [
        "We begin by redefining the `Net()` class, similar to how we did earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "id": "f06b6f46",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, D_in, H1, D_out):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(D_in, H1)        # Linear transformation for hidden layer\n",
        "        self.linear2 = nn.Linear(H1, D_out)       # Linear transformation for output layer\n",
        "        self.activation = nn.ReLU()               # Activation function for hidden layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = self.activation(self.linear1(x))   # Hidden layer: linear transformation + ReLU\n",
        "        y_pred = self.linear2(y_pred)               # Output layer: linear transformation\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "id": "81b21a6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "D_in, D_out = X_train.shape[1], y_train.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7479638b",
      "metadata": {},
      "source": [
        "Then we define the ranges for the hyperparameters you want to explore:\n",
        "\n",
        "- **Neurons**: Test with hidden layers containing **50**, **150**, **200**, and **1000** neurons.\n",
        "- **Loss Functions**: Experiment with **MSELoss** and **L1Loss** to assess model performance.\n",
        "\n",
        "Additionally, initialize variables to track the best `RMSE` and the corresponding hyperparameters. Fix the learning rate at **0.0001** and the number of epochs at **500** for consistency across all tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "id": "60ec656a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the ranges for neurons and criterion you want to test\n",
        "neurons_list = [50, 150, 200, 1000]  # Number of neurons to test in the hidden layer\n",
        "criteria_list = [nn.MSELoss(reduction='sum'), nn.L1Loss()]  # List of criteria to test\n",
        "\n",
        "# Initialize a variable to store the best metrics and hyperparameters\n",
        "best_rmse = float('inf')  # We're aiming for the lowest RMSE\n",
        "best_params = {}\n",
        "\n",
        "# Fixed hyperparameters\n",
        "learning_rate = 1e-4\n",
        "epochs = 500\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c741e15",
      "metadata": {},
      "source": [
        "This code performs hyperparameter tuning by testing various combinations of the number of neurons in the hidden layer (`neurons_list`) and loss functions (`criteria_list`). For each combination, it initializes a model, trains it over a specified number of epochs, and evaluates the model’s performance on the test set using regression metrics such as MSE, MAE, and R². After calculating the RMSE (Root Mean Squared Error), the code tracks the best-performing model and its corresponding hyperparameters based on the lowest RMSE.\n",
        "\n",
        "**Note**: This approach can be extended to tune other hyperparameters (such as learning rate, optimizer, etc.), and you can also use other evaluation metrics depending on the type of problem you're solving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd14cc2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loop over different neurons and criteria\n",
        "for neurons in neurons_list:\n",
        "    for criterion in criteria_list:\n",
        "        print(f\"\\nTesting with {neurons} neurons and criterion: {criterion}\")\n",
        "\n",
        "        # Initialize the model and optimizer for each combination\n",
        "        model1 = Net(D_in, neurons, D_out)\n",
        "        optimizer = torch.optim.SGD(model1.parameters(), lr=learning_rate)\n",
        "\n",
        "        losses_train = []\n",
        "        losses_test = []\n",
        "\n",
        "        for t in range(epochs):\n",
        "            model1.train()  # Set model to training mode\n",
        "            \n",
        "            # Forward pass: compute prediction on training set\n",
        "            y_pred = model1(X_train)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(y_pred, y_train)\n",
        "            losses_train.append(loss.item())\n",
        "            if t % 100 == 0:\n",
        "                # Print the training loss every 100 steps\n",
        "                print(f\"Epoch {t}, Loss: {loss.item()}\")\n",
        "\n",
        "            # Compute gradient\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Update model parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute loss on test set\n",
        "            model1.eval()  # Set model to evaluation mode\n",
        "            with torch.no_grad():\n",
        "                y_pred_test = model1(X_test)\n",
        "                loss_test = criterion(y_pred_test, y_test).item()\n",
        "                losses_test.append(loss_test)\n",
        "\n",
        "        # Calculate regression metrics on the test set\n",
        "        with torch.no_grad():\n",
        "            y_pred_test = model1(X_test).numpy()\n",
        "            y_test_np = y_test.numpy()\n",
        "\n",
        "            # Calculate MSE, MAE, and R²\n",
        "            mse = mean_squared_error(y_test_np, y_pred_test)\n",
        "            mae = mean_absolute_error(y_test_np, y_pred_test)\n",
        "            r2 = r2_score(y_test_np, y_pred_test)\n",
        "\n",
        "            # Print the metrics\n",
        "            print(f\"Test MSE: {mse}\")\n",
        "            print(f\"Test MAE: {mae}\")\n",
        "            print(f\"Test R²: {r2}\")\n",
        "            print(f\"Test RMSE: {mse ** 0.5}\")\n",
        "\n",
        "            # Keep track of the best model based on RMSE (sqrt of MSE)\n",
        "            rmse = mse ** 0.5\n",
        "            if rmse < best_rmse:\n",
        "                best_rmse = rmse\n",
        "                best_params = {'neurons': neurons, 'criterion': criterion}\n",
        "                print(f\"New best RMSE: {best_rmse} with {neurons} neurons and criterion: {criterion}\")\n",
        "\n",
        "# Print the best hyperparameters and corresponding RMSE after testing all combinations\n",
        "print(\"\\nBest Test RMSE:\", best_rmse)\n",
        "print(\"Best Hyperparameters:\", best_params)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7763984b-c9d6-43b0-ba06-fd749431aa32"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
