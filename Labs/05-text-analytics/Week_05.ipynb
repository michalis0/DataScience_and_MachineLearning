{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg25MkDgcADy"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/06-text-analytics/Week_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XwVXjSMR15K",
        "outputId": "93dfb79a-2b62-45fb-b048-dfc6b40e7d30"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia\n",
        "!pip install -q transformers\n",
        "%pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1NCfM18ZRl8L"
      },
      "outputs": [],
      "source": [
        "# Import standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "\n",
        "# Import for text analytics\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import string\n",
        "import wikipedia\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import corpora\n",
        "import multiprocessing\n",
        "\n",
        "# Import libraries for logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Import libraries for hugginface\n",
        "from transformers import pipeline\n",
        "import gensim.downloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5WgfVqRRl8N"
      },
      "source": [
        "# Text Analytics\n",
        "\n",
        "<img src='https://images.unsplash.com/photo-1605429201125-37e867327609?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1176&q=80' width=\"450\">\n",
        "\n",
        "Credit: [Piotr Łaskawski](https://unsplash.com/@tot87)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onO46LysT2dh"
      },
      "source": [
        "## Content\n",
        "\n",
        "The goal of this walkthrough is to provide you with insights on text analytics. [Text Analytics](https://en.wikipedia.org/wiki/Text_mining) (or text mining) is \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" ([Marti Hearst](https://people.ischool.berkeley.edu/~hearst/text-mining.html)). Written resources may include websites, books, emails, reviews, and articles.\n",
        "\n",
        "There are many applications of text analytics, for example:\n",
        "- Search for relevant websites or articles using a search engine;\n",
        "- Sentiment Analysis (e.g., classify tweets or film reviews as positive, neutral or negative);\n",
        "- Summarize, anonymize, or translate documents;\n",
        "- Chatbots (e.g., ChatGPT, Siri, Alexa);\n",
        "- etc.\n",
        "\n",
        "In this notebook, we will see how to prepare and represent texts and explore various text-analytics techniques, before doing an application on text similarity:\n",
        "- [Text Preparation](#Text-Preparation)\n",
        "    - [Tokenization](#Tokenization)\n",
        "    - [Remove Stopwords](#Remove-Stopwords)\n",
        "    - [Lemmatization](#Lemmatization)\n",
        "    - [Your turn!](#Your-turn-Preparation)\n",
        "- [Text Representation](#Text-Representation)\n",
        "    - [Bag of Words (BOW)](#Bag-of-Words-(BOW))\n",
        "    - [TF-IDF Representation](#TF-IDF-Representation)\n",
        "    - [Your turn!](#Your-turn-Representation)\n",
        "- [Introduction to Gensim and Word Embedding](#Introduction-to-Gensim-and-Word-Embedding)\n",
        "    - [Background](#Background)\n",
        "    - [Implementing Word2vec with Gensim](#Implementing-Word2vec-with-Gensim)\n",
        "    - [Using pretrained models](#Using-pretrained-models)\n",
        "    - [Your turn](#Your-turn)\n",
        "- [Application: Text Classification with TF-IDF](#Application:-Text-Classification-with-TF-IDF)\n",
        "    - [Load and clean data](#Load-and-clean-data)\n",
        "    - [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
        "    - [Classification using TF-IDF and Logistic Regression](#Classification-using-TF-IDF-and-Logistic-Regression)\n",
        "- [Introduction to Hugginface and sentiment analysis](#Introduction-to-Hugginface-and-sentiment-analysis)\n",
        "    - [Implementation of Hugginface](#Implementation-of-Hugginface)\n",
        "    - [Your turn !](#Your-turn-!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXlVy05IUC-D"
      },
      "source": [
        "## Text Preparation\n",
        "\n",
        "In this section, we explain how to prepare a text for analysis. This includes tokenizing the text, removing stopwords, etc.\n",
        "\n",
        "We will use the [spaCy](https://spacy.io/) library, an open-source natural language processing library for Python. It is designed particularly for production use, and it can help us to build applications that process massive volumes of text efficiently.\n",
        "\n",
        "You can directly [install the library](https://spacy.io/usage) in your Anaconda environment, or, if you opened this notebook in Colab, with the following line of code:\n",
        "```python\n",
        "!pip install -U spacy\n",
        "```\n",
        "\n",
        "We also install the English-language model: in you Anaconda environment install \"spacy-model-en_core_web_sm\"; in Colab, run the following line of code:\n",
        "```python\n",
        "!python -m spacy download en_core_web_sm\n",
        "```\n",
        "\n",
        "Note: If you obtain the error `Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed\"`, try the following:\n",
        "```python\n",
        "!pip --trusted-host github.com --trusted-host objects.githubusercontent.com install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install -U spacy\n",
        "#!python -m spacy download en_core_web_sm\n",
        "!pip --trusted-host github.com --trusted-host objects.githubusercontent.com install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPv4i-ZERl8Q"
      },
      "source": [
        "Once everything is installed, and imported (at the beginning of this notebook), we can load our language dictionary, namely the English language model, using `spacy.load('en_core_web_sm')`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dXkDfXDXRl8R"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load English language model\n",
        "sp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUibE-SyTp8d"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "**Tokenization** is the process of breaking a text into pieces called tokens. A **token** simply refers to an individual part of a sentence having some semantic value. In other words, tokens are the elementary building blocks (words, numbers, characters) in a document.\n",
        "\n",
        "SpaCy's tokenizer takes input in form of unicode text and outputs a sequence of token objects. In addition, SpaCy automatically breaks your document into tokens when a document is created using the language model.\n",
        "\n",
        "There are a couple of different ways we can approach this. The first is called **word tokenization**, which means breaking up the text into individual words. This is a critical step for many language processing applications, as they often require inputs in the form of individual words rather than longer strings of text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI4Z4wu9Rl8R"
      },
      "source": [
        "Let’s take a look at a simple example. Imagine we have the following text, and we would like to tokenize it:\n",
        "\n",
        "> When learning data science, you shouldn't get discouraged!\n",
        "\n",
        "> Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\n",
        "\n",
        "We create a spaCy object, which contains linguistic annotations and various language properties:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj9OfB1BTp8e"
      },
      "outputs": [],
      "source": [
        "# Declare text\n",
        "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
        "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
        "\n",
        "# spaCy object is used to create a document\n",
        "my_doc = sp(text)\n",
        "\n",
        "my_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc7fwTbPZdXJ"
      },
      "outputs": [],
      "source": [
        "# This is a spaCy document\n",
        "type(my_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukEQ-vS4Rl8X"
      },
      "source": [
        "Let's now create a list of tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex7-T-BkYXZM"
      },
      "outputs": [],
      "source": [
        "# Create list of tokens\n",
        "token_list = [token.text for token in my_doc]\n",
        "token_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi29AcSsTp8j"
      },
      "source": [
        "As we can see, spaCy produces a list that contains each token as a separate item. Notice that it has recognized that contractions such as _shouldn’t_ actually represent two distinct words, and has thus broken them down into two distinct tokens.\n",
        "\n",
        "We can also see the parts-of-speech (POS) of each of these tokens using the `.pos_` attribute, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvkGwDJoWoIs"
      },
      "outputs": [],
      "source": [
        "# POS\n",
        "for word in my_doc:\n",
        "    print(word.text, '->', word.pos_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb_cILFJRl8Y"
      },
      "source": [
        "POS tagging can be really useful, particularly if you have words or tokens that can have multiple POS tags. For instance, the word \"fish\" can be used as both a noun and verb, depending upon the context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUsKHgp7Y3yM"
      },
      "outputs": [],
      "source": [
        "# Another example\n",
        "doc1 = sp(\"I like to fish\") # verb\n",
        "doc2 = sp(\"I eat a fish\") # noun\n",
        "\n",
        "for word in doc1:\n",
        "    print(word.text, '->', word.pos_)\n",
        "\n",
        "print(\"-----------------\")\n",
        "\n",
        "for word in doc2:\n",
        "    print(word.text, '->', word.pos_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCiU1-PDXKpp"
      },
      "source": [
        "If we want, we can also break the text into sentences rather than words. This is called **sentence tokenization**. When performing sentence tokenization, the tokenizer looks for specific characters that normally fall between sentences, like periods, exclamation points, and newline characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odi_WkNZTp8k"
      },
      "outputs": [],
      "source": [
        "# create list of sentence tokens\n",
        "sents_list = [sent.text for sent in my_doc.sents]\n",
        "\n",
        "sents_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Z6MiCtTp8n"
      },
      "source": [
        "### Remove Stopwords\n",
        "\n",
        "Most text data that we work with is going to contain a lot of words that are not actually useful to our analysis (e.g., \"is\", \"and\", \"you\", etc.). These words, called **stopwords**, are useful in human speech, but they do not have much to contribute to the meaning of a sentence. Removing stopwords helps us eliminate noise and distraction from our text data, and also speeds up the time of the analysis (since there are fewer words to process). This makes text analysis more efficient.\n",
        "\n",
        "\n",
        "Let’s take a look at the stopwords spaCy includes by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kB8kI0eTp8o"
      },
      "outputs": [],
      "source": [
        "# Import stopwords from English language\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Print total number of stopwords\n",
        "print('Number of stopwords: %d' % len(spacy_stopwords))\n",
        "\n",
        "# Print 20 stopwords\n",
        "print('20 stopwords: %s' % list(spacy_stopwords)[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw0Wfvu4Tp8q"
      },
      "source": [
        "Now that we’ve got our list of stopwords, let’s use it to remove the stopwords from the text string we were working on in the previous section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq5yvoZtlhsd"
      },
      "outputs": [],
      "source": [
        "# Which words will be removed?\n",
        "my_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y54Kiz9zTp8r"
      },
      "outputs": [],
      "source": [
        "# Filter stopwords\n",
        "filtered_sent = [word.text for word in my_doc if word.is_stop == False]\n",
        "\n",
        "print('The filtered sentence contains the words:', filtered_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4u0DM8kRl8c"
      },
      "source": [
        "We can also remove the punctuation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOvM2NFdl29W"
      },
      "outputs": [],
      "source": [
        "# Filter stopwords, punctuation and spaces\n",
        "filtered_sent2 = []\n",
        "removed_tokens = []\n",
        "\n",
        "for word in my_doc:\n",
        "    if (word.is_stop == True) or (word.is_punct == True) or (word.is_space == True):\n",
        "        removed_tokens.append(word.text)\n",
        "    else:\n",
        "        filtered_sent2.append(word.text)\n",
        "\n",
        "print('We remove the following tokens:', removed_tokens)\n",
        "print('The filtered sentence contains the words:', filtered_sent2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imbw_TUkTp8v"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "**Lemmatization** is a way of dealing with the fact that while words like connect, connection, connecting, connected, etc. are not exactly the same, they all have the same essential meaning: connect. The differences in spelling have grammatical functions in spoken language, but for machine processing, those differences can be confusing, so we need a way to change all the words that are forms of the word connect into the word connect itself.\n",
        "\n",
        "One method for doing this is called **stemming**. Stemming involves simply lopping off easily-identified prefixes and suffixes to produce what is often the simplest version of a word, the root. Connection, for example, would have the -ion suffix removed and be reduced to connect. This kind of simple stemming is often all that is needed, but lemmatization — which actually looks at words and their roots (called lemma) as described in the dictionary — is more precise (e.g feet -> foot).\n",
        "\n",
        "Let's look at this simple example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTqSImYfTp8v"
      },
      "outputs": [],
      "source": [
        "# Lemmatization\n",
        "lem = sp(\"run runs ran running runner runners\")\n",
        "\n",
        "# Find lemma for each word\n",
        "for word in lem:\n",
        "    print(word.text, '->', word.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HutxBkWKRl8d"
      },
      "source": [
        "### Your turn! <a id = \"Your-turn-Preparation\"></a>\n",
        "\n",
        "The text below is taken from the [the presentation](https://www.unil.ch/formations/en/home/menuinst/masters/systemes-dinformation.html) of the Master of Science (MSc) in Information Systems and Digital Innovation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HRCf0C51Rl8d"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"The Master of Science in Information Systems and Digital Innovation allows you to acquire advanced skills in New Information and Communications Technologies (NICT) for use within organisations.\n",
        "Subjects are studied with a balanced multidisciplinary approach and cover both information technology and management techniques.\n",
        "The Master’s degree thus trains high-level specialists with the skills needed to design, manage, evaluate and implement IT services and applications.\n",
        "This course also allows to undertake doctoral studies.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzmqPJQ1Rl8d"
      },
      "source": [
        "- Create two lists:\n",
        "    - the first one containing the punctuation and the stopwords,\n",
        "    - the second one containing the words (tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TquitNSjRl8e"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGwkfIRkRl8e"
      },
      "source": [
        "- For each token, print its lemma\n",
        "\n",
        "*Note:* You can convert a list of strings into a string using for instance the `join()` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVY7DYZcRl8e"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QZOutDwsYij"
      },
      "source": [
        "## Text Representation\n",
        "\n",
        "The goal is to transform text into numerical features such that it can be used by ML algorithms. There are different techniques:\n",
        "- **Bag of Words (BOW)** simply treat every document as an unordered set of words. It works in many case but order is not preserved. As a solution, we can use **n-grams**, i.e., we count token pairs, triplets, etc.\n",
        "- **TF-IDF**: emphasizes important words, i.e., words that appear frequently in a document, (informing about the topic of the document), and words that are rare in a corpus of documents (setting one document apart from other similar ones).\n",
        "\n",
        "To transform our text, we are going to use our old friend, the scikit learn library. As input it will require strings (and not a spaCy object). Here, our corpus of documents will consist of four sentences on [symbiosis](https://en.wikipedia.org/wiki/Symbiosis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2MjeqoQxdOx"
      },
      "outputs": [],
      "source": [
        "# Sentences (as strings, not spaCy objects)\n",
        "s1 = \"Symbiosis is any type of a close and long-term biological interaction between two biological organisms of different species.\"\n",
        "s2 = \"Mutualism describes the ecological interaction between two or more species where each species has a net benefit.\"\n",
        "s3 = \"Commensalism is a long-term biological interaction (symbiosis) in which members of one species gain benefits while those of the other species neither benefit nor are harmed.\"\n",
        "s4 = \"Parasitism is a close relationship between species, where one organism, the parasite, lives on or inside another organism, the host, causing it some harm, and is adapted structurally to this way of life.\"\n",
        "\n",
        "# List of sentences\n",
        "texts = [s1, s2, s3, s4]\n",
        "texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCFQrtHLtMTt"
      },
      "source": [
        "### Bag of Words (BOW)\n",
        "\n",
        "We use the `CountVectorizer` class of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)), using as parameters:\n",
        "- `ngram_range=(1,2)`, i.e., we consider tokens (1-grams) and pair of tokens (2-grams);\n",
        "- `stop_words=\"english\"`, a built-in stop word list for English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyzBm0BUtLc0"
      },
      "outputs": [],
      "source": [
        "# Using default tokenizer\n",
        "count = CountVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
        "\n",
        "# Learn the vocabulary dictionary and return document-term matrix\n",
        "bow = count.fit_transform(texts)\n",
        "\n",
        "# Show feature matrix\n",
        "print(bow.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDXxETa2Rl8t"
      },
      "source": [
        "Let's check the n-grams (tokens and pair of tokens) created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRmUUc0kzi3_"
      },
      "outputs": [],
      "source": [
        "# Get feature names\n",
        "feature_names = count.get_feature_names_out()\n",
        "\n",
        "# View feature names\n",
        "print('Our n-grams are:', ', '.join(feature_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x64RpSdPRl8u"
      },
      "source": [
        "We can better visualize the result in a dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxCSvGUTz2tl"
      },
      "outputs": [],
      "source": [
        "# Show as a dataframe\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.DataFrame(\n",
        "    bow.todense(),              # Feature matrix\n",
        "    columns=feature_names,      # n-grams\n",
        "    index= ['s1', 's2', 's3', 's4']\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7tNGrTutQ2r"
      },
      "source": [
        "### TF-IDF Representation\n",
        "\n",
        "**TF-IDF** emphasizes important words. It is the product of term frequency (TF) and inverse document frequency (IDF):\n",
        "- **Term Frequency** identifies tokens that appear frequently in a document: TF(token, document) = number of times token appears in document / total number of tokens in document\n",
        "- **Inverse Document Frequency** identifies words that appear rarely in the corpus: IDF(token, corpus) = log( total number of documents in corpus / number of documents containing token )\n",
        "\n",
        "Note that the IDF value for a token remains the same throughout all the documents as it depends upon the total number of documents. On the other hand, TF values of a token differ from document to document.\n",
        "\n",
        "The goal of using TF-IDF instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus since those are less informative than tokens that occur in a small fraction of the corpus.\n",
        "\n",
        "Ok, let's implement TD-IDF. We are using the `TfidfVectorizer` class of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djkUI6hXtWlr"
      },
      "outputs": [],
      "source": [
        "# Sentences (as strings, not spaCy objects)\n",
        "s1 = \"Symbiosis is any type of a close and long-term biological interaction between two biological organisms of different species.\"\n",
        "s2 = \"Mutualism describes the ecological interaction between two or more species where each species has a net benefit.\"\n",
        "s3 = \"Commensalism is a long-term biological interaction (symbiosis) in which members of one species gain benefits while those of the other species neither benefit nor are harmed.\"\n",
        "s4 = \"Parasitism is a close relationship between species, where one organism, the parasite, lives on or inside another organism, the host, causing it some harm, and is adapted structurally to this way of life.\"\n",
        "\n",
        "texts = [s1, s2, s3, s4]\n",
        "\n",
        "# Using default tokenizer in TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
        "\n",
        "# Learn the vocabulary dictionary and return document-term matrix\n",
        "features = tfidf.fit_transform(texts)\n",
        "\n",
        "# Visualize result in dataframe\n",
        "tfidf_df = pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names_out(),\n",
        "    index = ['s1', 's2', 's3', 's4']\n",
        ")\n",
        "\n",
        "display(tfidf_df)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's now possible to calculate the summed TF-IDF value for each term. This gives us an indication of the importance or relevance of each word across the different sentences.\n",
        "\n",
        " The higher the summed TF-IDF value, the more frequently and uniquely the term appears within the documents. This helps identify key terms that stand out and may hold significant meaning in the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_score_per_word = tfidf_df.sum(axis=0).sort_values(ascending=False)\n",
        "print(tfidf_score_per_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another interesting metric to calculate is the sum of all the token TF-IDF values across a single sentence.\n",
        "\n",
        "This tells us the overall importance or uniqueness of the sentence within the context of the entire dataset. A higher summed TF-IDF score for a sentence indicates that the sentence contains more unique or significant terms, making it stand out compared to other sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_score_per_sentence = tfidf_df.sum(axis=1).sort_values(ascending=False)\n",
        "print(tfidf_score_per_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcrRL3rRqgBy"
      },
      "source": [
        "### Your turn! <a id = \"Your-turn-Representation\"></a>\n",
        "\n",
        "- Create a TF-IDF Representation of the three sentences below using bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0gq4mpedRl8w"
      },
      "outputs": [],
      "source": [
        "# Sentences\n",
        "s1 = \"Information systems are structured arrangements of people, data, processes, and technology that work together to collect, process, store, and disseminate information within an organization.\"\n",
        "s2 = \"Information systems encompass various components, such as hardware, software, databases, networks, and human resources, all working in synergy to manage and distribute information effectively.\"\n",
        "s3 = \"Information systems come in different types, including transaction processing systems, management information systems, decision support systems, and executive information systems, tailored to specific organizational needs.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzo8uviDrTLm"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkm6F65kUzR_"
      },
      "source": [
        "## Introduction to Gensim and Word Embedding\n",
        "\n",
        "With BOW and TF-IDF, similar sentences/words have a completely different representation. Thus, sentences with different words but same meaning/semantics will be very distant.\n",
        "\n",
        "In the following, we illustrate how we can find out the relations between words in a dataset, compute the similarity between them, or use the vector representation of those words as input for other applications such as text classification or clustering.\n",
        "\n",
        "We will use the [Gensim](https://pypi.org/project/gensim/) library. Gensim stands for \"Generate Similar\". It is a popular open-source natural language processing (NLP) library used for unsupervised topic modeling. A complete tutorial can be found [here](https://www.tutorialspoint.com/gensim/gensim_introduction.htm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-lFcpJJPXAq"
      },
      "source": [
        "### Background\n",
        "\n",
        "Word embedding approaches use deep learning and neural network-based techniques to convert words into corresponding vectors so that semantically similar vectors are close to each other in an N-dimensional space, where N refers to the dimensions of the vectors. The underlying assumption is that two words sharing similar contexts also share a similar meaning and consequently a similar vector representation from the model.\n",
        "\n",
        "Two word embedding methods:\n",
        "- [Word2vec](https://en.wikipedia.org/wiki/Word2vec), by Google\n",
        "- [GloVe](https://en.wikipedia.org/wiki/GloVe) (Global vectors for Word Representation), by Stanford\n",
        "\n",
        "Word2vec gives astonishing results. Its ability to maintain a semantic relationship is reflected in a classic example where if you have a vector for the word \"King\" and you remove the vector represented by the word \"Man\" from the \"King\" and add \"Woman\", you get a vector that is close to the vector \"Queen\":\n",
        "- King - Man + Woman = Queen\n",
        "\n",
        "Second example: \"dog\", \"puppy\" and \"pup\" are often used in similar situations, with similar surrounding words like \"good\", \"fluffy\" or \"cute\", and according to Word2vec they will therefore share a similar vector representation.\n",
        "\n",
        "In real applications, Word2vec models are created from billions of documents. For example, [Google's Word2Vec model](https://code.google.com/archive/p/word2vec/) is formed from 3 million words and phrases.\n",
        "\n",
        "GloVe is an extension of Word2vec. More information [here](https://nlp.stanford.edu/projects/glove/).\n",
        "\n",
        "Recently, more advanced models have been developed, such as [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) - Bidirectional Encoder Representations from Transformers-  and [GPT-3](https://en.wikipedia.org/wiki/GPT-3) - Generative Pre-trained Transformer 3. While Word2vec models represent tokens (word) with a single vector, BERT generates different output vectors for a same word when used in different context. You can find further readings on the topic at the end of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dr0Qae5Rr99"
      },
      "source": [
        "### Implementing Word2vec with Gensim\n",
        "\n",
        "We will implement Word2vec using the Gensim library. We are going to use a corpus of text extracted from Wikipedia by web scrapping. We first define a function to retrieve texts from a Wikipedia url:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4ONCENA5RHYX"
      },
      "outputs": [],
      "source": [
        "# Get texts from Wikipedia\n",
        "def get_text(url):\n",
        "    # Retrieve data\n",
        "    scrapped_data = urllib.request.urlopen(url)\n",
        "    article = scrapped_data.read()\n",
        "    # Parse data: # The text is contained in the HTML tag 'p'\n",
        "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "    paragraphs = parsed_article.find_all('p')\n",
        "    # Create a string with all the paragraphs\n",
        "    article_text = \"\"\n",
        "    for p in paragraphs:\n",
        "        article_text += p.text\n",
        "    return article_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LQ7fpc3Rr99"
      },
      "source": [
        "Let's get the Wikipedia articles on [Machine Learning](https://en.wikipedia.org/wiki/Machine_learning) and on [Artificial Intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence). This will be our corpus of documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzkzK_YHRr99"
      },
      "outputs": [],
      "source": [
        "# Get articles\n",
        "machine_learning = get_text(\"https://en.wikipedia.org/wiki/Machine_learning\")\n",
        "ai = get_text(\"https://en.wikipedia.org/wiki/Artificial_intelligence\")\n",
        "\n",
        "print(machine_learning[:705])\n",
        "print(ai[:741])\n",
        "\n",
        "# Group texts in list\n",
        "texts = [machine_learning, ai]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5pRQ4uxRr9-"
      },
      "source": [
        "Next, we preprocess out texts. We create a tokenizer function to lemmatize each token and remove stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "56Z0fduhRMb5"
      },
      "outputs": [],
      "source": [
        "# Create tokenizer function for preprocessing\n",
        "def spacy_tokenizer(text):\n",
        "\n",
        "    # Define stopwords, punctuation, and numbers\n",
        "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "    punctuations = string.punctuation +'–' + '—'\n",
        "    numbers = \"0123456789\"\n",
        "\n",
        "    # Create spacy object\n",
        "    mytokens = sp(text)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = ([ word.lemma_.lower().strip() for word in mytokens ])\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = ([ word for word in mytokens\n",
        "                 if word not in stop_words and word not in punctuations ])\n",
        "\n",
        "    # Remove sufix like \".[1\" in \"experience.[1\"\n",
        "    mytokens_2 = []\n",
        "    for word in mytokens:\n",
        "        for char in word:\n",
        "            if (char in punctuations) or (char in numbers):\n",
        "                word = word.replace(char, \"\")\n",
        "        if word != \"\":\n",
        "            mytokens_2.append(word)\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxsHnTVARr9-"
      },
      "source": [
        "Let's apply our function to tokenize our corpus of documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph0bwy3YRr9-"
      },
      "outputs": [],
      "source": [
        "# Tokenize texts\n",
        "processed_texts = [spacy_tokenizer(text) for text in texts]\n",
        "\n",
        "for processed_text in processed_texts:\n",
        "    print(processed_text[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3poKcxyLRr9_"
      },
      "source": [
        "Now that our text is preprocessed, we can train a Word2vec model. We use the `Word2Vec` module of Gensim ([Documentation](https://radimrehurek.com/gensim/models/word2vec.html)). As input, we provide the processed texts, i.e., a list of lists of tokens. In addition, we use as parameters:\n",
        "- `min_count`: minimum number of occurence of single word in corpus to be taken into account\n",
        "- `vector_size`: dimension of the vectors representing the tokens\n",
        "\n",
        "Once the model is trained, we can access to the mapping between words and embeddings with the method `.wv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYbGv4LbRRlZ"
      },
      "outputs": [],
      "source": [
        "# Word embedding\n",
        "word2vec = Word2Vec(processed_texts, min_count=2, vector_size=100)\n",
        "\n",
        "# Vocabulary\n",
        "vocab = word2vec.wv.key_to_index\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfiqRkeURr9_"
      },
      "source": [
        "Each token (word) is represented by a vector (array) of size 100:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynLxnGryTXKc"
      },
      "outputs": [],
      "source": [
        "# Vector\n",
        "v1 = word2vec.wv['intelligence']\n",
        "v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWPAWovGRr-B"
      },
      "source": [
        "In this space, we can explore the similarities between tokens. For instance, let's find the most similar words to \"intelligence\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCQRhSF5bPFI"
      },
      "outputs": [],
      "source": [
        "# Similar vectors/words\n",
        "sim_words = word2vec.wv.most_similar('intelligence')\n",
        "sim_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqxDGwfQRr-C"
      },
      "source": [
        "Or the similarity between two words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRBwWMjtcrOH"
      },
      "outputs": [],
      "source": [
        "# Similarity between two words\n",
        "print('The similarity between \"computer\" and \"argiculture\" is: ', word2vec.wv.similarity('computer', 'agriculture'))\n",
        "print('The similarity between \"computer\" and \"machine\" is: ', word2vec.wv.similarity('computer', 'machine'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwoQg0xEf9kN"
      },
      "source": [
        "Remarks:\n",
        "- There are other models than Word2Vec in Gensim. For instance, `Doc2Vec` is used to create a vectorised representation of a group of words (i.e., a document) taken collectively as a single unit (illustrated in the next section).\n",
        "- Gensim has many applications besides word embedding, see e.g., [topic modelling](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/). Feel free to explore the library!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c4GzcyCVHCf"
      },
      "source": [
        "### Using pretrained models\n",
        "Gensim comes with pretrained models. This means that you don't necessarily have to create your model from scartch in some cases. You can see how to use these pretrained models [here](https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models).\n",
        "\n",
        "In our example, we want to see the 10 most similar words to 'twitter' :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZLYg6aRV6OO"
      },
      "outputs": [],
      "source": [
        "# Download the \"glove-twitter-25\" embeddings\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
        "# Check the most similar terms to the word 'twitter'\n",
        "glove_vectors.most_similar('twitter')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-_83p20NjO-"
      },
      "source": [
        "### Your turn\n",
        "\n",
        "- Using the functions defined above, create a corpus of documents with the following Wikipedia articles: [Photovoltaics](https://en.wikipedia.org/wiki/Photovoltaics), [Wind turbine](https://en.wikipedia.org/wiki/Wind_turbine), [Hydropower](https://en.wikipedia.org/wiki/Hydropower), and [Nuclear power plant](https://en.wikipedia.org/wiki/Nuclear_power_plant). Do you know the share of each technology in the Swiss electricity mix? Check the [Electricity sector in Switzerland](https://en.wikipedia.org/wiki/Electricity_sector_in_Switzerland) for the answer..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "txadH-T7Rr-C"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toZHtMbRRr-C"
      },
      "source": [
        "- Preprocessing: Tokenize your corpus of documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "kTiVRKpjcrJT"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOZAfwuRRr-D"
      },
      "source": [
        "- What is the number of occurrences of the word \"energy\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz5BBzckeLAe"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUo2bdpFRr-D"
      },
      "source": [
        "- Create a Word2Vec representation of the article with a min_count of 1 and a vector size of 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3-1RbfX7fpzO"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXzm4tKpRr-D"
      },
      "source": [
        "- What are the 10 most similar words to \"electricity\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3p5FtNmRr-D"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3he4niYadil"
      },
      "source": [
        "## Application: Text Classification with TF-IDF\n",
        "\n",
        "In this section, we do an application on text classification to illustrate how the embedding can influence the accuracy of a classifier.\n",
        "\n",
        "Our goal is to classify consumer finance complaints into 12 pre-defined categories using:\n",
        "- TF-IDF and logistic regression\n",
        "\n",
        "We use the same tokenizer function, train-test split, classification algorithm, etc. The only difference is the mathematical representation (i.e., the vectorization from the tokens) of the complaints.\n",
        "\n",
        "This application was inspired by the articles published by Susan Li on Towards Data Science:\n",
        "- [Multi-Class Text Classification with Scikit-Learn](https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAdIAkTNRr-D"
      },
      "source": [
        "### Load and clean data\n",
        "\n",
        "We work with a sample of a large dataset from Data.gov that can be found [here](https://catalog.data.gov/dataset/consumer-complaint-database)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiRTQshsihUu"
      },
      "outputs": [],
      "source": [
        "# Load data from GitHub\n",
        "path = \"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/refs/heads/master/Labs/05-text-analytics/data/complaints_sample.csv\"\n",
        "df = pd.read_csv(path, index_col=0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cys9L2XidZsq"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyg-rztyPJ6e"
      },
      "source": [
        "The data set includes 18 columns and 9101 rows describing consumer complaints about financial products. In this case, we want to predict the `Product` category based on the text of the complaint (i.e., `Consumer complaint narrative`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "9mgx17mnpy2s"
      },
      "outputs": [],
      "source": [
        "# Select columns of interest\n",
        "data = df[[\"Product\", \"Consumer complaint narrative\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhB01GG6QXVP"
      },
      "source": [
        "Around 2/3 of the complaints are null values. They are not useful for the prediction so we drop them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbno4wghqCNH"
      },
      "outputs": [],
      "source": [
        "# Drop NaN\n",
        "print(data.isnull().sum())\n",
        "data = data.dropna().reset_index(drop=True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgb3yc3KqCLr"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCasthTDQrYb"
      },
      "source": [
        "We end up with 3137 complaints for which we would like to predict the product concerned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc8z_6SkTIx2"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXpBtEOyRr-F"
      },
      "source": [
        "As always, we start by an EDA to better understand our data and inform our analysis. First note that we are dealing with a dataset containing a large number of words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JQ8GFf0ihb3"
      },
      "outputs": [],
      "source": [
        "# Total number of words - over 600,000\n",
        "words_number = data['Consumer complaint narrative'].apply(lambda x: len(x.split(' '))).sum()\n",
        "print(f'The complaints contain {words_number} words.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADcRg1jNRr-F"
      },
      "source": [
        "Let's extract a sample to see how the complaints look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppQTFaZ6t9dy"
      },
      "outputs": [],
      "source": [
        "# Sample\n",
        "data['Consumer complaint narrative'].sample().values[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDpSTJyZQ6O5"
      },
      "source": [
        "The data has been anonymized (i.e., names, dates, IDs, etc. have been replaced by XXXX).\n",
        "\n",
        "Next, note that the classes (products) are imbalanced:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdDpzPPouaSN"
      },
      "outputs": [],
      "source": [
        "# Imbalanced dataset\n",
        "data.Product.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDon6qpmRMzo"
      },
      "source": [
        "There are 17 categories. We group some of them together (e.g. \"Credit card\", \"Prepaid card\", and \"Credit or prepaid card\") because they are sub-categories of each other. We end up with 12 categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er1kzzv3vBSt"
      },
      "outputs": [],
      "source": [
        "# Merge categories\n",
        "dic_replace = {'Credit reporting':'Credit reporting, credit repair services, or other personal consumer reports',\n",
        "               'Credit card':'Credit card or prepaid card',\n",
        "               'Payday loan':'Payday loan, title loan, or personal loan',\n",
        "               'Money transfers':'Money transfer, virtual currency, or money service',\n",
        "               'Prepaid card':'Credit card or prepaid card',\n",
        "               'Virtual currency':'Money transfer, virtual currency, or money service'}\n",
        "data.replace(dic_replace, inplace=True)\n",
        "data.Product.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y_4RF_VRr-G"
      },
      "source": [
        "Let's visualize the number of observation per product using a bar plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEeOpaMQtgAA"
      },
      "outputs": [],
      "source": [
        "# Plot number of complaints per category\n",
        "cnt_pro = data['Product'].value_counts()\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.countplot(x=data['Product'], order = cnt_pro.index)\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "plt.xlabel('Product', fontsize=12)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG9sgsx0Rr-G"
      },
      "source": [
        "Finally, let's compute the base rate, i.e., the accuracy obtained using a naive classifier that predicts that all observations are from the largest class (\"Credit reporting, credit repair services, or other personal consumer reports\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JdF1razudYJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Base rate\n",
        "base_rate = round(len(data[data.Product == \"Credit reporting, credit repair services, or other personal consumer reports\"]) / len (data), 4)\n",
        "print(f'The base rate is: {base_rate*100:0.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBr6OSytTLgD"
      },
      "source": [
        "### Classification using TF-IDF and Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55qGlGQMRr-H"
      },
      "source": [
        "We first define our training and test set, using the `train_test_split` module of sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ANzpWDFjRr-H"
      },
      "outputs": [],
      "source": [
        "# Select features\n",
        "X = data['Consumer complaint narrative'] # Features we want to analyze\n",
        "ylabels = data['Product']                # Labels we test against\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e3f8HiTRr-H"
      },
      "source": [
        "Next, we use the `TfidfVectorizer` class of sklearn for the word embedding. Since we are dealing with very specific data (e.g., the anonymization process generated non-standard sequence of characters), we are defining our own tokenizer function, which we can use as parameter of `TfidfVectorizer` instead of the default one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "vXHnUbjerLPD"
      },
      "outputs": [],
      "source": [
        "# Define tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "\n",
        "    punctuations = string.punctuation\n",
        "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = sp(sentence)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # Remove anonymous dates and people\n",
        "    mytokens = [ word.replace('xx/', '').replace('xxxx/', '').replace('xx', '') for word in mytokens ]\n",
        "    mytokens = [ word for word in mytokens if word not in [\"xxxx\", \"xx\", \"\"] ]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDE3sJ_IRr-H"
      },
      "source": [
        "As other parameters of `TfidfVectorizer`, we are using token and pair of tokens (`ngram_range = (1,2)`) and we ignore terms that have a document frequency strictly lower than 5 (`min_df = 5`).\n",
        "\n",
        "Note that we also rely on the `Pipeline` module of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)) to sequentially apply models, first the vectorizer, then the classifier. We also time our training (it might take a few minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tTbx3oswIan"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Define vectorizer\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), tokenizer=spacy_tokenizer)\n",
        "\n",
        "# Define classifier\n",
        "classifier = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBQl6oFvRr-H"
      },
      "source": [
        "Finally, we predict the test set values and evalute the performance of our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "dC3Pbqxlxod2"
      },
      "outputs": [],
      "source": [
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L0On28bRr-I"
      },
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "\n",
        "## Accuracy\n",
        "accuracy_tfidf = round(accuracy_score(y_test, y_pred), 4)\n",
        "print(f'The accuracy using TF-IDF is: {accuracy_tfidf*100:0.2f}%')\n",
        "\n",
        "## Confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "fig, ax = plt.subplots(figsize=(8,7))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cJeN3YmsV4p"
      },
      "source": [
        "## Introduction to Hugginface and sentiment analysis\n",
        "Another powerful tool is the Hugginface library. You can acces the documentation [here](https://huggingface.co/docs)\n",
        "\n",
        "In our example, we will try to implement a sentiment analysis on a text. A sentiment analysis tries to classify a text into three categories: positive, neutral and negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX0IIJafXfTI"
      },
      "source": [
        "### Implementation of Hugginface\n",
        "First, we have to load the model that we want to use. There are many differents models that you can find [here](https://huggingface.co/docs/transformers/main_classes/model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade transformers huggingface_hub tensorflow\n",
        "%pip install --upgrade accelerate\n",
        "%pip install --upgrade tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scyNna-dXdL_"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Test with a simpler model\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv3rvKVMXw8z"
      },
      "source": [
        "Then, we will try to classify these two sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elisWT-EX2bR"
      },
      "outputs": [],
      "source": [
        "data = [\"I love you\", \"I hate you\"]\n",
        "sentiment_pipeline(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PacdclyX94T"
      },
      "source": [
        "This is it! We are already done and we see that the first sentence is obviously classified as *positive* and that the second one is *negative*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FREWVtLbYde7"
      },
      "source": [
        "### Your turn !\n",
        "Try to use this [pipeline](https://huggingface.co/SamLowe/roberta-base-go_emotions) to classify the emotions of this sentence : \"I am not having a great day\":\n",
        "- task : 'text-classification'\n",
        "- model : 'SamLowe/roberta-base-go_emotions'\n",
        "- top_k : None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJLgE8mvaTw3"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiWToag0YTw2"
      },
      "outputs": [],
      "source": [
        "# print the outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfBeVuIpagVu"
      },
      "source": [
        "What are the top three emotions related to this sentence : \"I don't like clowns, I am afraid of them\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2eQGbVJZIwK"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
