{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ixVvdJcCCckE",
   "metadata": {
    "id": "ixVvdJcCCckE"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Labs/03-classification/Week_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc4475-7f7d-4457-8762-bb031a0629b2",
   "metadata": {
    "id": "15dc4475-7f7d-4457-8762-bb031a0629b2"
   },
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import to load arff file from url\n",
    "from scipy.io import arff\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "# Sklearn import\n",
    "from sklearn.model_selection import train_test_split # Splitting the data set\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler # Normalization and standard scaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label and 1-hot encoding\n",
    "from sklearn.linear_model import LogisticRegression # Logistic regression model\n",
    "from sklearn.linear_model import LogisticRegressionCV # Logistic regression with cross-validation\n",
    "from sklearn.metrics import accuracy_score  # Accuracy\n",
    "from sklearn.metrics import confusion_matrix # Confusion matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score  # Precision, recall, and f1 score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree # Decision Trees\n",
    "from sklearn.model_selection import GridSearchCV   # Grid search for cross validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844a8c0-078c-4946-8927-e570469c601c",
   "metadata": {
    "id": "a844a8c0-078c-4946-8927-e570469c601c"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392fab0-671b-45ea-b27a-61a797c0e090",
   "metadata": {
    "id": "4392fab0-671b-45ea-b27a-61a797c0e090"
   },
   "source": [
    "## Content\n",
    "\n",
    "The goal of this walkthrough is to provide you with insights on classification, focusing on one technique called logistic regression. After presenting the main concepts, you will be introduced to the techniques to implement the algorithms in Python. Finally, it will be your turn to practice, using an application on forest fires.\n",
    "\n",
    "The notebook is organized as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ESZqmq0kKPWX",
   "metadata": {
    "colab_type": "toc",
    "id": "ESZqmq0kKPWX"
   },
   "source": [
    ">[Classification](#scrollTo=a844a8c0-078c-4946-8927-e570469c601c)\n",
    "\n",
    ">>[Content](#scrollTo=4392fab0-671b-45ea-b27a-61a797c0e090)\n",
    "\n",
    ">>[Background](#scrollTo=7763984b-c9d6-43b0-ba06-fd749431aa32)\n",
    "\n",
    ">>>[1.1 Objective](#scrollTo=7763984b-c9d6-43b0-ba06-fd749431aa32)\n",
    "\n",
    ">>>[1.2 Examples of classification](#scrollTo=5292f5e7-73c2-41f3-af5f-17d4dcbe9fd9)\n",
    "\n",
    ">>>[1.3 Logistic regression model](#scrollTo=251e9265-3d22-4a07-8838-23b32fce472f)\n",
    "\n",
    ">>>>[Logistic Loss function](#scrollTo=1edb4df8-98f9-4903-ae42-65e57e0d8a84)\n",
    "\n",
    ">>[Implementation](#scrollTo=ce878d70-296b-4ce2-9a05-a5d0410d79e1)\n",
    "\n",
    ">>>[2.1 Load and discover dataset](#scrollTo=751923ff-7b61-4ab9-af76-1b6b98b9e1f7)\n",
    "\n",
    ">>>[2.2 Splitting the dataset](#scrollTo=2da1d609-f411-4205-9026-f452795d8dac)\n",
    "\n",
    ">>>[2.3 Rescaling](#scrollTo=d607ddfc-bfab-4834-a4fb-a54e93b12114)\n",
    "\n",
    ">>>[2.4 Building and training our classifier](#scrollTo=4f0a1b43-4e06-4b70-8a83-c5e58d99e2d7)\n",
    "\n",
    ">>>[2.5 Using the classifier to make prediction](#scrollTo=4185d54b-63ca-41d0-a48a-2b4ea7930baa)\n",
    "\n",
    ">>>[2.6 Evaluating our classifier](#scrollTo=0dc6921d-d075-4f8c-9ef8-e7ebdb150513)\n",
    "\n",
    ">>>>[2.6.1 Accuracy](#scrollTo=c9a3e15a-dd3f-40ec-afa7-782e5ba2fc5c)\n",
    "\n",
    ">>>>[2.6.2 Default Rate](#scrollTo=d5a03395)\n",
    "\n",
    ">>>>[2.6.3 Confusion matrix](#scrollTo=00d4d413-7a87-4590-a54f-cc5b6d44bb8e)\n",
    "\n",
    ">>>>[2.6.4 Precision and Recall](#scrollTo=c22860d3-e863-4045-b487-9ffbdc36093c)\n",
    "\n",
    ">>>>[2.6.5 Classification Report](#scrollTo=3a44c38e)\n",
    "\n",
    ">>>[2.7 Adding cross-validation](#scrollTo=3592471e-12a9-4ef8-9085-d7b58cefeded)\n",
    "\n",
    ">>[Your turn !](#scrollTo=h28lWAXVDtu7)\n",
    "\n",
    ">>>[Discover your dataset](#scrollTo=6ea0a3ee-2fda-45e2-bfbe-98f601ad0671)\n",
    "\n",
    ">>>[Multi-features logistic regression](#scrollTo=ypmEjcXnN1yQ)\n",
    "\n",
    ">>[Decision Trees](#scrollTo=3ee02d36-e94a-43b6-a3b6-c3d6eaac1741)\n",
    "\n",
    ">>>[3.1 Background](#scrollTo=14273048-92d9-48a1-a974-71edda505929)\n",
    "\n",
    ">>>>[3.1.1 Intuition](#scrollTo=28ba57c8)\n",
    "\n",
    ">>>>[3.1.2 Decision criteria](#scrollTo=eea9938a)\n",
    "\n",
    ">>>[3.2 Implementation](#scrollTo=8bed40bd-9056-405a-b89b-15e479752fd0)\n",
    "\n",
    ">>>>[3.2.1 Building, training, and evaluating our classifier](#scrollTo=59fa75a7-714d-4128-8ab2-93730c5f7c79)\n",
    "\n",
    ">>>[3.3 Tuning parameters with cross-validation](#scrollTo=95489c7f-e616-47a0-8908-8c26c2eca5e9)\n",
    "\n",
    ">>>[3.4 Visualize tree](#scrollTo=76e13259-d288-4de5-9c2e-32e9c2127848)\n",
    "\n",
    ">>[Your turn !](#scrollTo=a8367747-4cbe-45e1-8003-b4ba277f2741)\n",
    "\n",
    ">>[k-Nearest Neighbords (kNN)](#scrollTo=e43f558b)\n",
    "\n",
    ">>>[4.1 Intuition](#scrollTo=748919c8)\n",
    "\n",
    ">>>[4.2 Implementation](#scrollTo=7b0ff6e8)\n",
    "\n",
    ">>[Encoding categorical variable](#scrollTo=93cf961d)\n",
    "\n",
    ">>>[One-Hot Encoding](#scrollTo=96f87619)\n",
    "\n",
    ">>>[Label Encoding](#scrollTo=029ea815)\n",
    "\n",
    ">>>[Implementation](#scrollTo=19a12e8c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763984b-c9d6-43b0-ba06-fd749431aa32",
   "metadata": {
    "id": "7763984b-c9d6-43b0-ba06-fd749431aa32"
   },
   "source": [
    "## 1. Background <a class=\"anchor\" id=\"Background\"></a>\n",
    "\n",
    "### 1.1 Objective <a class=\"anchor\" id=\"Objective\"></a>\n",
    "\n",
    "We now move from regression another important branch of machine learning: classification. As the name indicates, the idea is to classify items of a dataset into **predefined classes** for which labelled data is already available.\n",
    "\n",
    "Classification is similar to regression, but instead of predicting a continuous target, classification algorithms attempt to apply a discrete number of labels or classes to each observation.\n",
    "\n",
    "While in a regression the targets are generally numerical and continuous-valued, oftentimes the targets in classification are categorical.  \n",
    "\n",
    "However, classification can be applied in settings where the target is numerical. For example, we might want to predict whether the unemployment rate for a country will be low, medium, or high, but do not care about the actual number. In such cases, determining the \"optimal\" way to categorize our target variable might require some creativity, and it is important to properly justify our assumptions.\n",
    "\n",
    "Finally, note that many problems can be written either as classification or regression. Hence, many ML algorithms have variants that perform regression or classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292f5e7-73c2-41f3-af5f-17d4dcbe9fd9",
   "metadata": {
    "id": "5292f5e7-73c2-41f3-af5f-17d4dcbe9fd9"
   },
   "source": [
    "### 1.2 Examples of classification\n",
    "\n",
    "- Image classification: e.g., is this a cat or not?\n",
    "- Audio classification: identify bird species from birdsong\n",
    "- Labeling emails: is it a spam or not?\n",
    "- Risky or safe loan application: should a bank provide a loan to applicant or not?\n",
    "- Prediction of customer behaviour: will a customer buy this new product or not?  \n",
    "- Prediction of economic performance of a country: will there be a recession or not?  \n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e9265-3d22-4a07-8838-23b32fce472f",
   "metadata": {
    "id": "251e9265-3d22-4a07-8838-23b32fce472f",
    "tags": []
   },
   "source": [
    "### 1.3 Logistic regression model\n",
    "\n",
    "Suppose we have *n* observations of an outcome $\\boldsymbol{y}$ and *d* associated features $\\boldsymbol{x_1}$, $\\boldsymbol{x_2}$, ... , $\\boldsymbol{x_d}$ (note that $\\boldsymbol{y}$, $\\boldsymbol{x_1}$, ..., $\\boldsymbol{x_d}$ are vectors):\n",
    "\n",
    "| | Outcome | Feature 1 | Feature 2 | ... | Feature d |\n",
    "|:-------|:----------:|:----------:|:----------:|:----------:|:----------:|\n",
    "| Observation 1 | $y_1$ | $x_{11}$ | $x_{12}$ | ... | $x_{1d}$ |\n",
    "| Observation 2 | $y_2$ | $x_{21}$ | $x_{22}$ | ... | $x_{2d}$ |\n",
    "| ... | ... | ... | ... | ... | ... |\n",
    "| Observation n | $y_n$ | $x_{n1}$ | $x_{n2}$ | ... | $x_{nd}$ |\n",
    "\n",
    "We will focus on binary classification for now. In other words, our outcome can take two values, 0 and 1, which represent two classes (e.g., cat or dog, spam email or not, risky or safe loan, etc.).\n",
    "\n",
    "Remember when we did multivariate linear regression, we assumed that our model function $f_{\\text{mv}}$, i.e., our prediction, was a linear combination of our features. For each observation $i$, we assumed:\n",
    "$$f_{\\text{mv}}(\\boldsymbol{X_{i*}}, \\boldsymbol{w}):=w_0 + w_1 x_{i,1} +  w_2 x_{i,2} + ... +  w_d x_{i,d}$$\n",
    "with $\\boldsymbol{w}=(w_0, w_1, ..., w_d)$ the vector of weights, and $\\boldsymbol{X}=[\\boldsymbol{x_1}$, ... , $\\boldsymbol{x_d}]$ the matrix of feature variables.\n",
    "\n",
    "For each observation, our true outcome was $y_i = f_{\\text{mv}}(\\boldsymbol{X_{i*}}, \\boldsymbol{w}) + \\epsilon_i$, and our goal was to minimize the errors.\n",
    "\n",
    "In this setting, our model function $f_{\\text{mv}}$ can take any values. It is thus suited when our outcome is continuous. However, with binary classification, we are dealing with discrete values, and more precisely with 0 and 1. How can we modify our model to obtain better prediction?\n",
    "\n",
    "The idea of logistic regression is to transform the predictions obtained with a linear regression such that the predictions are between 0 and 1. To do so, we rely on the [Sigmoid (logistic) function](https://en.wikipedia.org/wiki/Sigmoid_function):\n",
    "\n",
    "$$S(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "<center>\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1280px-Logistic-curve.svg.png' width=\"300\">\n",
    "</center>\n",
    "\n",
    "Source: Qef, from [Wikipedia Logistic Curve plot](https://commons.wikimedia.org/wiki/File:Logistic-curve.svg)\n",
    "\n",
    "With logistic regression, we apply the sigmoid function to the output of the multivariate regression model. Let $f_{\\text{logi}}$ be the prediction function of a logistic regression, we have:\n",
    "\n",
    "$$f_{\\text{logi}}(\\boldsymbol{X_{i*}}, \\boldsymbol{w}):= \\frac{1}{1 + e^{-(w_0 + w_1 x_{i,1} +  w_2 x_{i,2} + ... +  w_d x_{i,d})}}$$\n",
    "\n",
    "$f_{\\text{logi}}$ represents the probability that a given observation belongs to class 1, i.e., $y_i=1$:\n",
    "- We predict that the observation belongs to class 1 when $f_{\\text{logi}}(\\boldsymbol{X_{i*}}, \\boldsymbol{w}) \\geq 0.5$, i.e., when $w_0 + w_1 x_{i,1} +  w_2 x_{i,2} + ... +  w_d x_{i,d} \\geq 0$;\n",
    "- Reciprocally, we predict that the observation belongs to class 0 when $f_{\\text{logi}}(\\boldsymbol{X_{i*}}, \\boldsymbol{w})<0.5$, i.e., $w_0 + w_1 x_{i,1} +  w_2 x_{i,2} + ... +  w_d x_{i,d}<0$.\n",
    "\n",
    "Now our problem is the same as before: we want to minimize the errors of our model, learning the weights $w_0$, $w_1$, ..., $w_d$ from our data. To do so, we are minimizing our loss function... but which one? We will explore one option below.\n",
    "\n",
    "*Note*: In the case of multiclass logistic regression, the idea is the same: we need to transform the output of our multivariate model. However, instead of using the sigmoid function, we are applying the [Softmax function](https://en.wikipedia.org/wiki/Softmax_function), a generalization of the sigmoid function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb4df8-98f9-4903-ae42-65e57e0d8a84",
   "metadata": {
    "id": "1edb4df8-98f9-4903-ae42-65e57e0d8a84"
   },
   "source": [
    "#### Logistic Loss function\n",
    "\n",
    "For linear regression, we used the Least Squared Error as loss function:\n",
    "\n",
    "$$ \\min_{\\boldsymbol{w}} \\sum_{i=1}^n (y_i - f_{\\text{mv}}(\\boldsymbol{X_{i*}}, \\boldsymbol{w}))^2 $$\n",
    "\n",
    "\n",
    "Can we use the same for logistic regression? No! Indeed, using Least Squared Error with our new prediction function $f_{\\text{mv}}$ would result in a non-convex graph, which is not ideal for our minimization problem since we could be stuck in local minima:\n",
    "\n",
    "<center>\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*3o9_XoQP4TaceYPRZVHlxg.png' width=\"300\">\n",
    "</center>\n",
    "\n",
    "Source: Issam Laradji, [Non-convex Optimization](https://www.cs.ubc.ca/labs/lci/mlrg/slides/non_convex_optimization.pdf)\n",
    "\n",
    "So which loss function can we use? Ideally, we want to assign more punishment when predicting 1 while the actual value is 0 and when predicting 0 while the actual value is 1. One such function is the... **Logistic Loss**:\n",
    "\n",
    "$$L(\\boldsymbol{y}, \\boldsymbol{X}, \\boldsymbol{w})= -\\frac{1}{n} \\sum_{i=1}^n [y_i \\log(f_{\\text{logi}}(\\boldsymbol{X_{i*}}, \\boldsymbol{w})) + (1-y_i) \\log(1-f_{\\text{logi}}(\\boldsymbol{X_{i*}}, \\boldsymbol{w}))] $$\n",
    "\n",
    "Let's decompose our function to understand a bit more how it works. For each observation $i$, the cost is:\n",
    "\n",
    "$$\\text{Cost}_i = - y_i \\log(f_{\\text{logi}}(\\boldsymbol{X_{i*}}, \\boldsymbol{w})) - (1-y_i) \\log(1-f_{\\text{logi}}(\\boldsymbol{X_{i*}}, \\boldsymbol{w}))$$\n",
    "\n",
    "- When $y_i = 1$, $\\text{Cost}_i = - \\log(f_{\\text{logi}}(\\boldsymbol{X_{i*}}, \\boldsymbol{w}))$. Hence, if our predicted probability is 1, we have $\\text{Cost}_i=0$, i.e., no cost. However, when our predicted probability is approaching 0, our cost goes to infinity (because the logarithm goes to minus infinity when we get closer to zero).\n",
    "- When $y_i = 0$, $\\text{Cost}_i = - \\log(1-f_{\\text{logi}}(\\boldsymbol{X_{i*}}, \\boldsymbol{w}))$, and it works the other way around. If our predicted probability is zero, the cost is zero, but when our predicted probability is approaching 1, our cost goes to infinity.\n",
    "\n",
    "<center>\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_NeTem-yeZ8Pr9cVUoi_HA.png' width=\"400\">\n",
    "</center>\n",
    "\n",
    "Source: Shuyu Luo, [Loss Function (Part II): Logistic Regression](https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11)\n",
    "\n",
    "The Logistic Loss not only punishes errors with a very large cost, it is also convex. Hence, we can still apply Gradient Descent, Newton's Method, and other optimization algorithms!\n",
    "\n",
    "To learn more:\n",
    "- [Loss Function (Part II): Logistic Regression](https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11), by Shuyu Luo, Published in Towards Data Science\n",
    "- [Understanding the log loss function](https://medium.com/analytics-vidhya/understanding-the-loss-function-of-logistic-regression-ac1eec2838ce), by Susmith Reddy, Published in Analytics Vidhya\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce878d70-296b-4ce2-9a05-a5d0410d79e1",
   "metadata": {
    "id": "ce878d70-296b-4ce2-9a05-a5d0410d79e1"
   },
   "source": [
    "## 2. Implementation\n",
    "\n",
    "We are already familiar with the **sklearn** library, when we implemented regression algorithms last week ([Documentation](https://scikit-learn.org/stable/index.html)). We will keep on using this library to implement a logistic regression.\n",
    "\n",
    "For the walkthough we will use a **dataset on wine quality**. As usual, you can find it in the /data folder.\n",
    "\n",
    "<img src='https://assets.pbimgs.com/pbimgs/rk/images/dp/wcm/202145/0002/schott-zwiesel-classico-wine-glasses-c.jpg' width=\"300\">\n",
    "\n",
    "The wine data set consists of 11 different parameters of wine such as alcohol content, acidity, and pH, which were measured for several wine samples from the North of Portugal.\n",
    "\n",
    "Source: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In *Decision Support Systems*, Elsevier, 47(4):547-553, 2009. Dataset obtained from UCI Machine Learning repository, [Wine Quality Data Set](https://archive.ics.uci.edu/ml/datasets/wine+quality).\n",
    "\n",
    "These wines were derived from different cultivars; therefore there have different quality, as a score between 0 and 10. We grouped the wines into two quality classes: 0 and 1, representing respectively  \"poor quality\" (score 0-5), and \"good quality\" (score 6-10).\n",
    "\n",
    "Our goal here is to find a model that can predict the class of wine given the 11 measured parameters, and find out the major differences among the two classes.\n",
    "\n",
    "Ok, let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751923ff-7b61-4ab9-af76-1b6b98b9e1f7",
   "metadata": {
    "id": "751923ff-7b61-4ab9-af76-1b6b98b9e1f7"
   },
   "source": [
    "### 2.1 Load and discover dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c288953-a267-4f5c-9271-40079cb5fea5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "0c288953-a267-4f5c-9271-40079cb5fea5",
    "outputId": "38313177-7a64-46cf-ad37-a1f0d69979a4"
   },
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/refs/heads/master/Labs/03-classification/data/wine-quality-red.csv\"\n",
    "wines = pd.read_csv(url).drop_duplicates().dropna() # drop duplicates and NaN values\n",
    "\n",
    "# Display a sample of the data\n",
    "display(wines.head())\n",
    "\n",
    "# Print columns\n",
    "print(wines.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef78718-ca7e-47fc-9099-348f12837f8c",
   "metadata": {
    "id": "aef78718-ca7e-47fc-9099-348f12837f8c"
   },
   "source": [
    "Note that we only have numerical variables, and thus won't need to encode categorical variables.\n",
    "\n",
    "However, we will need to rescale our features since, for instance, chlorides values are lower than 1 while sulfur dioxide can attain a value of 289:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e5eaf-6304-4801-abd4-b629e61de2c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "185e5eaf-6304-4801-abd4-b629e61de2c8",
    "outputId": "37c6fa32-3e14-42ec-bfa9-29f0c659ea84"
   },
   "outputs": [],
   "source": [
    "wines.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409233c-f2d4-48fa-a560-c9f45b0c4c64",
   "metadata": {
    "id": "0409233c-f2d4-48fa-a560-c9f45b0c4c64"
   },
   "source": [
    "We now define our features - all wine parameters - and our target variable - the wine quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66708daf-fce1-4eb2-84ca-709c3b556ff1",
   "metadata": {
    "id": "66708daf-fce1-4eb2-84ca-709c3b556ff1"
   },
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = wines.drop(columns='quality')\n",
    "y = wines['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1dffca-cb02-4261-81df-b0729f88d975",
   "metadata": {
    "id": "6e1dffca-cb02-4261-81df-b0729f88d975"
   },
   "source": [
    "We know check how many observations we have for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5976a37-8e49-4f00-8bfb-76f3a5060e36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5976a37-8e49-4f00-8bfb-76f3a5060e36",
    "outputId": "896d0632-1018-4ac2-9afc-6f5f686a5879"
   },
   "outputs": [],
   "source": [
    "# Count the number of observations (rows) corresponding to each value\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d8f2c-5bd6-441c-96a8-3d1965171e68",
   "metadata": {
    "id": "a16d8f2c-5bd6-441c-96a8-3d1965171e68"
   },
   "source": [
    "We have 719 \"good\" wines and 640 \"poor\" quality wines. The number of observations for each class influence the quality of our predictions. Here, our dataset is reasonably balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1d609-f411-4205-9026-f452795d8dac",
   "metadata": {
    "id": "2da1d609-f411-4205-9026-f452795d8dac"
   },
   "source": [
    "### 2.2 Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e022dc7-61eb-41cd-b112-8f099274231c",
   "metadata": {
    "id": "2e022dc7-61eb-41cd-b112-8f099274231c"
   },
   "source": [
    "As always, the first step is to split our data into random training and test subsets. Recall that the training set is used to learn the parameters of our model while the test set is used to evaluate our predictions.\n",
    "\n",
    "We use the `train_test_split` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) of sklearn, imported with the following line of code (already done at the beginning of the notebook):\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "The test size here is of 25% of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed44fb-c22b-4bc3-8c28-5bc1d2f469cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bed44fb-c22b-4bc3-8c28-5bc1d2f469cb",
    "outputId": "3a700c17-bdb9-423d-a52c-cabc524a8583"
   },
   "outputs": [],
   "source": [
    "#Split data set into a train and a test data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, shuffle=True)\n",
    "\n",
    "print(f\"The training set has {X_train.shape[0]} observations, and the test set has {X_test.shape[0]} observations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d607ddfc-bfab-4834-a4fb-a54e93b12114",
   "metadata": {
    "id": "d607ddfc-bfab-4834-a4fb-a54e93b12114"
   },
   "source": [
    "### 2.3 Rescaling\n",
    "\n",
    "When we have a dataset with features that have very distinct ranges, we might get biased results. We want the features to be in the same or similar range, which also helps the interpretation of the model parameters (weights).\n",
    "\n",
    "We therefore **normalize** the data. It involves transforming all values for a specific attribute so that they fall within a small specified range. We can use `StandardScaler()`, ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)) `MinMaxScaler()` ([Documentation](https://scikit-learn.org/0.15/modules/generated/sklearn.preprocessing.MinMaxScaler.html)) or others for normalization.\n",
    "\n",
    "In our example we will normalize both our **train AND test data** using `MinMaxScaler()`. Here is the import line:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "```\n",
    "\n",
    "For each observation $x_{ij}$, with $i$ the observation (row) and $j$ the feature (column), the MinMax Scaler applies the following transformation:  \n",
    "$$x_{scaled,ij}=\\frac{x_{ij} - \\min(\\boldsymbol{x_j})}{\\max(\\boldsymbol{x_j})-\\min(\\boldsymbol{x_j})}$$\n",
    "\n",
    "**IMPORTANT**: When you normalize the train data, you need to do the same modification (here normalization) to the test data. In other words, you train your scaler on your training set, and apply the same transformation to the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a107b7-f1cb-4b31-a20f-ecdc9b61b7e6",
   "metadata": {
    "id": "10a107b7-f1cb-4b31-a20f-ecdc9b61b7e6"
   },
   "outputs": [],
   "source": [
    "# Define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "scaler.fit(X_train) # here the scaler learns the min and max of each attribute from the training set\n",
    "\n",
    "# Transform the train and the test set\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "## Note that the fit and transform steps can be merged into one for the training set:\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a1b43-4e06-4b70-8a83-c5e58d99e2d7",
   "metadata": {
    "id": "4f0a1b43-4e06-4b70-8a83-c5e58d99e2d7"
   },
   "source": [
    "### 2.4 Building and training our classifier\n",
    "\n",
    "To predict the class of our target variable we use a logistic regression. The sklearn module is called `LogisticRegression()` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). Here is the import line:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "```\n",
    "\n",
    "Note that L2-regularization is applied by default. By specifying the argument *penalty*, you can specify the regularization techniques, namely 'l1', 'l2', 'elasticnet', or None.\n",
    "\n",
    "You can also specify the *solver*. By default, 'lbfgs' is used, which stands for [Limited-memory Broyden–Fletcher–Goldfarb–Shanno](https://en.wikipedia.org/wiki/Limited-memory_BFGS).\n",
    "Note that the choice of the algorithm depends on the penalty chosen. You can refer to the documentation for insights on the choice of solver/penalty depending on your problem and data.\n",
    "\n",
    "**A short note on solver:**\n",
    "L-BFGS approximates the Broyden–Fletcher–Goldfarb–Shanno algorithm ([BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)), which is based on [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization), an alternative to Gradient Descent. While the Gradient Descent rely on the gradient (first-order derivatives) to update our parameters, Newton's method also makes use of the [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix), i.e., the second-order derivatives. Newton's method generally converges faster than Gradient Descent. However, Newton's method is computationally-expensive and the Hessian might not even exist. Hence, numerical methods called [Quasi-Newton](https://en.wikipedia.org/wiki/Quasi-Newton_method), such as BFGS, have been developed to solve optimization problems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e909b23-c4db-408d-8bd9-3267b8281632",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "6e909b23-c4db-408d-8bd9-3267b8281632",
    "outputId": "4cfd68c6-5f74-4ede-cdc9-83a803f562fd"
   },
   "outputs": [],
   "source": [
    "# 1. Set up our model\n",
    "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# 2. Fit our model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e935629-c646-44e6-8491-ecbf664b8e75",
   "metadata": {
    "id": "7e935629-c646-44e6-8491-ecbf664b8e75"
   },
   "source": [
    "After fitting the model, we can easily retrieve the values of the different weights coefficients: the intercept with `.intercept_`, and the weights of each feature with `.coef_.flatten()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a615f08f-a1b1-40df-b897-d2e01edc9b6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "a615f08f-a1b1-40df-b897-d2e01edc9b6e",
    "outputId": "183ddb00-f01c-4a9e-be0a-26bf59302779"
   },
   "outputs": [],
   "source": [
    "# Dataframe with the intercept and coefficients (weights) of the logistic model\n",
    "model_coeff = pd.DataFrame(np.concatenate((model.intercept_, model.coef_.flatten())),\n",
    "                     index=[\"Intercept\"] + list(X.columns.values),\n",
    "                     columns=['Coefficients logistic model'])\n",
    "model_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e583fd-4373-47b2-a70b-23ad6e621981",
   "metadata": {
    "id": "41e583fd-4373-47b2-a70b-23ad6e621981"
   },
   "source": [
    "It seems like the level of alcohol and volatile acidity were the most important features to predict the wine quality, at least in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4185d54b-63ca-41d0-a48a-2b4ea7930baa",
   "metadata": {
    "id": "4185d54b-63ca-41d0-a48a-2b4ea7930baa"
   },
   "source": [
    "### 2.5 Using the classifier to make prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554cc06-ed5f-4b9d-a477-9943e551cf7a",
   "metadata": {
    "id": "5554cc06-ed5f-4b9d-a477-9943e551cf7a"
   },
   "source": [
    "Once our model has been trained, we can use `predict()` to predict new values. We predict the values from the test set to then evaluate the model, estimating the accuracy of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883ebcd-bcfc-4bab-85ef-ad0ea8d78778",
   "metadata": {
    "id": "2883ebcd-bcfc-4bab-85ef-ad0ea8d78778"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1941fb-44bb-4abf-b9d8-0555a499fa6d",
   "metadata": {
    "id": "8e1941fb-44bb-4abf-b9d8-0555a499fa6d"
   },
   "source": [
    "We can even access the probabilities that one observation belongs to one class or the other with `predict_proba()`. The largest probability determines the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8bf5e9-f35c-44f2-9ce0-612bfe5bd53d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "dd8bf5e9-f35c-44f2-9ce0-612bfe5bd53d",
    "outputId": "cdba8502-b494-4c71-ed58-293be15a184c"
   },
   "outputs": [],
   "source": [
    "# Dataframe with probabilities that our first 5 observations belong to each class\n",
    "model_proba = pd.DataFrame(model.predict_proba(X_test)[0:4],\n",
    "                     columns=['Probability poor-quality wine', 'Probability good wine'])\n",
    "model_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6921d-d075-4f8c-9ef8-e7ebdb150513",
   "metadata": {
    "id": "0dc6921d-d075-4f8c-9ef8-e7ebdb150513"
   },
   "source": [
    "### 2.6 Evaluating our classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af149f-9cf2-45a1-9b15-279ecf50ef0a",
   "metadata": {
    "id": "c2af149f-9cf2-45a1-9b15-279ecf50ef0a"
   },
   "source": [
    "We will now evaluate the performance of our classifier using several metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3e15a-dd3f-40ec-afa7-782e5ba2fc5c",
   "metadata": {
    "id": "c9a3e15a-dd3f-40ec-afa7-782e5ba2fc5c"
   },
   "source": [
    "#### 2.6.1 Accuracy\n",
    "\n",
    "Perhaps the most intuitive classification metric is *accuracy*, which is the fraction of correct vs total predictions:\n",
    "\n",
    "$$ \\mathrm{Accuracy} = \\frac{\\# \\mathrm{\\, correct \\, predictions}}{\\# \\mathrm{\\, total \\, predictions}} $$\n",
    "\n",
    "For a sklearn classifier, this can be computed using the `score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81238743-9b28-410e-8f6a-f0934d6c8f6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81238743-9b28-410e-8f6a-f0934d6c8f6f",
    "outputId": "b7a7c125-4faf-46db-b955-66292ed42dc9"
   },
   "outputs": [],
   "source": [
    "# Accuracy on the test set\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(model.score(X_test, y_test)))\n",
    "\n",
    "# Accuracy on the training set\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(model.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8747b9-9300-4568-9144-5011d0bd7d57",
   "metadata": {
    "id": "3f8747b9-9300-4568-9144-5011d0bd7d57"
   },
   "source": [
    "Alternatively, we could use the `accuracy_score` module, imported with the following line of code:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385d7cb-2e5e-44dd-b6d6-1895303f0d6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4385d7cb-2e5e-44dd-b6d6-1895303f0d6c",
    "outputId": "aadc98a5-1e21-498f-9800-6a37aaf9b4da"
   },
   "outputs": [],
   "source": [
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accurary of Logistic regression classifier on test set: {accuracy_test :.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4376c9e-5a5d-4d49-a30b-9b98d4b6f0b4",
   "metadata": {
    "id": "a4376c9e-5a5d-4d49-a30b-9b98d4b6f0b4"
   },
   "source": [
    "When the testing accuracy is much lower than the training accuracy, we have an overfitting issue. Reciprocally, when the testing accuracy is similar to or higher than the training accuracy, the model might be underfitting, and we could consider either using a more powerful model or adding additional\n",
    "features.\n",
    "\n",
    "Our testing accuracy is 72%. Is that good? It depends! The quality of our prediction depends on the distribution of class in our original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba86eb-3539-4b63-864a-c9643c7755bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "15ba86eb-3539-4b63-864a-c9643c7755bf",
    "outputId": "6fd356a3-a27c-437b-a971-6ca4f2432432"
   },
   "outputs": [],
   "source": [
    "y.value_counts().plot.bar(color=['purple', 'blue'], grid=False)\n",
    "plt.ylabel('Number of observations')\n",
    "plt.title('Number of observations of each class in the wine dataset');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a03395",
   "metadata": {
    "id": "d5a03395"
   },
   "source": [
    "#### 2.6.2 Default Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26719ec2-0d1f-4516-a1d0-94afd091365a",
   "metadata": {
    "id": "26719ec2-0d1f-4516-a1d0-94afd091365a"
   },
   "source": [
    "Imagine we have a naive classifier that always predict the majority class. We call the default rate (or base rate) the accuracy of this classifier, which is equal to the size of the most common class over the size of the full dataset:\n",
    "\n",
    "$$\\text{Default rate} = \\frac{\\# \\text{ most frequent class}}{\\# \\text{ total observations}}$$\n",
    "\n",
    "If the default rate is too high, then the classification can be biased, meaning that the data set has too many observations of one class compared to the other classes, and has hence more impact on the classification results.\n",
    "\n",
    "The accuracy of our classifier should be better than the default rate. Let's calculate this default rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a06e9a-9230-4a7b-b3f7-b46caf19b7a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67a06e9a-9230-4a7b-b3f7-b46caf19b7a2",
    "outputId": "825a5506-424b-4c4f-8304-97375c1be2b4"
   },
   "outputs": [],
   "source": [
    "# Compute the default rate\n",
    "quality_0 = wines.loc[wines[\"quality\"] == 0].shape[0]\n",
    "print('# occurrence of class 0: ', quality_0)\n",
    "quality_1 = wines.loc[wines[\"quality\"] == 1].shape[0]\n",
    "print('# occurence of class 1: ', quality_1)\n",
    "defaultrate = max(quality_0, quality_1)/(wines[\"quality\"].shape[0])\n",
    "print(f'Default rate = {defaultrate:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc42bc-843e-4872-b1f8-df31dd2b34bf",
   "metadata": {
    "id": "abfc42bc-843e-4872-b1f8-df31dd2b34bf"
   },
   "source": [
    "Our default rate is about 52.9% while our classifier accuracy is 72.4%. Not too bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4d413-7a87-4590-a54f-cc5b6d44bb8e",
   "metadata": {
    "id": "00d4d413-7a87-4590-a54f-cc5b6d44bb8e"
   },
   "source": [
    "#### 2.6.3 Confusion matrix\n",
    "\n",
    "The confusion matrix allows us to get more details on the performance of our model. It will allow us to see what our classification model is getting right and what types of errors it is making.\n",
    "\n",
    "Here is how a confusion matrix looks like:\n",
    "\n",
    "|   | Class 0 predicted  | Class 1 predicted  |    \n",
    "|---|---|---|\n",
    "| **Class 0 actual**  |  TN |FP   |    \n",
    "| **Class 1 actual**  | FN  | TP  |   \n",
    "\n",
    "\n",
    "where TP = true positive, FN = false negative, FP = false positive and TN = true negative. Here class 1 is considered the \"True\" class.\n",
    "\n",
    "And here is how to calculate TN, TP, FP, FN on a multiple row confusion matrix :\n",
    "\n",
    "<img src='https://i.sstatic.net/AuTKP.png' width=\"400\">\n",
    "\n",
    "We are using the `confusion_matrix` module of sklearn, imported with the following line of code:\n",
    "\n",
    "``` python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "```\n",
    "\n",
    "It requires as input the true values and the predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f11c82-0201-4916-93c2-7309f84f7f91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06f11c82-0201-4916-93c2-7309f84f7f91",
    "outputId": "e756be46-9ffb-445a-b952-4107bd8ca900"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b9179-14f6-42cc-8715-6a9248f10007",
   "metadata": {
    "id": "6b7b9179-14f6-42cc-8715-6a9248f10007"
   },
   "source": [
    "To obtain a more visual representation, we will use `heatmap` from the `seaborn` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b68ac-74a0-4f8a-8e53-c4e5bd1f7838",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "760b68ac-74a0-4f8a-8e53-c4e5bd1f7838",
    "outputId": "94d90bb8-2fb5-4895-803b-f0b9e37e49be"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', fmt='.4g')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc130704-0459-49be-8643-559f37b2c428",
   "metadata": {
    "id": "dc130704-0459-49be-8643-559f37b2c428"
   },
   "source": [
    "On the diagonal of the confusion matrix are our correct predictions, while the off-diagonal elements are incorrect predictions. We can thus quickly identify whether one class is driving down or up the accuracy results, or if the results are more balanced.\n",
    "\n",
    "Recall that the accuracy is the number of correct predictions divided by the number of incorrect predictions. Thus, the accuracy is the sum of the diagonal elements of the confusion matrix, divided by the total number of elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dadc4b-ec06-4b29-843b-1d37aacc10e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70dadc4b-ec06-4b29-843b-1d37aacc10e9",
    "outputId": "1f2368a8-d752-44c9-c97b-38b349365657"
   },
   "outputs": [],
   "source": [
    "print('Accuracy on test set using sklearn: {:.6f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Accuracy on test set by hand: {:.6f}'.format((113+133)/(113+133+45+49)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22860d3-e863-4045-b487-9ffbdc36093c",
   "metadata": {
    "id": "c22860d3-e863-4045-b487-9ffbdc36093c"
   },
   "source": [
    "#### 2.6.4 Precision and Recall\n",
    "\n",
    "In many contexts, the accuracy would be an appropriate way to evaluate a model, but in others, this would be insufficient.\n",
    "\n",
    "For example, suppose we want to use a classification model to predict the likelihood of someone having a rare, but serious health condition. If the condition is very rare (say it appears in 0.01% of the population), then a model that always predicts false would have 99.99% accuracy, but the false negatives could have large consequences.\n",
    "\n",
    "To capture such situations, we often use two other very common metrics: the *precision* and *recall*.\n",
    "\n",
    "Let class 1 represents our positive cases (\"true\" class), the confusion matrix is:\n",
    "\n",
    "|   | Class 0 predicted  | Class 1 predicted  |    \n",
    "|---|---|---|\n",
    "| **Class 0 actual**  |  TN | FP   |    \n",
    "| **Class 1 actual**  | FN  | TP  |\n",
    "\n",
    "- *Precision*: The number of true positives over the number of positive  predictions. Precision tells us how often the model was correct when it predicted true.\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\# \\text{ true positives}}{\\# \\text{ predicted positives}} = \\frac{ \\text{TP}}{\\text{TP+FP}}$$\n",
    "\n",
    "- *Recall*: The number of true positives over the number of actual positives. Recall answers the question, “What fraction of the positives did we get correct?”  \n",
    "\n",
    "$$\\text{Recall} = \\frac{\\# \\text{ true positives}}{\\# \\text{ actual positives}} = \\frac{\\text{TP}}{ \\text{TP+FN}}$$\n",
    "\n",
    "In many settings, both precision and recall are equally important and a compound metric known as the *F1-score* is used:\n",
    "\n",
    "$$\\text{F1} = 2 \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "\n",
    "The F1 score is bounded between 0 and 1. It will only achieve a value of 1 if both precision and recall are exactly 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec43ad-713a-48bb-8517-5768fe3c35e9",
   "metadata": {
    "id": "6aec43ad-713a-48bb-8517-5768fe3c35e9"
   },
   "source": [
    "We will compute the precision using `precision_score` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)), the recall using `recall_score` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)), and the F1 score using `f1_score` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)). Here are the import lines:\n",
    "\n",
    "``` python\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "```\n",
    "\n",
    "For a binary classifier, all metrics will report by default the scores associated with the positive class (i.e., with observations equal to 1). If we are interested in the results for another class, we can specify some parameters. For instance, the parameter `average = None` will return the scores of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45baf3f7-da07-4ffd-b88d-56967671f118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45baf3f7-da07-4ffd-b88d-56967671f118",
    "outputId": "875c1189-01ae-4fc5-f675-8b98e1a694f0"
   },
   "outputs": [],
   "source": [
    "print('The precision for class 1 (good wines) is: {:0.3f}'.format(precision_score(y_test, y_pred)))\n",
    "print('The recall for class 1 is: {:0.3f}'.format(recall_score(y_test, y_pred)))\n",
    "print('The F1 score for class 1 is: {:0.3f}'.format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97534c50-3407-4bef-b79f-44da9ed81142",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "97534c50-3407-4bef-b79f-44da9ed81142",
    "outputId": "a9522bad-45ed-44e6-e277-16049d279666"
   },
   "outputs": [],
   "source": [
    "# Precision of each class\n",
    "model_precision = precision_score(y_test, y_pred, average = None)\n",
    "# Recall of each class\n",
    "model_recall = recall_score(y_test, y_pred, average = None)\n",
    "# F1 score of each class\n",
    "model_f1 = f1_score(y_test, y_pred, average = None)\n",
    "\n",
    "# Visualize all results in a dataframe:\n",
    "model_eval = pd.DataFrame([model_precision, model_recall, model_f1],\n",
    "                    index = ['Precision', 'Recall', 'F1 score'],\n",
    "                    columns=['Class 0', 'Class 1'])\n",
    "model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef31e3-c610-4964-be00-9b54f4be1cad",
   "metadata": {
    "id": "b1ef31e3-c610-4964-be00-9b54f4be1cad"
   },
   "source": [
    "Evaluating your classifier is key, and it depends on our objectives and on our data. You can find all the sklearn model evaluation metrics [here](https://scikit-learn.org/stable/modules/model_evaluation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44c38e",
   "metadata": {
    "id": "3a44c38e"
   },
   "source": [
    "#### 2.6.5 Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d51e65",
   "metadata": {
    "id": "26d51e65"
   },
   "source": [
    "To gain a comprehensive breakdown of our model's performance metrics, including precision, recall, F1-score, and support for each class, we utilize the classification report. This detailed assessment helps provide deeper insights into how effectively the logistic regression model classifies each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726639ad",
   "metadata": {
    "id": "726639ad",
    "outputId": "92655f70-18aa-4117-8a5f-f5cf1b3b8901"
   },
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592471e-12a9-4ef8-9085-d7b58cefeded",
   "metadata": {
    "id": "3592471e-12a9-4ef8-9085-d7b58cefeded"
   },
   "source": [
    "### 2.7 Adding cross-validation\n",
    "\n",
    "Roughly speaking, cross-validation splits the training dataset into many training/testing subsets, then chooses the regularization parameter value that minimizes the average MSE.\n",
    "\n",
    "More precisely, k-fold cross-validation does the following:\n",
    "\n",
    "1. Partition the dataset randomly into k subsets/”folds”.  \n",
    "2. Compute $MSE_j(\\alpha)=$ mean squared error in j-th subset when using the j-th subset as test data, and other k-1 as training data.  \n",
    "3. Minimize average (across folds) MSE $\\min_\\alpha \\frac{1}{k}\\sum_{j=1}^k MSE_j(\\alpha)$.\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/grid_search_cross_validation.png' width=\"500\">\n",
    "\n",
    "You can find a more detailed description of cross-validation [here](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "With the argument `cv`, we can specify the number of folds:\n",
    "\n",
    "We will see how the performance of our model evolves when implementing cross-validation technique. We are using the `LogisticRegressionCV()` module ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV)), imported as follows:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e9465-2718-4f0d-b7c0-8b390e6c1610",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "346e9465-2718-4f0d-b7c0-8b390e6c1610",
    "outputId": "15e28612-762c-4f67-b567-dbec8bce5633"
   },
   "outputs": [],
   "source": [
    "# Set up our model\n",
    "model_cv = LogisticRegressionCV(penalty='l2', solver='lbfgs', cv=5, max_iter=1000)\n",
    "\n",
    "# Fit our model\n",
    "model_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47f105-c0ec-4539-8343-9bde6172e907",
   "metadata": {
    "id": "1a47f105-c0ec-4539-8343-9bde6172e907"
   },
   "source": [
    "Let's check the accuracy of our new model and compare it to the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b05c52-2d01-444a-87bd-6a65dfe55dc7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "00b05c52-2d01-444a-87bd-6a65dfe55dc7",
    "outputId": "88ec21b5-7b41-4358-ca72-dfd6a123120f"
   },
   "outputs": [],
   "source": [
    "# Accuracy on the test set\n",
    "model_cv_test_accuracy = model_cv.score(X_test, y_test) # Cross-validation model\n",
    "model_test_accuracy = model.score(X_test, y_test)       # Logistic regression\n",
    "# Accuracy on the training set\n",
    "model_cv_train_accuracy = model_cv.score(X_train, y_train) # Cross-validation model\n",
    "model_train_accuracy = model.score(X_train, y_train)       # Logistic regression\n",
    "# Gather results in a dataframe:\n",
    "models_accuracy = [[model_cv_test_accuracy, model_test_accuracy],\n",
    "                   [model_cv_train_accuracy, model_train_accuracy]]\n",
    "model_compar = pd.DataFrame(models_accuracy,\n",
    "                    index = ['Test accuracy', 'Train accuracy'],\n",
    "                    columns=['Cross-validation model', 'Logistic regression'])\n",
    "model_compar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbaaaf2-5845-4c20-b376-b5c3166085e3",
   "metadata": {
    "id": "2fbaaaf2-5845-4c20-b376-b5c3166085e3"
   },
   "source": [
    "The accuracy improved a bit. Let's have a look at the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bcf462-6c10-4bb0-b184-31b293babeba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "69bcf462-6c10-4bb0-b184-31b293babeba",
    "outputId": "05a5312b-e30e-422b-fc89-bc03b837fb88"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion_cv = confusion_matrix(y_test, model_cv.predict(X_test))\n",
    "# Heatmap of confusion matrix\n",
    "sns.heatmap(confusion_cv, annot=True, cmap='Blues', fmt='.4g')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix with Cross Validation');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f987181-f5c1-47f7-aadd-df9b61c8bf98",
   "metadata": {
    "id": "0f987181-f5c1-47f7-aadd-df9b61c8bf98"
   },
   "source": [
    "We correctly predicted one additional observation from the class 0. Not a spectacular improvement but we'll take it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h28lWAXVDtu7",
   "metadata": {
    "id": "h28lWAXVDtu7"
   },
   "source": [
    "## Your turn !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_gs989cqMvBj",
   "metadata": {
    "id": "_gs989cqMvBj"
   },
   "source": [
    "Now it's your turn to implement a classifier! In this application, you will try to predict whether a forest fire spread and burned forest areas in the Montesinho natural park in Portugal.\n",
    "\n",
    "We are using the [Forest Fires dataset](https://www.kaggle.com/datasets/sumitm004/forest-fire-area), created by Paulo Cortez and Aníbal Morais, and available on Kaggle.\n",
    "\n",
    "Source: P. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data. In J. Neves, M. F. Santos and J. Machado Eds., New Trends in Artificial Intelligence, Proceedings of the 13th EPIA 2007 - Portuguese Conference on Artificial Intelligence, December, Guimaraes, Portugal, pp. 512-523, 2007. APPIA, ISBN-13 978-989-95618-0-9.\n",
    "\n",
    "The original dataset contains 13 columns:\n",
    "- X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "- Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "- month - month of the year: \"jan\" to \"dec\"\n",
    "- day - day of the week: \"mon\" to \"sun\"\n",
    "- FFMC - Fine Fuel Moisture Code (FFMC) index from the [Fire Weather Index (FWI)](https://www.nwcg.gov/publications/pms437/cffdrs/fire-weather-index-system) system: 18.7 to 96.20\n",
    "- DMC - Duff Moisture Code (DMC) index from the FWI system: 1.1 to 291.3\n",
    "- DC - Drought Code (DC) index from the FWI system: 7.9 to 860.6\n",
    "- ISI - Initial Spread Index (ISI) index from the FWI system: 0.0 to 56.10\n",
    "- temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "- RH - relative humidity in %: 15.0 to 100\n",
    "- wind - wind speed in km/h: 0.40 to 9.40\n",
    "- rain - outside rain in mm/m2 : 0.0 to 6.4\n",
    "- area - the burned area of the forest (in ha): 0.00 to 1090.84\n",
    "\n",
    "In addition, we created a new column, \"class\", detailing whether the fire burned an area of forest:\n",
    "- class is equal to 0 if area = 0.00 ha\n",
    "- class is equal to 1 if area > 0.00 ha\n",
    "\n",
    "Our goal will be to predict the class using logistic regression, given the weather and FWI features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea817c8-e3db-42fd-a429-8804c19b528d",
   "metadata": {
    "id": "fea817c8-e3db-42fd-a429-8804c19b528d"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "url_ff = 'https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/refs/heads/master/Labs/03-classification/data/forestfires.csv'\n",
    "forest_fire = pd.read_csv(url_ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0a3ee-2fda-45e2-bfbe-98f601ad0671",
   "metadata": {
    "id": "6ea0a3ee-2fda-45e2-bfbe-98f601ad0671"
   },
   "source": [
    "### Discover your dataset\n",
    "\n",
    "Explore your dataset, displaying a few observations, the types of your data, some summary statistics, the classification report and the confusion matrix. Feel free to push forward your EDA using a few graphs e.g., boxplot and pairplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c1303a-7a43-4bcc-a930-b91d16acf1af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "11c1303a-7a43-4bcc-a930-b91d16acf1af",
    "outputId": "13b3ed51-8ea4-4826-92fb-71f2bae3d2d3"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ypmEjcXnN1yQ",
   "metadata": {
    "id": "ypmEjcXnN1yQ"
   },
   "source": [
    "### Multi-features logistic regression\n",
    "\n",
    "We'll start with only four features, the temperature, the rain, the FFMC and wind.\n",
    "\n",
    "- Define your features and target variable ('class'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pw3tnywxN1yR",
   "metadata": {
    "id": "Pw3tnywxN1yR"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VWnHRgkDN1yS",
   "metadata": {
    "id": "VWnHRgkDN1yS"
   },
   "source": [
    "- Split you data intro training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LfITZBy_N1yS",
   "metadata": {
    "id": "LfITZBy_N1yS"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NjozmW_8N1yS",
   "metadata": {
    "id": "NjozmW_8N1yS"
   },
   "source": [
    "- Rescale your data, using the scaler of your choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GhRPOR3jN1yS",
   "metadata": {
    "id": "GhRPOR3jN1yS"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gbB_uaxAN1yT",
   "metadata": {
    "id": "gbB_uaxAN1yT"
   },
   "source": [
    "- Build and train a simple logistic regression classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3KD_rGS9N1yT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "3KD_rGS9N1yT",
    "outputId": "7c89f76b-a01b-43e9-9979-a08c9444efc8"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_LtIMzU0N1yT",
   "metadata": {
    "id": "_LtIMzU0N1yT"
   },
   "source": [
    "- Compare the training and testing accuracy of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5UAG09j0N1yT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5UAG09j0N1yT",
    "outputId": "568dee24-747e-46d4-88be-948ef12b94f0"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UUJEXATON1yT",
   "metadata": {
    "id": "UUJEXATON1yT"
   },
   "source": [
    "The accuracy seems low, which was expected following our EDA. Let's try to gather more information to better evaluate our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "za_P7O0LN1yU",
   "metadata": {
    "id": "za_P7O0LN1yU"
   },
   "source": [
    "- Plot the distribution of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m-ZTpYpKN1yU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "m-ZTpYpKN1yU",
    "outputId": "ebfa39f9-9539-4a1a-9ed3-0b285792551e"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EopnGTX8N1yU",
   "metadata": {
    "id": "EopnGTX8N1yU"
   },
   "source": [
    "- Compute the default rate and compare it to the accuracy of your model. What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0vL13TZIN1yU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vL13TZIN1yU",
    "outputId": "77c537d0-9dc9-418e-f3f0-922192641740"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeDwIqqN1yU",
   "metadata": {
    "id": "9eeDwIqqN1yU"
   },
   "source": [
    "The default rate is almost equal to our accuracy, so our classifier does not outperform a naive classifier that would always predict class 1..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLyHTxKnN1yU",
   "metadata": {
    "id": "iLyHTxKnN1yU"
   },
   "source": [
    "- Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tmKt_XrvN1yV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "tmKt_XrvN1yV",
    "outputId": "713daf13-de87-4bf1-9a1c-bd0640a61563"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52caece6",
   "metadata": {
    "id": "52caece6"
   },
   "source": [
    "- Generate a classification to get insight for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d156f3",
   "metadata": {
    "id": "e2d156f3",
    "outputId": "9a80e7ef-294d-4d76-a1e2-bd88d90f90c5"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee02d36-e94a-43b6-a3b6-c3d6eaac1741",
   "metadata": {
    "id": "3ee02d36-e94a-43b6-a3b6-c3d6eaac1741"
   },
   "source": [
    "## 3. Decision Trees  <a id = \"DT\"></a>\n",
    "\n",
    "<img src='https://regenerativetoday.com/wp-content/uploads/2022/04/dt.png' width=\"400\">\n",
    "\n",
    "Source: [Simple Explanation on How Decision Tree Algorithm Makes Decisions](https://regenerativetoday.com/simple-explanation-on-how-decision-tree-algorithm-makes-decisions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14273048-92d9-48a1-a974-71edda505929",
   "metadata": {
    "id": "14273048-92d9-48a1-a974-71edda505929"
   },
   "source": [
    "### 3.1 Background  <a id = \"DT-Background\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba57c8",
   "metadata": {
    "id": "28ba57c8"
   },
   "source": [
    "#### 3.1.1 Intuition <a id = \"DT-Intuition\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713dde7-504d-45c1-803c-7e0e2501b76f",
   "metadata": {
    "id": "4713dde7-504d-45c1-803c-7e0e2501b76f"
   },
   "source": [
    "Decision trees, as the name goes, use a tree-like model of decisions. At each node, the algorithm chooses a splitting rule (based on a feature) that maximizes the accuracy of the model. More precisely, at every split the algorithm maximizes a certain criterion previously given (e.g., Gini index, information gain).\n",
    "\n",
    "The objective of the algorithm is to find the simplest possible decision tree (i.e., only a few nodes = a small depth) with the highest accuracy.\n",
    "\n",
    "Consider the example below, where the objective is to classify if a person is fit or not. If we would have chosen another criterion for the root node (e.g., \"Exercises in the morning\" intead of \"Age<30\"), we could have ended up with a lower accuracy and/or more splits (i.e., a more complex tree). The same logic applies at each decision node, until we reach the leafs, i.e., the final decision.\n",
    "\n",
    "<img src='https://cdn.educba.com/academy/wp-content/uploads/2019/05/is-a-person-fit.png' width=\"300\">\n",
    "\n",
    "Decision Trees are simple to understand, interpret, and visualize. They can handle both numerical and categorical data, they do not require feature scaling and can deal with outliers. The algorithm is also good at at handling non-linearly separable data.\n",
    "\n",
    "As a drawback, Decision Trees suffer for a risk of overfitting, especially with large dataset since the tree might become too complex. They can also be unstable because small variations in the data might result in a completely different tree being generated. Potential solutions to avoid overfitting and get better performance include:\n",
    "- Rely on cross-validation to find the proper depth.\n",
    "- Building a collection of trees, i.e., a Random Forest - you can for instance read [Understanding Random Forest](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) for more explanation on the topic.\n",
    "\n",
    "Finally, note that, as for KNN, Decision Trees can be used for both classification and regression. You can read [Machine Learning Basics: Decision Tree Regression](https://towardsdatascience.com/machine-learning-basics-decision-tree-regression-1d73ea003fda) for a walk through on how to apply Decision Tree for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea9938a",
   "metadata": {
    "id": "eea9938a"
   },
   "source": [
    "#### 3.1.2 Decision criteria  <a id = \"DT-Decision\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396ea1e-5d83-4722-9e47-62e1aea1463d",
   "metadata": {
    "id": "d396ea1e-5d83-4722-9e47-62e1aea1463d"
   },
   "source": [
    "Growing a tree involves deciding on which features to choose and what conditions to use for splitting, along with knowing when to stop. How to do so? We need decision criteria, which evaluate the cost of a split, or alternatively, the \"purity\" of the selection. Here are some measures:\n",
    "\n",
    "- [Information Gain](https://en.wikipedia.org/wiki/Information_gain_(decision_tree)) measures... the information gained thanks to the split, relying on the notion of [Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)):\n",
    "$$\\text{Entropy}= - \\sum_{i=1}^c p_i \\log_2(p_i)$$\n",
    "where $c$ is the number of class and $p_i$ is the probability of randomly selecting an observation in class $i$. Let's consider two classes \"0\" and \"1\" for simplicity: $\\text{Entropy} = - p_0 \\log_2(p_0) - p_1 \\log_2(p_1)$:\n",
    "    - When our dataset (or node) has 50% of observations belonging to class \"0\" and 50% belonging to class \"1\", then $p_0=p_1=1/2$ and $\\text{Entropy} = 1$.\n",
    "    - When our dataset (or node) is \"pure\", say 0% of observations belonging to class \"0\" and 100% to class \"1\", then $p_0=0$, $p_1=1$, and $\\text{Entropy} = 0$\n",
    "\n",
    "<center>\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/800px-Binary_entropy_plot.svg.png' width=\"300\">\n",
    "</center>\n",
    "\n",
    "Source: Brona, Wikipedia [Binary entropy plot](https://commons.wikimedia.org/wiki/File:Binary_entropy_plot.svg)\n",
    "\n",
    "At each decision node, we compute the associated Entropy. This allows to calculate the information gain:\n",
    "$$\\text{Information Gain}=\\text{Entropy}_\\text{parent}-\\text{Average Entropy}_\\text{children}$$\n",
    "\n",
    "Our objective is to obtain pure leaf nodes, and thus to reduce the entropy in the children nodes. Say differently, we need to find the splits that maximize the Information Gain.\n",
    "\n",
    "- Gini Index, also called [Gini Impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity), is an alternative decision criterion, inspired by the [Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient), a measure intended to represent the wealth inequality within a nation or a social group. The Gini of a dataset is:\n",
    "$$\\text{Gini}=1-\\sum_{i=1}^c p_i^2$$\n",
    "Let's again consider two classes for simplicity \"0\" and \"1\", $\\text{Gini}=1-p_0^2-p_1^2$\n",
    "    - When our dataset (or node) has 50% of observations belonging to class \"0\" and 50% belonging to class \"1\", then $p_0=p_1=1/2$ and $\\text{Gini} = 0.5$.\n",
    "    - When our dataset (or node) is \"pure\", say 0% of observations belonging to class \"0\" and 100% to class \"1\", then $p_0=0$, $p_1=1$, and $\\text{Gini} = 0$\n",
    "\n",
    "At each decision node we compute the associated Gini Index, and then the average Gini Index of the split. Our objective is to minimize the Gini Index.\n",
    "\n",
    "For further information on the topic, you can read the articles:\n",
    "- [Decision Trees Explained — Entropy, Information Gain, Gini Index, CCP Pruning](https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c)\n",
    "- [Understanding the Gini Index and Information Gain in Decision Trees](https://medium.com/analytics-steps/understanding-the-gini-index-and-information-gain-in-decision-trees-ab4720518ba8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed40bd-9056-405a-b89b-15e479752fd0",
   "metadata": {
    "id": "8bed40bd-9056-405a-b89b-15e479752fd0"
   },
   "source": [
    "### 3.2 Implementation  <a id = \"DT-Implementation\"></a>\n",
    "\n",
    "We will use the same dataset, trying to predict the quality of the wine based on the same features. We are using the same training and test set for comparability between model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa75a7-714d-4128-8ab2-93730c5f7c79",
   "metadata": {
    "id": "59fa75a7-714d-4128-8ab2-93730c5f7c79"
   },
   "source": [
    "#### 3.2.1 Building, training, and evaluating our classifier <a id = \"DT-Building\"></a>\n",
    "\n",
    "We implement a Decision Tree algorithm to predict the class of our target variable using the **sklearn** module `DecisionTreeClassifier()` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)). Here is the import line:\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "```\n",
    "\n",
    "We can specify various parameters:\n",
    "- `criterion`= determines how to measure the quality of a split: \"gini\" for Gini Impurity (default value), \"entropy or \"log_loss\" for Information Gain\n",
    "- `max_depth`= determines the depth of the tree, i.e., the amount of nodes we allow the tree to generate. If None (default value), then nodes are expanded until all leaves are pure or until all leaves contain less than a given number of samples.\n",
    "\n",
    "Please refer to the documentation for the full list of parameters.\n",
    "\n",
    "We will start by arbitrarily selecting a maximum depth of 3, and default values for the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b53c4-939f-4ec8-a717-ca803af15489",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f98b53c4-939f-4ec8-a717-ca803af15489",
    "outputId": "f5ce4971-b000-4ddb-d846-1a5a49fab110"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model_tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 3)\n",
    "\n",
    "# Fit model\n",
    "model_tree.fit(X_train, y_train)\n",
    "\n",
    "# Test accuracy\n",
    "print('Accuracy of Decision Tree on test set: {:.2f}'.format(model_tree.score(X_test, y_test)))\n",
    "print('Accuracy of Decision Tree on training set: {:.2f}'.format(model_tree.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b200d-dfb8-46a2-971f-ea2ca3afd697",
   "metadata": {
    "id": "446b200d-dfb8-46a2-971f-ea2ca3afd697"
   },
   "source": [
    "Let's see if we can reduce max depth without losing accuracy by using cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95489c7f-e616-47a0-8908-8c26c2eca5e9",
   "metadata": {
    "id": "95489c7f-e616-47a0-8908-8c26c2eca5e9"
   },
   "source": [
    "### 3.3 Tuning parameters with cross-validation <a id = \"DT-Tuning\"></a>\n",
    "\n",
    "Tuning the hyperparameters of a Decision Tree involves determining the best criteria and depth that lead to the most effective model. We use `GridSearchCV` to automate this optimization by testing combinations of criterion and `max_depth` values through systematic `cross-validation`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095f0ad",
   "metadata": {
    "id": "2095f0ad"
   },
   "source": [
    "We specify the criteria ('gini' and 'entropy') and maximum tree depths (1 to 3) to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3b152-bab7-4733-be1c-a3488e970ecb",
   "metadata": {
    "id": "2ce3b152-bab7-4733-be1c-a3488e970ecb"
   },
   "outputs": [],
   "source": [
    "# Define parameters to test\n",
    "grid_tree = {'criterion':['gini','entropy'] ,     # criterion\n",
    "        'max_depth':np.arange(1,3),               # array from 1 to 3, maximum depth\n",
    "       }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24732ee0",
   "metadata": {
    "id": "24732ee0"
   },
   "source": [
    "We set up GridSearchCV with the Decision Tree, the parameter grid, and 5-fold cross-validation, fitting it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85883076",
   "metadata": {
    "id": "85883076",
    "outputId": "b97f5f1d-1863-41c7-c026-536ed2057c61"
   },
   "outputs": [],
   "source": [
    "# Define and fit model\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "dec_tree_cv = GridSearchCV(dec_tree, grid_tree, cv=5)\n",
    "dec_tree_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d590af9",
   "metadata": {
    "id": "6d590af9"
   },
   "source": [
    "After identifying the optimal parameters, we evaluate and report the training and test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828744f",
   "metadata": {
    "id": "0828744f",
    "outputId": "a21258a9-f3ef-4a57-c4e6-94afdd33d1db"
   },
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Hyperparameters:\", dec_tree_cv.best_params_)\n",
    "print(\"Best model:\", dec_tree_cv.best_estimator_)\n",
    "print(\"Train Score: {:0.2f}\".format(dec_tree_cv.best_score_))\n",
    "print(\"Test Score: {:0.2f}\".format(dec_tree_cv.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f7badf-2b72-4728-b4fd-9c9dea66d933",
   "metadata": {
    "id": "92f7badf-2b72-4728-b4fd-9c9dea66d933"
   },
   "source": [
    "We could reduce the depth to 2 and still got a good accuracy on the test set. However, we should remember that we have a limited number of observations so the results should be understood with a grain of salt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e13259-d288-4de5-9c2e-32e9c2127848",
   "metadata": {
    "id": "76e13259-d288-4de5-9c2e-32e9c2127848"
   },
   "source": [
    "### 3.4 Visualize tree <a id = \"DT-Visualize\"></a>\n",
    "\n",
    "We can visualize our Decision Tree, allowing us to get a better understanding of the decisions made by our algorithm and of the features that played a key role in our classification.\n",
    "\n",
    "To do so, we are using the `plot_tree()` module of sklear ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)). Here is the import line:\n",
    "\n",
    "```python\n",
    "from sklearn.tree import plot_tree\n",
    "```\n",
    "\n",
    "We plot our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dfead1-0fc6-4e52-805e-35974ad6c074",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "30dfead1-0fc6-4e52-805e-35974ad6c074",
    "outputId": "ea59e0c3-1dde-4a52-f6bb-fe4741cb56b6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "feature_names = list(X.columns.values)\n",
    "plot_tree(model_tree, filled=True, feature_names=feature_names, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c501ff5-145a-4758-95ea-27ffb42febe6",
   "metadata": {
    "id": "6c501ff5-145a-4758-95ea-27ffb42febe6"
   },
   "source": [
    "Our tree starts with the root in which we have 1019 samples (our data points), 482 belonging to class 0 and 537 belonging to class 1.\n",
    "\n",
    "Each node represents a condition on which the tree splits into branches. For instance, the first two depth levels are using the \"aclohol\" as feature to split, and the last node is using the mass.\n",
    "\n",
    "The end of a branch that no longer splits is a leaf. Here we have perfectly classified our observations.\n",
    "\n",
    "The colors represent the purity of a node. In our case, blue corresponds to class 0 and orange corresponds to class 1. They are displayed because we specified the parameter `filled = True`.\n",
    "\n",
    "Finally, the gini coefficient is our measure of purity for each node. In our dataset we start with 0.499 (corresponds to the almost 50-50 distribution of classes in the root) and then gradually go down to 0 (maximum purity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8367747-4cbe-45e1-8003-b4ba277f2741",
   "metadata": {
    "id": "a8367747-4cbe-45e1-8003-b4ba277f2741",
    "tags": []
   },
   "source": [
    "## Your turn !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exXKaFjSOeja",
   "metadata": {
    "id": "exXKaFjSOeja"
   },
   "source": [
    "Using the Forset Fires dataset again:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d48b11-4cee-4904-9adb-03d891b1adfa",
   "metadata": {
    "id": "90d48b11-4cee-4904-9adb-03d891b1adfa"
   },
   "source": [
    "- Build and train a Decision Tree Classifier with a maximum depth of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547a215-9365-4db4-8cf3-034ac336f1e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "c547a215-9365-4db4-8cf3-034ac336f1e7",
    "outputId": "a0ebdee8-f956-4d69-df84-cbbfae794b6e"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a6f3c8-4d5c-4d63-a41d-6766f5e383a8",
   "metadata": {
    "id": "30a6f3c8-4d5c-4d63-a41d-6766f5e383a8"
   },
   "source": [
    "- Compare the accuracy on the training set and the test set, and create a heatmap of your confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000bf99-2eb4-4946-9ae9-9bad47af96c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "d000bf99-2eb4-4946-9ae9-9bad47af96c1",
    "outputId": "143dd0fa-3e29-4cbb-a66e-b2ec74ebda81"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e60ea99-a687-49d2-8b6d-2790a521e59d",
   "metadata": {
    "id": "4e60ea99-a687-49d2-8b6d-2790a521e59d"
   },
   "source": [
    "- Explore with `GridSearchCV` a grid of parameters. Check the accuracy on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17ec90-6fce-4073-a459-d859f8b66dde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a17ec90-6fce-4073-a459-d859f8b66dde",
    "outputId": "f6c1c7bf-cc5e-4e89-92a3-3d8c7b360c7b"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yq91Bc63SEq-",
   "metadata": {
    "id": "yq91Bc63SEq-"
   },
   "source": [
    "Using the best parameters, visualize the decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4NVLKFXOxvm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "a4NVLKFXOxvm",
    "outputId": "66a376d1-5a56-49be-9f1d-e2e92fba2e3d"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f558b",
   "metadata": {
    "id": "e43f558b"
   },
   "source": [
    "## 4. k-Nearest Neighbords (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748919c8",
   "metadata": {
    "id": "748919c8"
   },
   "source": [
    "### 4.1 Intuition <a id=\"kNN-Intuition\"></a>\n",
    "\n",
    "The k-Nearest Neighbors (kNN) algorithm is a simple, yet powerful machine learning technique that predicts the label of a data point by looking at the 'k' closest labeled data points and letting them vote on what label the query point should have. It is inherently a lazy learning algorithm as it does not explicitly learn a model. Instead, it memorizes the training dataset.\n",
    "\n",
    "The primary principle behind kNN is that similar things exist in close proximity. In other words, similar data points are near to each other in the feature space. The proximity or closeness of points is calculated using distance measures such as Euclidean, Manhattan, or Minkowski distance.\n",
    "\n",
    "Consider an example where we want to classify whether a customer will buy a product or not. If we choose k=3, the algorithm looks at the three nearest neighbors of a customer to determine the purchasing behavior based on the majority vote of the neighbors' purchasing statuses.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:990/1*3SwcOCUyVdGauhHrHvOaLA.png' width=\"500\">\n",
    "\n",
    "The algorithm is versatile and can handle both classification and regression tasks but is predominantly known for its application in classification. It’s easy to implement and understand but becomes significantly slower as the size of the data in use grows.\n",
    "\n",
    "**Benefits of kNN:**\n",
    "- No assumptions about the data — no need to build a model, tune several parameters, or make additional assumptions.\n",
    "- Adapts easily to real-time data.\n",
    "- Very easy to implement for multi-class problems.\n",
    "\n",
    "**Drawbacks of kNN:**\n",
    "- The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase.\n",
    "- Sensitive to the scale of the data and irrelevant features.\n",
    "- Poor performance on imbalanced datasets.\n",
    "- It requires a good definition of what it means for two points to be close or similar.\n",
    "\n",
    "**To improve kNN performance:**\n",
    "- Pre-processing data with scaling can bring improvement.\n",
    "- Choosing the right value of k is crucial — too low a value of k can be noisy and lead to effects of outliers, while a high value makes computation expensive and might also include points from other classes.\n",
    "- Weighted kNN can help if some types of votes are more important than others.\n",
    "  \n",
    "For more in-depth understanding and practical applications of kNN, consider reading [K-Nearest Neighbor by Anthony Christopher](https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4) which offers further insights and examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ff6e8",
   "metadata": {
    "id": "7b0ff6e8"
   },
   "source": [
    "### 4.2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df392a40",
   "metadata": {
    "id": "df392a40",
    "outputId": "6e02586a-13cc-49a4-a06e-1b138379c614"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the kNN classifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Train the Decision Tree Classifier on the training data\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance on the training set\n",
    "train_accuracy = knn_classifier.score(X_train, y_train)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f} or {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "test_accuracy = knn_classifier.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} or {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf961d",
   "metadata": {
    "id": "93cf961d"
   },
   "source": [
    "## 5. Encoding categorical variable\n",
    "\n",
    "Categorical data represents types of data which may be divided into groups. Examples include race, sex, age group, and educational level. While this data can be very informative, most machine learning algorithms require numerical input and output variables. Thus, converting categorical data into a form that could be provided to ML algorithms to do a better job in prediction is essential. Two common approaches to this problem are One-Hot Encoding and Label Encoding, which convert categorical data into numerical format, each in a different way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f87619",
   "metadata": {
    "id": "96f87619"
   },
   "source": [
    "### 5.1 One-Hot Encoding\n",
    "One-Hot Encoding transforms each categorical feature with n possible values into n binary features, with only one active. This method creates new columns indicating the presence of each possible value from the original data. For instance, a feature like \"Color\" with three categories (Red, Blue, Green) will transform into three features: 'Color_Red', 'Color_Blue', and 'Color_Green'. Each will have a binary value:\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*ggtP4a5YaRx6l09KQaYOnw.png\" width=\"800px\">\n",
    "\n",
    "Advantages:\n",
    "- Prevents the model from assuming a natural ordering between categories which can result in poor performance or unexpected results (assumes no hierarchy).\n",
    "- Very effective when the categorical feature is not ordinal (i.e., the categories are not in any particular order).\n",
    "\n",
    "Disadvantages:\n",
    "- The number of features can explode with categorical variables with many levels, leading to high memory consumption and potentially degraded model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ea815",
   "metadata": {
    "id": "029ea815"
   },
   "source": [
    "### 5.2 Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b6f6e",
   "metadata": {
    "id": "005b6f6e"
   },
   "source": [
    "Label Encoding converts each value in a categorical column into a number. These numerical values can be used in the algorithms directly. For example, \"Color\" can be encoded as Red=1, Green=2, Blue=3.\n",
    "\n",
    "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6476f92b-cebf-4cea-8098-ebc92e2d6166_1326x705.jpeg\" width=\"600px\">\n",
    "\n",
    "Advantages:\n",
    "- Keeps the dataset compact, which is particularly useful if there are many levels in the categorical variable.\n",
    "- Simple to implement and does not increase the dimensionality of the data.\n",
    "\n",
    "Disadvantages:\n",
    "- The numerical assignment introduces an order or priority (e.g., Blue is greater than Green), which might lead to poor performance or unexpected results if such a hierarchy does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a12e8c",
   "metadata": {
    "id": "19a12e8c"
   },
   "source": [
    "### 5.3 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473322f",
   "metadata": {
    "id": "1473322f"
   },
   "source": [
    "Load the [dataset](https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/04-classification/data/student-performance-factor.csv) from github about student performance factors and remove columns that are not required for the analysis. Display the first few rows of the cleaned dataset to verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422e72d",
   "metadata": {
    "id": "0422e72d",
    "outputId": "a5d12479-db7b-40a6-975e-d76f5fc22612"
   },
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "df_student = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/refs/heads/master/Labs/03-classification/data/student-performance-factor.csv\")\n",
    "df_student = df_student.drop(columns=['Access_to_Resources', 'Parental_Involvement', 'Family_Income', 'Teacher_Quality', 'School_Type', 'Peer_Influence', 'Parental_Education_Level', 'Distance_from_Home', 'Learning_Disabilities', 'Gender', 'Internet_Access'])\n",
    "df_student.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441b6c3",
   "metadata": {
    "id": "6441b6c3"
   },
   "source": [
    "As you can see, we have 2 columns that contains categorical value : \"Extracurricular_Activities\", \"Motivation_Level\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8acb61",
   "metadata": {
    "id": "2e8acb61"
   },
   "source": [
    "We can then identify and display the unique values in the 'Extracurricular_Activities' and 'Motivation_Level' columns to understand the variety of data entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39976600",
   "metadata": {
    "id": "39976600",
    "outputId": "90426dc5-11b7-4fce-d607-b64d7b5e1057"
   },
   "outputs": [],
   "source": [
    "# Print the unique values\n",
    "print(\"Unique values in 'Extracurricular_Activities' column:\")\n",
    "print(df_student['Extracurricular_Activities'].unique())\n",
    "\n",
    "# Print the unique values\n",
    "print(\"\\nUnique values in 'Motivation_Level' column:\")\n",
    "print(df_student['Motivation_Level'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377fad9",
   "metadata": {
    "id": "0377fad9"
   },
   "source": [
    "After, we can apply one-hot encoding to the 'Extracurricular_Activities' column to handle nominal data and label encoding to 'Motivation_Level' for ordinal data. Display the first few rows of each to inspect the encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a6452",
   "metadata": {
    "id": "da0a6452",
    "outputId": "874751d8-af02-4602-971a-7cae766cfd33"
   },
   "outputs": [],
   "source": [
    "#Apply one-hot encoding to nominal variables\n",
    "ohe_fit_Extra_Act = OneHotEncoder().fit(df_student[[\"Extracurricular_Activities\"]])\n",
    "ohe_Extra_Act = ohe_fit_Extra_Act.transform(df_student[[\"Extracurricular_Activities\"]]).toarray()\n",
    "ohe_Extra_Act = pd.DataFrame(ohe_Extra_Act, columns=ohe_fit_Extra_Act.get_feature_names_out())\n",
    "display(ohe_Extra_Act.head())\n",
    "\n",
    "\n",
    "# Apply label encoding to ordinal variables\n",
    "le_fit_Motivation = LabelEncoder().fit(df_student[\"Motivation_Level\"])\n",
    "le_Motivation_Level = pd.DataFrame(le_fit_Motivation.transform(df_student[\"Motivation_Level\"]), columns=[\"le_Motivation_Level\"])\n",
    "display(le_Motivation_Level.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a5341",
   "metadata": {
    "id": "fe1a5341"
   },
   "source": [
    "Then, we merge the encoded columns back into the main dataset and remove the original unencoded columns. And we display the new shape of the dataset and the first few rows to see the updated structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25895722",
   "metadata": {
    "id": "25895722",
    "outputId": "a226d83f-3baa-4fc0-c859-13f0fb0a75fc"
   },
   "outputs": [],
   "source": [
    "df_student_encoded = pd.concat([df_student, ohe_Extra_Act, le_Motivation_Level], axis=1)\n",
    "\n",
    "# Drop the original columns that have been encoded if they exist\n",
    "columns_to_drop = [\"Extracurricular_Activities\", \"Motivation_Level\"]\n",
    "df_student_encoded.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Now df has the original data minus the categorical columns, plus the new encoded columns\n",
    "print(df_student_encoded.shape)\n",
    "display(df_student_encoded.head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
