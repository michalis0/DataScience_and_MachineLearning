{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week9/Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw22Uv7KoBZb"
      },
      "source": [
        "# Data Mining and Machine Learning - Week 9\n",
        "# Clustering\n",
        "\n",
        "Now we come to unsupervised learning (i.e., unlabelled data). The objectives of clustering are:\n",
        "* to organise the data into classes so that there is a high degree of intra-class similarity and a low degree of inter-class similarity\n",
        "* to find the class labels and the number of classes\n",
        "directly from the data (in contrast to\n",
        "classification).\n",
        "* to find natural groupings between objects.\n",
        "\n",
        "There are two types of custering:\n",
        "* Partitioning algorithms (for example, k-means)\n",
        "* Hierarchical clustering\n",
        "\n",
        "The aim of this laboratory session is to implement and compare them.\n",
        "\n",
        "### Table of Contents\n",
        "#### 1. Basic Examples\n",
        "* 1.1 k-means in 1 dimension\n",
        "* 1.2 Hierarchical clustering\n",
        "\n",
        "#### 2. Runtime complexity comparison exercise\n",
        "\n",
        "#### 3. Customers Segmentation\n",
        "* 3.1 Load, prepare, and explore data\n",
        "* 3.2 k-means algorithms\n",
        "* 3.3 Hierarchical clustering\n",
        "\n",
        "#### 4. Clustering vs. classification\n",
        "* 4.1 Load data\n",
        "* 4.2 Algorithms comparison: k-means vs. logistic regression\n",
        "\n",
        "\n",
        "Author: Luc Kunz\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQP1xZ8p1i_7"
      },
      "source": [
        "# Import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import cluster\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "# Customize plots\n",
        "%matplotlib inline\n",
        "plt.style.use('dark_background')\n",
        "#plt.style.available"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjB9fn6A8TB8"
      },
      "source": [
        "## 1. Basic Examples\n",
        "As always, we first go through basic examples to better visualize and understand what happens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnBprgbU9UYI"
      },
      "source": [
        "### 1.1 k-means in 1 dimension\n",
        "\n",
        "We first work with a simple data set of students grades.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NUKDjkz1oF3"
      },
      "source": [
        "# Load data\n",
        "grades = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/week9/data/student_grades.csv\")\n",
        "print(len(grades))\n",
        "grades.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbgB-QD8XV25"
      },
      "source": [
        "There are 36 grades that are between 0 and 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6QxAAAN1ub1"
      },
      "source": [
        "# Plot the grades\n",
        "plt.figure(figsize=(20,2))\n",
        "plt.scatter(grades, np.zeros_like(grades), marker='x')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQVEX7Gb0JhU"
      },
      "source": [
        "We clearly see 4 clusters:\n",
        "* around 72.5 are the worst students\n",
        "* around 80 are normal students\n",
        "* around 87.5 are good students\n",
        "* around 92.5 are excellent students\n",
        "\n",
        "We now fit k-means algortims with different k (i.e. number of clusters)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3zKL6oe12yg"
      },
      "source": [
        "# Fit k-means algorithm for different k\n",
        "fig, ax = plt.subplots(4, 1, figsize=(20, 8))\n",
        "for k in [2, 3, 4, 5]:\n",
        "  # Create instance of class\n",
        "  model = KMeans(n_clusters=k)\n",
        "  # Fit model\n",
        "  model.fit(grades)\n",
        "  # Get centers of clusters\n",
        "  centers = model.cluster_centers_\n",
        "  # Get predictions\n",
        "  pred = model.predict(grades)\n",
        "  # Plot\n",
        "  ax[k-2].scatter(grades, np.zeros_like(grades), marker='x', c=pred, cmap='Dark2')\n",
        "  ax[k-2].scatter(centers, np.zeros_like(centers), color='yellow')\n",
        "  ax[k-2].set_title('k-means algorithm with k = ' + str(k))\n",
        "plt.subplots_adjust(hspace=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PySrrVKyYkg7"
      },
      "source": [
        "With 2 clusters, it already works well. We could then label students as `good` and `not so good` for example. However, it works best with 4 clusters. In order to find out the optimal number of groups, we can use the `elbow method` (i.e. plotting the objective/cost function for different k)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6WXdTnq5GfW"
      },
      "source": [
        "# Elbow Method - inertia\n",
        "inertia = []\n",
        "k_values = range(2, 20)\n",
        "for k in k_values:\n",
        "  model = KMeans(n_clusters=k)\n",
        "  model.fit(grades)\n",
        "  inertia.append(model.inertia_)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(k_values, inertia, marker='o', linestyle='solid')\n",
        "plt.title('Inertia for different k values')\n",
        "plt.ylabel('Inertia')\n",
        "plt.xlabel('k')\n",
        "plt.xticks(np.arange(min(k_values)-1, max(k_values)+1, step=1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FVR7WxnaMWe"
      },
      "source": [
        "\n",
        "In this case, 4 is the optimal number of clusters. The `inertia` is defined as the sum of squared distances of samples to their closest cluster center. This is the cost function (i.e. what the algorithm tries to minimize)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j3xiR1Y2Yy3"
      },
      "source": [
        "### 1.2 Hierarchical clustering\n",
        "In order to show how it works, we use a small set of 9 points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdKFuVFO_UjI"
      },
      "source": [
        "# Create Data\n",
        "data = {\n",
        "    \"p\":[\"p0\", \"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\", \"p8\"],\n",
        "    \"x1\":[0.84, 0.15, 0.87, 0.90, 0.93, 0.88, 0.87, 0.12, 0.91],\n",
        "    \"x2\":[0.91, 0.15, 0.47, 0.54, 0.84, 0.51, 0.86, 0.18, 0.73]}\n",
        "\n",
        "data = pd.DataFrame(data)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "676LpTjH_UhF"
      },
      "source": [
        "# Plot\n",
        "data.plot.scatter(\"x1\", \"x2\", figsize=(15, 10))\n",
        "for point in data.values:\n",
        "  plt.text(point[1]+0.01, point[2], point[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tZYsxv7a4hw"
      },
      "source": [
        "In this case, there are 2 or 3 clusters. We can further compute the euclidean distances between the points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf5VmKxIbFbF"
      },
      "source": [
        "# Show distances\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(pd.DataFrame(pairwise_distances(data[[\"x1\", \"x2\"]], metric='euclidean')), annot=True, cmap='Blues', fmt='.3g')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8KfUGtdbaAs"
      },
      "source": [
        "For example p0 is:\n",
        "* close to p6, p4, p8\n",
        "* a little further away from p3, p5, p2\n",
        "* far from p1, p7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCiwVmL02FBx"
      },
      "source": [
        "# Plot dendrogram - linkage = 'single'\n",
        "plt.figure(figsize=(10,6))\n",
        "dendrogram = sch.dendrogram(sch.linkage(data[[\"x1\", \"x2\"]], method = 'single'))\n",
        "plt.title('Dendrogram - single linkage')\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('Euclidean distances')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZaeYzyLcQFv"
      },
      "source": [
        "With `single linkage`, we can see that p0 is first attached to p6, then the two to p4, and finally the three to p8. This is the first cluster. The resulting dendrogram depends on the linkage, as shown below. With 2 clusters we have: {p1, p7} and {p2, p3, p5, p8, p4, p0, p6} and with 3 clusters we have: {p1, p7}, {p2, p3, p5} and {p8, p4, p0, p6}."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyqAwbnI2Vc_"
      },
      "source": [
        "# Plot dendrogram - linkage = 'complete'\n",
        "plt.figure(figsize=(10,6))\n",
        "dendrogram = sch.dendrogram(sch.linkage(data[[\"x1\", \"x2\"]], method = 'complete'))\n",
        "plt.title('Dendrogram - complete linkage')\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('Euclidean distances')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azg8N3zsfpU0"
      },
      "source": [
        "With `complete linkaage`, p0 is first attached to p6 and p4 to p8. Then the two groups are comnined to form a superset. This is because p4 and p8 are closer to each other than to the max distance to p0 or p6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFXk-Lal4IwD"
      },
      "source": [
        "# Plot with diiferent number of clusters\n",
        "fig, ax = plt.subplots(3,1, figsize=(5,15))\n",
        "for k in [2, 3, 4]:\n",
        "  clustering = AgglomerativeClustering(n_clusters=k, linkage='single', affinity='euclidean')\n",
        "  assignment = clustering.fit_predict(data[[\"x1\", \"x2\"]])\n",
        "  ax[k-2].scatter(data[\"x1\"], data[\"x2\"], marker='x', c=assignment, cmap='Dark2')\n",
        "  ax[k-2].set_title('Hierarchical Clustering with k = ' + str(k))\n",
        "  ax[k-2].set_xlabel(\"x1\")\n",
        "  ax[k-2].set_ylabel(\"x2\")\n",
        "plt.subplots_adjust(hspace=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsR4PC-F75w0"
      },
      "source": [
        "We see that k = 2 and k = 3 is fine. However, k = 4 does not work in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuPuIV_O08ws"
      },
      "source": [
        "## 2. Runtime complexity comparison exercise\n",
        "\n",
        "To do in groups: follow the steps and send your answers and code @Luc Kunz on Slack (direct message). This is a good way to improve your participation grade.\n",
        "\n",
        "We want to compare the computation time needed between **Kmeans** and **Hierarchial clustering** for different numbers of points.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQAO7w4V1KVJ"
      },
      "source": [
        "# We create a function that generate 3 clusters\n",
        "def generate_three_clusters(num_points):\n",
        "  centers = [(-15, -15), (0, 0), (15, 15)]\n",
        "  cluster_std = [2, 3, 2]\n",
        "  X, y = make_blobs(n_samples=num_points, cluster_std=cluster_std, centers=centers, n_features=3, random_state=1)\n",
        "  return X, y\n",
        "\n",
        "# Example with 100 points\n",
        "X, y = generate_three_clusters(100)\n",
        "# Plot clusters\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"red\", s=10)\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"blue\", s=10)\n",
        "plt.scatter(X[y == 2, 0], X[y == 2, 1], color=\"green\", s=10)\n",
        "plt.title('Number of points: 100')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjccGsJD2zrx"
      },
      "source": [
        "### 2.1 Generate three cluster using the above function for n = 100, 1000, 2500, 5000, 7500, 10000, 25000 points. Store the returned X in a list `X_list`. You have to complete the code below where you see `[COMPLETE]`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQAGGEEU1Np2"
      },
      "source": [
        "# Define list\n",
        "X_list = []\n",
        "# Define numbers of points\n",
        "num_points = [100, 1000, 2500, 5000, 7500, 10000, 25000]\n",
        "\n",
        "for n in num_points:\n",
        "  # Generate three clusters\n",
        "  X, y = # [COMPLETE]\n",
        "\n",
        "  # Append X to X_list\n",
        "  # [COMPLETE]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QBRI8cg4YMz"
      },
      "source": [
        "### 2.2 For all samples of points, fit a k-means algorithm and a hierarchical clustering algorithm and record to completion time. Use [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and [`AgglomerativeClustering`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) with a number of cluster equal to 3. In order to record the time, use what method you want. Suggestion: you can use the `time` package as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi8PiVGp4XrF"
      },
      "source": [
        "# Example with the time package\n",
        "import time\n",
        "start = time.time()\n",
        "for i in range(10000000):\n",
        "  a = 1 + 1 # do something\n",
        "end = time.time()\n",
        "print('Start: ' + str(start))\n",
        "print('End: ' + str(end))\n",
        "print('Time: ' + str(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0umAoaOB1TJI"
      },
      "source": [
        "# Record time in list\n",
        "k_means_time = []\n",
        "hc_time = []\n",
        "\n",
        "for X in X_list:\n",
        "\n",
        "  # Create instance of KMeans class (with 3 clusters)\n",
        "  model = # [COMPLETE]\n",
        "\n",
        "  # Fit the model on X and record the time to fit\n",
        "  # [COMPLETE]\n",
        "\n",
        "  # Store the execution time in k_means_time\n",
        "  k_means_time.append(.....)\n",
        "\n",
        "  # Create instance of AgglomerativeClustering class (with 3 clusters)\n",
        "  model = # [COMPLETE]\n",
        "\n",
        "  # Fit the model on X and record the time to fit\n",
        "  # [COMPLETE]\n",
        "\n",
        "  # Store the execution time in hc_time\n",
        "  hc_time.append(.....)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNqVDKxS9PlG"
      },
      "source": [
        "### 2.3 Plot the execution time vs. number of points for k-means and hierarchical clustering using an appropriate plot type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6i05g-D1UNu"
      },
      "source": [
        "# [COMPLETE]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EwOjJ8U9mbF"
      },
      "source": [
        "### 2.4 Send you code and a screenshot of your plot to @Luc Kunz on Slack to get the point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYW9a6qgEPnj"
      },
      "source": [
        "## 3. Customer segmentation\n",
        "We now implement customer segmentation with the two algorithms. The data set can be found [here](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcsUYpNfGLBI"
      },
      "source": [
        "### 3.1 Load, prepare, and explore data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbBoV9aywVOq"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/week9/data/Mall_Customers.csv', index_col=0)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIcKdZghbkN"
      },
      "source": [
        "We have 200 customers with their gender, age, annual income, and spending score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgFn2utoEafN"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjaYIlDfGc7J"
      },
      "source": [
        "# Convert Gender to numerical\n",
        "df.Gender = df.Gender.astype('category').cat.codes\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7IVdFIqGJVC"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwvV0lEqAw1U"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDYZXr2QAwxr"
      },
      "source": [
        "# Univariate Analysis\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
        "i = 0\n",
        "j = 0\n",
        "for var in df:\n",
        "  if var == \"Gender\":\n",
        "    sns.countplot(x=df[var], ax=ax[i, j], palette='Dark2')\n",
        "  else:\n",
        "    sns.histplot(df[var], ax=ax[i, j], color=(0.4, 0.3, 0.7))\n",
        "  i += 1\n",
        "  if i == 2:\n",
        "    i = 0\n",
        "    j += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnm_nwREAwt0"
      },
      "source": [
        "# Bivariate Analysis\n",
        "plt.figure(figsize=(15,15))\n",
        "sns.pairplot(df, hue='Gender', palette='Dark2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkye7zJWh0jt"
      },
      "source": [
        "Using `Annual Income` and `Spending Score`, we can observe 5 clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blRRuPmcIkWV"
      },
      "source": [
        "### 3.2 k-means algorithms\n",
        "Again, we first want to find out the good number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setautyAIcV_"
      },
      "source": [
        "# Function to plot inertia\n",
        "def k_means_inertia_plot(variables):\n",
        "\n",
        "  # Create new dataframe\n",
        "  new_df = df[variables]\n",
        "\n",
        "  # Fit k-means algorithm for different k\n",
        "  inertia = []\n",
        "  k_values = range(2, 20)\n",
        "  for k in k_values:\n",
        "    model = KMeans(n_clusters=k)\n",
        "    model.fit(new_df)\n",
        "    inertia.append(model.inertia_)\n",
        "\n",
        "  # Plot\n",
        "  plt.figure(figsize=(10,6))\n",
        "  plt.plot(k_values, inertia, marker='o', linestyle='solid')\n",
        "  plt.title('Inertia for different k values')\n",
        "  plt.ylabel('Inertia')\n",
        "  plt.xlabel('k')\n",
        "  plt.xticks(np.arange(min(k_values)-1, max(k_values)+1, step=1))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVgjgHfVAwoZ"
      },
      "source": [
        "# Use function - two variables\n",
        "k_means_inertia_plot([\"Annual Income (k$)\", \"Spending Score (1-100)\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W993tofPZuUI"
      },
      "source": [
        "This confirms what we saw in the EDA. Using these two variables, 5 clusters are optimal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZC881HnLpvs"
      },
      "source": [
        "# Use function - three variables\n",
        "k_means_inertia_plot([\"Annual Income (k$)\", \"Spending Score (1-100)\", \"Age\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAsaX63aZwIM"
      },
      "source": [
        "With three variables, this is less clear. Maybe 6 clusters would be great."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRNLUrrhLpqX"
      },
      "source": [
        "# Use function - four variables\n",
        "k_means_inertia_plot([\"Annual Income (k$)\", \"Spending Score (1-100)\", \"Age\", \"Gender\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa6XBN2HZzDB"
      },
      "source": [
        "Again, not clear with all four variables. Maybe 6 clusters would be good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnE6QtftGEaR"
      },
      "source": [
        "# Plot clusters for k = 2,3,4,5,6 and two variables\n",
        "new_df = df[['Annual Income (k$)', 'Spending Score (1-100)']]\n",
        "\n",
        "fig, ax = plt.subplots(5,1, figsize=(10,40))\n",
        "for k in [2, 3, 4, 5, 6]:\n",
        "  model = KMeans(n_clusters=k)\n",
        "  model.fit(new_df)\n",
        "  centers = model.cluster_centers_\n",
        "  pred = model.predict(new_df)\n",
        "  ax[k-2].scatter(new_df['Annual Income (k$)'], new_df['Spending Score (1-100)'], marker='x', c=pred)\n",
        "  for c in range(k):\n",
        "    ax[k-2].scatter(centers[c][0], centers[c][1], color='red', marker=',')\n",
        "  ax[k-2].set_title('k-means algorithm with k = ' + str(k))\n",
        "  ax[k-2].set_xlabel(\"Annual Income (k$)\")\n",
        "  ax[k-2].set_ylabel(\"Spending Score (1-100)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-HD7FGwZAkt"
      },
      "source": [
        "As already mentioned, the optimal number of cluster is 5 in this case. Once clustering is done, we may then use the result to label (i.e. classifiy) the customers, for example:\n",
        "* Customers who earn little and spend little (the `reasonable poor`)\n",
        "* Customer who earn little and spend a lot (the `senseless poor`)\n",
        "* Customer who earn average and spend average (the `average people`)\n",
        "* Customers who earn a lot and spend a lot (the `rich`). The marketing effort should focus mainly on them.\n",
        "* Customer who earn a lot and spend little (the `stingy`).\n",
        "\n",
        "With three variables and 6 clusters: [link](https://www.kaggle.com/kushal1996/customer-segmentation-k-means-analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4fN-neQU-jM"
      },
      "source": [
        "### 3.3 Hierarchical clustering\n",
        "We now do the same with hierarchical clustering. This yields similar results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7_I7zFfjmMz"
      },
      "source": [
        "# Plot with diiferent k and two variables\n",
        "fig, ax = plt.subplots(5,1, figsize=(10,40))\n",
        "for k in [2, 3, 4, 5, 6]:\n",
        "  clustering = AgglomerativeClustering(n_clusters=k, linkage='complete', affinity='euclidean')\n",
        "  assignment = clustering.fit_predict(new_df)\n",
        "  ax[k-2].scatter(new_df['Annual Income (k$)'], new_df['Spending Score (1-100)'], marker='x', c=assignment)\n",
        "  ax[k-2].set_title('Hierarchical Clustering with k = ' + str(k))\n",
        "  ax[k-2].set_xlabel(\"Annual Income (k$)\")\n",
        "  ax[k-2].set_ylabel(\"Spending Score (1-100)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af2-marEGzMV"
      },
      "source": [
        "# Plot dendrogram - complete\n",
        "plt.figure(figsize=(10,6))\n",
        "dendrogram = sch.dendrogram(sch.linkage(new_df, method = 'complete'))\n",
        "plt.title('Dendrogram - complete linkage')\n",
        "plt.xlabel('Customer')\n",
        "plt.ylabel('Euclidean distances')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_ODqCCuGPwH"
      },
      "source": [
        "# Plot dendrogram - single\n",
        "plt.figure(figsize=(10,6))\n",
        "dendrogram = sch.dendrogram(sch.linkage(new_df, method = 'single'))\n",
        "plt.title('Dendrogram - single linkage')\n",
        "plt.xlabel('Customer')\n",
        "plt.ylabel('Euclidean distances')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IWskeLWF7u7"
      },
      "source": [
        "It does not work with single linkage. With complete linkage, we can see the 5 clusters on the dendrogram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1KnnOL8-dLb"
      },
      "source": [
        "## 4. Clustering vs. classification\n",
        "We compare classification and clustering algortihms with labelled data. We use a data set of emails."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAer-XqDJK-S"
      },
      "source": [
        "### 4.1 Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFU81S6hJKnb"
      },
      "source": [
        "emails = pd.read_csv('https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/week9/data/spambase.csv')\n",
        "emails.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JruxcThGd9S"
      },
      "source": [
        "emails.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DNnEWJ0L4ly"
      },
      "source": [
        "# Base rate\n",
        "print(emails[\"class\"].value_counts())\n",
        "len(emails[emails[\"class\"] == 0]) / len(emails)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S4FPIR5Jpxd"
      },
      "source": [
        "### 4.2 Algorithms comparison: k-means vs. logistic regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0Ce3W1bLzl7"
      },
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Select variables\n",
        "X = emails.drop(\"class\", axis = 1)\n",
        "y = emails[\"class\"]\n",
        "\n",
        "# k-means\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "\n",
        "# Logistic regression\n",
        "logit = LogisticRegression()\n",
        "\n",
        "algorithms = [kmeans, logit]\n",
        "\n",
        "for algo in algorithms:\n",
        "\n",
        "  # K-fold cross validation\n",
        "  kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "  # Conduct k-fold cross-validation\n",
        "  cv_results = cross_val_score(algo, # Algorithm\n",
        "                             X, # Feature matrix\n",
        "                             y, # Target vector\n",
        "                             cv=kf, # Performance metric\n",
        "                             scoring=\"accuracy\", # Loss function\n",
        "                             n_jobs=-1) # Use all CPU cores\n",
        "\n",
        "  # Print mean\n",
        "  print (\"The Mean Score is\", round(cv_results.mean(), 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJMZtYRtP4Hn"
      },
      "source": [
        "Conclusion: In general, classification algorithms are more precise than clustering for classification problems. This means as soon as you have labelled data, you should use classification. When there are no label or if you want to label unlabelled data, you can use clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDtMJ-jEv_pm"
      },
      "source": [
        "## References\n",
        "* https://www.kaggle.com/kushal1996/customer-segmentation-k-means-analysis\n",
        "* https://www.kaggle.com/morecoding/hierarchical-clustering"
      ]
    }
  ]
}