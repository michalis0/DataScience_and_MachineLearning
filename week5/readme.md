# Linear Regression 2

In this week's lab we are going to cover the following topics:
- Training a linear regression model on a dataset with several features. We try to cover a complete pipline of solving such problem, from pre-processing and feature engineering to model selection, regularization, and cross-validation.
- How to deal with categorical features in the data.
- Performing a simple feature engineering. Pre selecting a subset of solumns based on their correlation to the target variable.
- Splitting the data to train and test sets.
- Training a ridge regression model.
- Normalization of the data.
- Observing the effect of the regularization hyper-parameter.
- Finding the best hyper-parameter using cross-validation.
- Diving deeper into polynomial features and the concepts of generalization and over-fitting.


## To Do
- Start with the notebook ["Ridge_Regression.ipynb"](https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week4/Linear_Regression_Basics.ipynb#scrollTo=f7a_hEfThRmX). In this notebook, we try to predict the house prices using a linear model. This time, we will be using all of the features in the data and will learn how to preprocess the features and how to select a subset of features that can improve the regression score. We then train a ridge regression model and observe the effect of the regularization hyper-parameter.

- Follow the notebook ["Linear_Regression_Deeper.ipynb"](https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week4/Linear_Regression_Deeper.ipynb#scrollTo=1K6vWbOsh95y) to get a deeper understanding of the overfitting and generalisation concepts.
