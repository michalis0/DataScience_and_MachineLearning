{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Text_Analytics.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "h0VsVUPLT4Tn",
        "RXlVy05IUC-D",
        "1QZOutDwsYij"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week6/Text_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onO46LysT2dh"
      },
      "source": [
        "# Data Mining and Machine Learning - Week 6\n",
        "# Text Analytics\n",
        "\n",
        "[Text Analytics](https://people.ischool.berkeley.edu/~hearst/text-mining.html) (or text mining) is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" Written resources may include websites, books, emails, reviews, and articles.\n",
        "\n",
        "### Table of Contents\n",
        "#### 1. Summary\n",
        "* 1.1 Applications\n",
        "* 1.2 Tokenization and Stopwords\n",
        "* 1.3 Stemming and Lemmatization\n",
        "* 1.4 Text Representation\n",
        "\n",
        "#### 2. Text Preparation\n",
        "* 2.1 Install spaCy\n",
        "* 2.2 Tokenization\n",
        "* 2.3 Dependency Parsing\n",
        "* 2.4 Remove Stopwords\n",
        "* 2.5 Lemmatization\n",
        "* 2.6 Entity Detection\n",
        "* 2.7 Exercise\n",
        "* 2.8 Solution\n",
        "\n",
        "#### 3. Text Representation\n",
        "* 3.1 Bag of Words (BOW)\n",
        "* 3.2 TF-IDF Representation\n",
        "* 3.3 Exercise\n",
        "* 3.4 Solution\n",
        "\n",
        "#### 4. Text Classification: Alexa Reviews\n",
        "* 4.1 Load and prepare data\n",
        "* 4.2 Classification of the reviews using logistic regression\n",
        "* 4.3 How can we improve the accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0VsVUPLT4Tn"
      },
      "source": [
        "## 1. Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHvrJj5yT8uQ"
      },
      "source": [
        "### 1.1 Applications\n",
        "There are many applications of text analytics, for example:\n",
        "* Search for relevant websites or articles using a search engine\n",
        "* Sentiment Analysis (e.g. classify tweets or film reviews as positive, neutral or negative)\n",
        "* Chatbots (e.g. Siri, Alexa)\n",
        "* Project idea: The Impact of Donald Trump’s Tweets on Financial Markets\n",
        "* Etc.\n",
        "\n",
        "### 1.2 Tokenization and Stopwords\n",
        "Tokens are the elementary building blocks (words, numbers, characters) in a document. Tokenization is the process of splitting an input\n",
        "sequence into tokens. Example: \"I love data science\" --> \"I\", \"love\", \"data\", \"science\". Stopwords are common words that appear very frequently (e.g. \"is\", \"and\", \"you\", etc.). It is convenient to remove them as they do not add much to the content of a document and are therefore generally not useful for text analysis or, worse still, make it worse by adding noise.\n",
        "\n",
        "### 1.3 Lemmatization and Stemming\n",
        "* Goal: have the same token for different forms of a word (e.g. fishing, fished, fisher, fishers, etc.)\n",
        "* Lemmatization: Find what is the lemma of a word (e.g. feet -> foot)\n",
        "* Stemming: one method for lemmatization where rules that remove the ending of a word are applied (e.g. fishing -> fish)\n",
        "\n",
        "\n",
        "### 1.4 Text Representation\n",
        "* Goal: transform text such that it can be used for text analysis\n",
        "* Bag of Words (BOW): works in many case but order is not preserved (solution: n-grams)\n",
        "* TF-IDF: emphasizes important words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXlVy05IUC-D"
      },
      "source": [
        "## 2. Text Preparation\n",
        "In this section, we explain how to prepare a text for analysis. This includes tockeninzing the text, removing stopwords, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA4tt9roTp8T"
      },
      "source": [
        "### 2.1 Install spaCy\n",
        "[spaCy](https://spacy.io/) is an open-source natural language processing library for Python. It is designed particularly for production use, and it can help us to build applications that process massive volumes of text efficiently.\n",
        "\n",
        "We install the library and its English-language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf6I0JIYTp8U"
      },
      "source": [
        "# Install and update spaCy\n",
        "!pip install -U spacy\n",
        "\n",
        "# Download the english language model\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhOeeRlqTp8Z"
      },
      "source": [
        "# Import required packages\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUibE-SyTp8d"
      },
      "source": [
        "### 2.2 Tokenization\n",
        "\n",
        "Tokenization is the process of breaking a text into pieces called tokens. A token simply refers to an individual part of a sentence having some semantic value. SpaCy‘s tokenizer takes input in form of unicode text and outputs a sequence of token objects. In addition, SpaCy automatically breaks your document into tokens when a document is created using the language model.\n",
        "\n",
        "Let’s take a look at a simple example. Imagine we have the following text, and we would like to tokenize it:\n",
        "\n",
        "> When learning data science, you shouldn't get discouraged!\n",
        "\n",
        "> Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\n",
        "\n",
        "There are a couple of different ways we can appoach this. The first is called __word tokenization__, which means breaking up the text into individual words. This is a critical step for many language processing applications, as they often require inputs in the form of individual words rather than longer strings of text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj9OfB1BTp8e"
      },
      "source": [
        "# Load English language model\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Declare text\n",
        "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
        "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
        "\n",
        "# spaCy object is used to create a document\n",
        "my_doc = sp(text)\n",
        "\n",
        "my_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc7fwTbPZdXJ"
      },
      "source": [
        "# This is a spaCy document\n",
        "type(my_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex7-T-BkYXZM"
      },
      "source": [
        "# Create list of tokens\n",
        "token_list = []\n",
        "\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi29AcSsTp8j"
      },
      "source": [
        "As we can see, spaCy produces a list that contains each token as a separate item. Notice that it has recognized that contractions such as _shouldn’t_ actually represent two distinct words, and has thus broken them down into two distinct tokens.\n",
        "\n",
        "In the example above, we first load the language dictionary. Here we load the english one using `spacy.load('en_core_web_sm')` create an object of this class, \"sp\", which is used to create documents with linguistic annotations and various language properties. After creating the document, we create a list of tokens.\n",
        "\n",
        "We can also see the parts-of-speech (POS) of each of these tokens using the `.pos_` attribute, as shown below. POS tagging can be really useful, particularly if you have words or tokens that can have multiple POS tags. For instance, the word \"fish\" can be used as both a noun and verb, depending upon the context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvkGwDJoWoIs"
      },
      "source": [
        "# POS\n",
        "for word in my_doc:\n",
        "    print(word.text, word.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUsKHgp7Y3yM"
      },
      "source": [
        "# Another example\n",
        "doc1 = sp(\"I like to fish\") # verb\n",
        "doc2 = sp(\"I eat a fish\") # noun\n",
        "\n",
        "for word in doc1:\n",
        "  print(word.text, word.pos_)\n",
        "\n",
        "print(\"-----------------\")\n",
        "\n",
        "for word in doc2:\n",
        "  print(word.text, word.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCiU1-PDXKpp"
      },
      "source": [
        "\n",
        "If we want, we can also break the text into sentences rather than words. This is called __sentence tokenization__. When performing sentence tokenization, the tokenizer looks for specific characters that normally fall between sentences, like periods, exclaimation points, and newline characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odi_WkNZTp8k"
      },
      "source": [
        "# create list of sentence tokens\n",
        "sents_list = []\n",
        "\n",
        "for sent in my_doc.sents:\n",
        "    sents_list.append(sent.text)\n",
        "\n",
        "sents_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te403rkXbGKf"
      },
      "source": [
        "### 2.3 Dependency Parsing\n",
        "__Depenency parsing__ is a language processing technique that allows to better determine the meaning of a sentence by analyzing how it is constructed to determine how the individual words relate to each other.\n",
        "\n",
        "Consider, for example, the sentence “Joe throws the ball.” We have two nouns (Joe and ball) and one verb (throws). But we can’t just look at these words individually, or we may end up thinking that the ball throws Joe! To understand the sentence correctly, we need to look at the word order and sentence structure, not just the words.\n",
        "\n",
        "Below, we have a short sentence. We’ll use a spaCy method called `noun_chunks`, which breaks the input down into nouns and the words describing them, and iterate through each chunk in our source text, identifying the word, its root, its dependency identification, and which chunk it belongs to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFLButK_bOor"
      },
      "source": [
        "doc = sp(\" Joe threw a ball, and President Donald, in pursuit of the ball, hit a wall.\") # notice the space at the beginning\n",
        "\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "          chunk.root.head.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VmqTQQ9bYTl"
      },
      "source": [
        "# Let's visualize this\n",
        "displacy.render(doc, style=\"dep\", jupyter= True, options={'distance': 120})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Z6MiCtTp8n"
      },
      "source": [
        "### 2.4 Remove Stopwords\n",
        "Most text data that we work with is going to contain a lot of words that are not actually useful to us. These words, called stopwords, are useful in human speech, but they do not have much to contribute to data analysis. Removing stopwords helps us eliminate noise and distraction from our text data, and also speeds up the time analysis takes (since there are fewer words to process). This makes text analysis more efficient.\n",
        "\n",
        "Let’s take a look at the stopwords spaCy includes by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kB8kI0eTp8o"
      },
      "source": [
        "# Import stopwords from English language\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Print total number of stopwords\n",
        "print('Number of stopwords: %d' % len(spacy_stopwords))\n",
        "\n",
        "# Print 20 stopwords\n",
        "print('20 stopwords: %s' % list(spacy_stopwords)[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw0Wfvu4Tp8q"
      },
      "source": [
        "Now that we’ve got our list of stopwords, let’s use it to remove the stopwords from the text string we were working on in the previous section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq5yvoZtlhsd"
      },
      "source": [
        "# Which words will be removed?\n",
        "my_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y54Kiz9zTp8r"
      },
      "source": [
        "# Declare list for filtered sentence\n",
        "filtered_sent = []\n",
        "\n",
        "# Filter stopwords\n",
        "for word in my_doc:\n",
        "    if word.is_stop == False:\n",
        "        filtered_sent.append(word.text)\n",
        "\n",
        "filtered_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOvM2NFdl29W"
      },
      "source": [
        "# We can also remove the punctuation\n",
        "filtered_sent2 = []\n",
        "removed_tokens = []\n",
        "\n",
        "# Filter stopwords, punctuation and spaces\n",
        "for word in my_doc:\n",
        "  if (word.is_stop == True) or (word.is_punct == True) or (word.is_space == True):\n",
        "    removed_tokens.append(word.text)\n",
        "  else:\n",
        "    filtered_sent2.append(word.text)\n",
        "\n",
        "removed_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw-ebvZom1EP"
      },
      "source": [
        "filtered_sent2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imbw_TUkTp8v"
      },
      "source": [
        "### 2.5 Lemmatization\n",
        "Lemmatization is a way of dealing with the fact that while words like connect, connection, connecting, connected, etc. aren’t exactly the same, they all have the same essential meaning: connect. The differences in spelling have grammatical functions in spoken language, but for machine processing, those differences can be confusing, so we need a way to change all the words that are forms of the word connect into the word connect itself.\n",
        "\n",
        "One method for doing this is called __stemming__. Stemming involves simply lopping off easily-identified prefixes and suffixes to produce what’s often the simplest version of a word, the root. Connection, for example, would have the -ion suffix removed and be correctly reduced to connect. This kind of simple stemming is often all that’s needed, but lemmatization—which actually looks at words and their roots (called lemma) as described in the dictionary—is more precise (e.g feet -> foot).\n",
        "\n",
        "Let's look at this simple example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTqSImYfTp8v"
      },
      "source": [
        "# Lemmatization\n",
        "lem = sp(\"run runs ran running runner runners\")\n",
        "\n",
        "# Find lemma for each word\n",
        "for word in lem:\n",
        "    print(word.text, word.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7J5Na4DTp84"
      },
      "source": [
        "### 2.6 Entity Detection\n",
        "\n",
        "__Entity detection__, also called entity recognition, is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within a text. This is really helpful for quickly extracting information from the text, since you can quickly pick out important topics or indentify key sections of it.\n",
        "\n",
        "Let’s try out some entity detection using a few paragraphs from this [article](https://www.bloomberg.com/features/trump-tweets-market/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvlb9S71Tp84"
      },
      "source": [
        "article = sp(\"\"\"\n",
        "President Donald Trump gets a lot of attention for using Twitter to attack American trading partners, political foes, and media companies. But he often takes to the platform to celebrate the strength of the world’s largest economy and its publicly-traded companies.\n",
        "\n",
        "Before U.S. stocks peaked in late January, he drew a direct connection between the increase in market value of American companies and his administration’s pro-growth policies on more than 10 occasions in that month alone.\n",
        "\"\"\")\n",
        "\n",
        "entities = [(i, i.label_, i.label) for i in article.ents]\n",
        "entities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1ScYkq7Tp87"
      },
      "source": [
        "The above example how spaCy is able to identify a variety of different entity types, including specific locations (GPE), date-related words (DATE), important numbers (CARDINAL), specific individuals (PERSON), etc.\n",
        "\n",
        "Using `displaCy` we can also visualize the text, with each identified entity highlighted by a color and labeled. We’ll use `style=\"ent\"` to tell displaCy that we want to visualize entities here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGDOBT6zTp88"
      },
      "source": [
        "displacy.render(article, style=\"ent\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erd3X6Q-n1_Y"
      },
      "source": [
        "### 2.7 Exercise\n",
        "For each word in the sentence below, print its lemma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyJOSspCn7W6"
      },
      "source": [
        "sentence = \"\"\"The happiness of your life depends upon the quality of your thoughts: therefore, guard accordingly, and take care that you entertain no notions unsuitable to virtue and reasonable nature.\"\"\"\n",
        "\n",
        "# Print lemma\n",
        "# [YOUR CODE HERE]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRR_jyLPp8Q2"
      },
      "source": [
        "Create two lists, the first one containing the punctuation and the second one the words (tokens)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVwmiV9_n7SU"
      },
      "source": [
        "# [YOUR CODE HERE]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKE3SNk6aHvL"
      },
      "source": [
        "### 2.8 Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDVhIdRwaHAG"
      },
      "source": [
        "# Lemma\n",
        "doc = sp(sentence)\n",
        "for word in doc:\n",
        "  print(word.text, word.lemma_)\n",
        "\n",
        "# punct and tokens\n",
        "punct = []\n",
        "tokens = []\n",
        "for word in doc:\n",
        "  if word.is_punct == True:\n",
        "    punct.append(word)\n",
        "  elif word.is_space == False:\n",
        "    tokens.append(word)\n",
        "\n",
        "print(punct)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QZOutDwsYij"
      },
      "source": [
        "## 3. Text Representation\n",
        "We now show how to transform a text into an usable input for text classification. We use the first sentence of the article from the last section and two other sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2MjeqoQxdOx"
      },
      "source": [
        "# Article as a string, not a spacy object\n",
        "article = \"\"\"\n",
        "President Donald Trump gets a lot of attention for using Twitter to attack American trading partners, political foes, and media companies.\"\"\"\n",
        "\n",
        "# Sentences\n",
        "s1 = \"\"\"Donald Trump is a great friend, and he has four or five Picassos on his plane. And that's where I would look at them.\"\"\" # from Shaquille O'Neal\n",
        "s2 = \"\"\"Donald Trump is a phony, a fraud. His promises are as worthless as a degree from Trump University.\"\"\" # from Mitt Romney\n",
        "\n",
        "# List of sentences\n",
        "texts = [article, s1, s2]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCFQrtHLtMTt"
      },
      "source": [
        "### 3.1 Bag of Words (BOW)\n",
        "We use the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class of scikit learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyzBm0BUtLc0"
      },
      "source": [
        "# Using default tokenizer \n",
        "count = CountVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
        "bow = count.fit_transform(texts)\n",
        "\n",
        "# Show feature matrix\n",
        "bow.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRmUUc0kzi3_"
      },
      "source": [
        "# Get feature names\n",
        "feature_names = count.get_feature_names()\n",
        "\n",
        "# View feature names\n",
        "feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxCSvGUTz2tl"
      },
      "source": [
        "# Show as a dataframe\n",
        "pd.DataFrame(\n",
        "    bow.todense(), \n",
        "    columns=feature_names\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7tNGrTutQ2r"
      },
      "source": [
        "### 3.2 TF-IDF Representation\n",
        "\n",
        "\n",
        "Recall that:\n",
        "\n",
        "- term frequency tf = count(word, document) / len(document) \n",
        "- term frequency idf = log( len(collection) / count(document_containing_term, collection) )\n",
        "- tf-idf = tf * idf \n",
        "\n",
        "It is important to mention that the IDF value for a word remains the same throughout all the documents as it depends upon the total number of documents. On the other hand, TF values of a word differ from document to document.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djkUI6hXtWlr"
      },
      "source": [
        "# Using default tokenizer in TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcyNjGlfHjlo"
      },
      "source": [
        "texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcrRL3rRqgBy"
      },
      "source": [
        "### 3.3 Exercise\n",
        "Create a TF-IDF Representation of the three above sentences using bigrams and the following stopwords: [\"and\", \"a\", \"is\"]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzo8uviDrTLm"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYgMIUcVbCC6"
      },
      "source": [
        "### 3.4 Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEUWiXjubBn_"
      },
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(2, 2), stop_words=[\"and\", \"a\", \"is\"])\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARu2ONJb06Sr"
      },
      "source": [
        "## 4. Text Classification: Alexa reviews\n",
        "\n",
        "We are going to use a real-world data set: [Amazon Alexa product reviews](https://www.kaggle.com/bittlingmayer/amazonreviews).\n",
        "\n",
        "This data set comes as a tab-separated file (.tsv). It has has five columns: `rating`, `date`, `variation`, `verified_reviews`, `feedback`.\n",
        "\n",
        "`rating` denotes the rating each user gave Alexa (out of 5). `date` indicates the date of the review, and `variation` describes which model the user reviewed. `verified_reviews` contains the text of each review, and `feedback` contains a sentiment label, with 1 denoting positive sentiment (the user liked it) and 0 denoting negative sentiment (the user didn’t).\n",
        "\n",
        "We are going develop a classification model that looks at the review text and predicts whether a review is positive or negative. Since this data set already includes whether a review is positive or negative in the `feedback` column, we can use those answers to train and test our model. Our goal here is to produce an accurate model that we could then use to process new user reviews and quickly determine whether they were positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzfnR4ua2uLw"
      },
      "source": [
        "### 4.1 Load and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rS4hAiA05wQ"
      },
      "source": [
        "# Import additional packages\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxrlx4eD0Ygk"
      },
      "source": [
        "# Load data\n",
        "url = \"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/week6/data/amazon_alexa.tsv\"\n",
        "df = pd.read_csv(url, delimiter=\"\\t\")\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNjV_B2E3nLd"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6hCSlbq30UT"
      },
      "source": [
        "# Change date to datetime\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP8sYT2y34CU"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK8u3mNw35HC"
      },
      "source": [
        "# Base rate: the data-set is unbalanced!\n",
        "df.feedback.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d4NPeOq4A0X"
      },
      "source": [
        "round(df.feedback.value_counts()[1] / len(df), 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iFd7_hl4eNJ"
      },
      "source": [
        "###### Tokening the Data With spaCy\n",
        "\n",
        "We create a `spacy_tokenizer()` function that accepts a sentence as input and processes the sentence into tokens, performing lemmatization, lowercasing, and removing stopwords.\n",
        "\n",
        "__A note from spacy documentation__: spaCy adds a special case for pronouns: all pronouns are lemmatized to the special token `-PRON-`. Unlike verbs and common nouns, there’s no clear base form of a personal pronoun. Should the lemma of “me” be “I”, or should we normalize person as well, giving “it” — or maybe “he”? spaCy’s solution is to introduce a novel symbol, `-PRON-`, which is used as the lemma for all personal pronouns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrHhCTFN5ZXw"
      },
      "source": [
        "# Create a list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "punctuations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCVM6xVQ5fP2"
      },
      "source": [
        "# Create a list of stopwords\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "list(stop_words)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_ALYORw4LN0"
      },
      "source": [
        "# Load English language model\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = sp(sentence)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens\n",
        "\n",
        "# Example\n",
        "review = df[\"verified_reviews\"].sample()\n",
        "review.values[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlOwdF_16JV-"
      },
      "source": [
        "spacy_tokenizer(review.values[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ5Yl8uR9eYv"
      },
      "source": [
        "#### Vectorization Feature Engineering (TF-IDF)\n",
        "\n",
        "We use the TF-IDF (Term Frequency-Inverse Document Frequency) to vectorize the documents. This is a way of representing how important a particular term is in the context of a given document, based on how many times the term appears and how many other documents that same term appears in. The higher the TF-IDF, the more important that term is to that document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-874sxt8iNM"
      },
      "source": [
        "tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer) # we use the above defined tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jO56WUV9yXn"
      },
      "source": [
        "### 4.2 Classification of the reviews using logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7JkcGTb9ty9"
      },
      "source": [
        "# Select features\n",
        "X = df['verified_reviews'] # the features we want to analyze\n",
        "ylabels = df['feedback'] # the labels, or answers, we want to test against\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=1234)\n",
        "\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6dbSYsF-HRi"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB8PkfsP-JYi"
      },
      "source": [
        "# Define classifier\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e07kHgHb-hC2"
      },
      "source": [
        "# Evaluate the model\n",
        "def evaluate(true, pred):\n",
        "    precision = precision_score(true, pred)\n",
        "    recall = recall_score(true, pred)\n",
        "    f1 = f1_score(true, pred)\n",
        "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(true, pred)}\")\n",
        "    print(f\"ACCURACY SCORE:\\n{accuracy_score(true, pred):.4f}\")\n",
        "    print(f\"CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PjJWqHp_HvQ"
      },
      "source": [
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation - test set\n",
        "evaluate(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRrITD4y_nbb"
      },
      "source": [
        "For the test set, the model correctly identifies a sentiment 92.22% of the time. This is only slightly better than the base rate (91.84%). The difference lies in the two cases that the model correctly classifies as negative. Therefore, the model does not work very well. This may be due to the fact that we have an unbalanced sample with too much positive reviews. Maybe the model cannot learn how to classify negative reviews well since there are too few examples of them.\n",
        "\n",
        "A recall of 1 means that if a sentiment is positive, it will be classififed as positive.\n",
        "\n",
        "We observe approximately the same on the training set, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeB_oSPKP7vZ"
      },
      "source": [
        "# Evaluate on training set\n",
        "evaluate(y_train, pipe.predict(X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZAzALsgXu_v"
      },
      "source": [
        "# Which reviews are classified as negative in test set?\n",
        "X_test.loc[pipe.predict(X_test) == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBMomG3RYeo2"
      },
      "source": [
        "# Which reviews are classified as negaive in training set?\n",
        "X_train.loc[pipe.predict(X_train) == 0].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SnrtzOhS9cP"
      },
      "source": [
        "# Prediction for new reviews\n",
        "example_review_1 = \"I really love the product. It is very helpful. I use it everyday\" # positive\n",
        "example_review_2 = \"It stopped working, I want to return it\" # negative\n",
        "example_review_3 = \"I don't like it, it is bad\" # negative\n",
        "\n",
        "examples = pd.Series([example_review_1, example_review_2, example_review_3])\n",
        "examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrnvd-ZPT7aR"
      },
      "source": [
        "pipe.predict(examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHne-tgCScWk"
      },
      "source": [
        "### 4.3 How can we improve the accuracy?\n",
        "In order to improve the prediction, we can try to:\n",
        "* Resample our data (i.e. add negative examples for better training)\n",
        "* Tun the hyperparameters of the model\n",
        "* Improve text preparation\n",
        "* Use another classififer\n",
        "\n",
        "We illustrate how to improve text preparation, resampling, and the use of another classifier below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyEyaUltsGNB"
      },
      "source": [
        "#### 4.3.1 Improve text preparation\n",
        "The purpose here is to optimize the parameters of the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0KXBFJyuKWn"
      },
      "source": [
        "# Create list of configs\n",
        "def configs():\n",
        "\n",
        "    models = list()\n",
        "    \n",
        "    # Define config lists\n",
        "    ngram_range = [(1,1), (1,2), (1, 3), (2, 2), (2, 3), (3, 3)]\n",
        "    min_df = [1]\n",
        "    max_df = [1.0]\n",
        "    analyzer=['word', 'char']\n",
        "    \n",
        "    # Create config instances\n",
        "    for n in ngram_range:\n",
        "        for i in min_df:\n",
        "            for j in max_df:\n",
        "              for a in analyzer:\n",
        "                    cfg = [n, i, j, a]\n",
        "                    models.append(cfg)\n",
        "    return models\n",
        "\n",
        "configs = configs()\n",
        "configs[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zypqialXQZTZ",
        "outputId": "3b12abf3-cb8b-452f-fbd3-44b32d7383e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define list for result\n",
        "result = []\n",
        "\n",
        "for config in configs:\n",
        "\n",
        "    # Redefine vectorizer\n",
        "    tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer, \n",
        "                                   ngram_range=config[0],\n",
        "                                   min_df=config[1], max_df=config[2], analyzer=config[3])\n",
        "\n",
        "    # Define classifier\n",
        "    classifier = LogisticRegression()\n",
        "\n",
        "    # Create pipeline\n",
        "    pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "    # Fit model on training set\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = pipe.predict(X_test)\n",
        "\n",
        "    # Print accuracy on test set\n",
        "    print(\"CONFIG: \", config)\n",
        "    evaluate(y_test, y_pred)\n",
        "    print(\"-----------------------\")\n",
        "\n",
        "    # Append to result\n",
        "    result.append([config, accuracy_score(y_test, y_pred)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 1), 1, 1.0, 'word']\n",
            "CONFUSION MATRIX:\n",
            "[[  2  49]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9222\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9220\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9594\n",
            "-----------------------\n",
            "CONFIG:  [(1, 1), 1, 1.0, 'char']\n",
            "CONFUSION MATRIX:\n",
            "[[  0  51]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9190\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9190\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9578\n",
            "-----------------------\n",
            "CONFIG:  [(1, 2), 1, 1.0, 'word']\n",
            "CONFUSION MATRIX:\n",
            "[[  0  51]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9190\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9190\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9578\n",
            "-----------------------\n",
            "CONFIG:  [(1, 2), 1, 1.0, 'char']\n",
            "CONFUSION MATRIX:\n",
            "[[  0  51]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9190\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9190\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9578\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 1.0, 'word']\n",
            "CONFUSION MATRIX:\n",
            "[[  0  51]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9190\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9190\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9578\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 1.0, 'char']\n",
            "CONFUSION MATRIX:\n",
            "[[  0  51]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9190\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9190\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9578\n",
            "-----------------------\n",
            "CONFIG:  [(2, 2), 1, 1.0, 'word']\n",
            "CONFUSION MATRIX:\n",
            "[[  0  51]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9190\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9190\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9578\n",
            "-----------------------\n",
            "CONFIG:  [(2, 2), 1, 1.0, 'char']\n",
            "CONFUSION MATRIX:\n",
            "[[  1  50]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9206\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9205\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9586\n",
            "-----------------------\n",
            "CONFIG:  [(2, 3), 1, 1.0, 'word']\n",
            "CONFUSION MATRIX:\n",
            "[[  0  51]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9190\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9190\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9578\n",
            "-----------------------\n",
            "CONFIG:  [(2, 3), 1, 1.0, 'char']\n",
            "CONFUSION MATRIX:\n",
            "[[  1  50]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9206\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9205\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9586\n",
            "-----------------------\n",
            "CONFIG:  [(3, 3), 1, 1.0, 'word']\n",
            "CONFUSION MATRIX:\n",
            "[[  0  51]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9190\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9190\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9578\n",
            "-----------------------\n",
            "CONFIG:  [(3, 3), 1, 1.0, 'char']\n",
            "CONFUSION MATRIX:\n",
            "[[  0  51]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9190\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9190\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9578\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Boqm0jsz4CvZ"
      },
      "source": [
        "Our tries do not work, we have to try further or use something else to improve prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pygcSii_4SF4"
      },
      "source": [
        "#### 4.3.2 Resampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L27GTYuFNRJU"
      },
      "source": [
        "# Create balanced dataframe - base rate = 0.5\n",
        "df_new = pd.concat([df[df[\"feedback\"] == 1].sample(len(df[df[\"feedback\"] == 0])), df[df[\"feedback\"] == 0]], axis=0).reset_index()\n",
        "df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzVuGW63NRF3"
      },
      "source": [
        "# Select features\n",
        "X = df_new['verified_reviews'] # the features we want to analyze\n",
        "ylabels = df_new['feedback'] # the labels, or answers, we want to test against\n",
        "\n",
        "# Train test split\n",
        "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X, ylabels, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Define classifier\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_b, y_train_b)\n",
        "\n",
        "# Predictions\n",
        "y_pred_b = pipe.predict(X_test_b)\n",
        "\n",
        "# Evaluation - test set\n",
        "evaluate(y_test_b, y_pred_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBKezH2Ctl3y"
      },
      "source": [
        "#### 4.3.3 Use another classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRze7VxPzZ14"
      },
      "source": [
        "# Use random forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define vectorizer\n",
        "tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer) # we use the above defined tokenizer\n",
        "\n",
        "# Define classifier\n",
        "classifier = RandomForestClassifier()\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation - test set\n",
        "evaluate(y_test, y_pred)\n",
        "\n",
        "# Evaluation - training set\n",
        "evaluate(y_train, pipe.predict(X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2I-HjNh5aTF"
      },
      "source": [
        "Of course, combining the three above-mentioned techniques should give the best result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io3YM1jV1YN3"
      },
      "source": [
        "#### BONUS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9D5xSc5_y9i"
      },
      "source": [
        "# BONUS: predict the rating\n",
        "df.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiyyP7T6Bykf"
      },
      "source": [
        "df.rating.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNNejlJgB2aY"
      },
      "source": [
        "# Base rate\n",
        "round(df.rating.value_counts()[5] / len(df), 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZE1JqorB9Lb"
      },
      "source": [
        "# Select features\n",
        "X = df['verified_reviews'] # the features we want to analyze\n",
        "y = df['rating'] # the labels, or answers, we want to test against\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Define classifier\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Generate Model on training set\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation - test set\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfE1iXi0Eeuz"
      },
      "source": [
        "# BONUS 2: use random forest\n",
        "\n",
        "# Define classifier\n",
        "classifier = RandomForestClassifier(n_estimators=50)\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Generate Model on training set\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation - test set\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFt-MD98E1E4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}