{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Assignements/Part%204/Assignment_4_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZu-7QbP9muh"
      },
      "source": [
        "### DSML investigation\n",
        "\n",
        "You are part of the Suisse Impossible Mission Force, or SIMF for short. You need to uncover a rogue agent that is trying to steal sensitive information.\n",
        "\n",
        "Your mission, should you choose to accept it, is to find that agent before stealing any classified information. Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyL7WNdV9sWV"
      },
      "source": [
        "# Assignement part four\n",
        "\n",
        "#### Identifying the suspects' credit score\n",
        "We received informations that the rogue agent has a *good* credit score.\n",
        "\n",
        "Our spies at SIMF have managed to collect financial information relating to our suspects as well as a training dataset.\n",
        "\n",
        "Create a Neural Network over the training dataset `df` to identify which of the suspects have a *standard* `Credit_Mix`.\n",
        "\n",
        "\n",
        "## Getting to know our data\n",
        "\n",
        "* `Age`: a user's age\n",
        "\n",
        "* `Occupation`: a user's employment field\n",
        "\n",
        "* `Annual_Income`: a user's annual income\n",
        "\n",
        "* `Monthly_Inh_Salary`: the calculated salary received by a given user on a monthly basis\n",
        "\n",
        "* `Num_Bank_Accounts`: the number of bank accounts possessed by a given user\n",
        "\n",
        "* `Num_Credit_Cards`: the number of credit cards a given user possesses\n",
        "\n",
        "* `Interest_Rate`: The interest rate on those cards (if multiple then it's the average)\n",
        "\n",
        "* `Num_of_Loans`: The number of loans of each user\n",
        "\n",
        "* `Delay_from_due_date`: payment tardiness of user\n",
        "\n",
        "* `Num_of_Delayed_Payment`: the count of delayed payments\n",
        "\n",
        "* `Changed_Credit_Limit`: NaN\n",
        "\n",
        "* `Num_Credit_Inquiries`: NaN\n",
        "\n",
        "* `Credit_Mix`: The user's credit score\n",
        "\n",
        "* `Outsting_Debt`: Outstanding debt\n",
        "\n",
        "* `Credit_Utilization_Ratio`: the percentage of borrowed money over borrowing allowance\n",
        "\n",
        "* `Payment_of_Min_Amount`: does the user usually pay the minimal amount (categorical)\n",
        "\n",
        "* `Total_EMI_per_month`: Monthly repayments to be made\n",
        "\n",
        "* `Amount_invested_monthly`: The amount put in an investment fund by the user on a monthly basis\n",
        "\n",
        "* `Payment_Behaviour`: the user's payment behavior (categorical)\n",
        "\n",
        "* `Monthly_Balance`: The user's end of the month balance\n",
        "\n",
        "* `AutoLoan`: If the user has an active loan for their vehicle\n",
        "\n",
        "* `Credit-BuilderLoan`: If the user has a loan to increase their credit score\n",
        "\n",
        "* `DebtConsolidationLoan`, `HomeEquityLoan`, `MortgageLoan`, `NotSpecified`, `PaydayLoan`, `PersonalLoan`, `StudentLoan`: different types of loans (categorical features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHhI95r5-tyD"
      },
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXm79UyPF1Bz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%204/data/train_classification.csv\", index_col='Unnamed: 0').dropna()\n",
        "suspects = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%204/data/suspects.csv\", index_col='Unnamed: 0').dropna()\n",
        "suspects.rename(columns={\"Payment_Behaviour\": \"le_Payment_Behaviour\", \"Payment_of_Min_Amount\": \"le_Payment_of_Min_Amount\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHi25uhvF1Bz"
      },
      "outputs": [],
      "source": [
        "display(df.head())\n",
        "print(df.shape)\n",
        "display(suspects.head())\n",
        "print(suspects.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpZT59dqF1B0"
      },
      "outputs": [],
      "source": [
        "df[\"Credit_Mix\"].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZENObtyefVk"
      },
      "source": [
        "# 1. Preparing the data\n",
        "## 1.1 Data cleaning\n",
        "Consider the dataset loaded into the DataFrame `df` which is *train_classification.csv*. We aim to preprocess this data for model training.\n",
        "\n",
        "Begin by encoding the categorical variables:\n",
        "- Apply One-Hot Encoding to `Occupation`\n",
        "- Apply Label Encoding to `Payment_of_Min_Amount` and `Payment_Behaviour`\n",
        "\n",
        "*Note: To clearly distinguish your newly encoded columns, especially for label encoding, consider renaming them with a prefix. Please, use `le_Payment_of_Min_Amount` and `le_Payment_Behaviour` for the label-encoded new columns.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGVrLNJTefVk"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyXUT-2ePq_0"
      },
      "source": [
        " After encoding, integrate the newly encoded columns back into a new DataFrame named `df_encoded`, and remove the original columns `Occupation`, `Payment_of_Min_Amount`, and `Payment_Behaviour` to avoid redundancy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7EsAt2EPq_0"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvdWdSY4Pq_0"
      },
      "source": [
        "Finally, display the first few rows of `df_encoded` to verify that the encodings are correctly implemented. This prepared DataFrame will be used for subsequent model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loEO-PPBPq_0"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfPnhUxAefVl"
      },
      "source": [
        "## 1.2 Dataset splitting and rescaling\n",
        "\n",
        "To effectively train and validate our model, it is crucial to properly prepare and partition the data. Follow these steps to preprocess and split the dataframe df_encoded into training and test subsets, ensuring that our model can generalize well to new data:\n",
        "\n",
        "- Set `X` as all columns except `Credit_Mix` and `y` as the dependent feature `Credit_Mix`.\n",
        "- Apply manually Label Encoding to `y` using the function `.map`and encoding with the following : Good=2, Standard=1, Bad=0.\n",
        "- Use a `random_state` of 42 to split `X` and the encoded `y` into training (80%) and test sets (20%).\n",
        "- Normalize `X` using `MinMaxScaler()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-mentarefVm"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmjXsvhKF1B3"
      },
      "source": [
        "### 1.2.2 Final touches\n",
        "Convert the features to torch tensors of type `torch.float` and the labels (dependent variables) to torch tensors of type `torch.long`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KS_U8stefVo"
      },
      "outputs": [],
      "source": [
        "#Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpJNiQADF1B3"
      },
      "source": [
        "# 2 Model preparation:\n",
        "\n",
        "## 2.1 Define a Neural network model and instantiate it.\n",
        "Define your neural network model as a class in PyTorch, extending from nn.Module. In the `__init__` method, initialize a linear layer using `nn.Linear()` with specified input and output sizes, and set up `nn.ReLU()` for activation. Implement the forward method to describe how data passes through this layer during the network's forward computation.\n",
        "\n",
        "Set the following parameters:\n",
        "* `hidden layer` : 1\n",
        "* `activation function` : ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx-3yvp5efVp"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQj60yL2Pq_2"
      },
      "source": [
        "Set `D_in` to the number of features in `X_train` and `D_out` to the number of target variables in `y_train`, then print these dimensions to verify their values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko7o5qU5Pq_2"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31MZTFSZPq_2"
      },
      "source": [
        "\n",
        "Initialize the `Net` model with the specified input size `D_in`, 150 hidden units, and output size `D_out`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYv596CPq_2"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REh5Skj1Pq_2"
      },
      "source": [
        "Let's calculate now how many parameters we have in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7ey7ekkPq_2"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Upc_PIcPq_2"
      },
      "source": [
        "**Q1. How many parameters does your model have ?**\n",
        "\n",
        "*Note: Enter an integer (e.g. 355)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bnfvHbEF1B4"
      },
      "source": [
        "## 2.2 Finding the best model\n",
        "\n",
        "Determine the optimal hyper-parameters for your model from the options listed below:\n",
        "\n",
        "* `criterion` : [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "* `optimizer` : [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
        "* `Epochs`: Test with **150**, **250**, **500**, and **1000** epochs.\n",
        "* `Learning Rate`: Experiment with learning rates of **0.00005**, **0.001**, **1**, and **10**.\n",
        "\n",
        "**Evaluation**: Assess your model's performance by measuring its accuracy on the test set.\n",
        "\n",
        "*Note: Run the code `torch.manual_seed(42)` to ensure consistency across all experiments.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAV15_3y9tyk"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)   # Set the seed for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfMlBIJ99tym"
      },
      "source": [
        "### 2.2.1 Automatically Tuning Hyperparameters\n",
        "\n",
        "In this section, you will automate the process of testing different hyperparameter combinations using a loop, as demonstrated in the lab.\n",
        "\n",
        "Begin by defining the ranges for the hyperparameters you want to explore:\n",
        "- **Epochs**: Test with **150**, **250**, **500**, and **1000** epochs.\n",
        "- **Learning Rate**: Experiment with learning rates of **0.00005**, **0.001**, **1**, and **10**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA78kwej9tym"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cbW9jFb9tyr"
      },
      "source": [
        "Next, create a loop that iterates over the list of learning rates, with an inner loop iterating over the list of number of epochs. Inside the inner loop, initialize (define) the model, and also define the optimizer and the loss function (criterion). Then train the model as demonstrated in the lab, and after the training evaluate its performance to obtain the test accuracy for each model. Ensure that you also display the test loss, as you will need it to answer the upcoming questions.\n",
        "\n",
        "Hereâ€™s a reminder of the criterion and optimizer you should use:\n",
        "- **Criterion**: [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "- **Optimizer**: [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
        "\n",
        "**Hint**: Make sure to define the criterion and optimizer at every iteration (inside the inner loop), otherwise the model might retain previous training states and produce biased results.\n",
        "\n",
        "(Optional) You can enhance the code by storing the best model along with its best_accuracy and best_params. This way, you'll have an automatic evaluation at the end to identify the best-performing model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQcxZtu09tyr"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6vOCFqx9tyr"
      },
      "source": [
        "### 2.2.3 Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyYej4T9Pq_3"
      },
      "source": [
        "**Q2. What is the test accuracy when we train the model with a learning rate of 0.001 and for 150 epochs? Round your answer to 2 decimal points, e.g., 0.25.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haESt1qnPq_4"
      },
      "source": [
        "**Q3. When using 1000 epochs, which learning rate results in the highest test accuracy?**\n",
        "\n",
        "*Note: Select among the following answers*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVn0_N37Lg2c"
      },
      "source": [
        "\n",
        "**Q4. Is BCELoss a suitable alternative to CrossEntropyLoss for our dataset?**\n",
        "\n",
        "*Hint: Consider the unique values in the Credit_Mix output variable when answering.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMa1WScWF1B8"
      },
      "source": [
        "### 3. Predict on the Suspects Dataset\n",
        "\n",
        "Now it's time to use your trained model to make predictions on the suspects dataset!\n",
        "\n",
        "Use the following parameters for the model:\n",
        "- **Hidden layer**: 1 hidden layer with 150 neurons\n",
        "- **Output layer**: 3 neurons for classification\n",
        "- **Optimizer**: [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
        "- **Criterion**: [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "- **Iterations**: 1000\n",
        "- **Learning rate**: 1.0\n",
        "\n",
        "Ensure consistency by setting the manual seed with `torch.manual_seed(42)` before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o88t-Juc9tys"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q1MRR169tys"
      },
      "source": [
        "\n",
        "Now, train your model on the training dataset just as you did in section 2.2.1, but you don't have to loop over all the hyper parameters. Ensure that you use the correct number of epochs specified earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hFdANGDPq_5"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvRkS6dOPq_5"
      },
      "source": [
        "Before making predictions, confirm that the feature column names in the `suspects` dataset match those expected by the model, particularly ensuring the X features correspond accurately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z4f2gcfPq_5"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beIK34osPq_6"
      },
      "source": [
        "Then scale your dataset and convert it into a torch tensor of dtype float, similar to the preprocessing done for the training set in section 1.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeIKEtZ8JKS9"
      },
      "outputs": [],
      "source": [
        "#Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3yxBxiiPq_6"
      },
      "source": [
        "Make predictions using the trained model and assign the predicted credit score to each user. Ensure to do the following encoding is used for the predicted categories:\n",
        "* `0` corresponds to bad credit score,\n",
        "* `1` corresponds to standard credit score,\n",
        "* `2` corresponds to good credit score.\n",
        "\n",
        "Use the predictions to add a new column `credit_score` in the `suspects` dataframe that maps the predicted numerical values to the respective credit score categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98VH2TJUPq_6"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxuCejWGPq_6"
      },
      "source": [
        "\n",
        "\n",
        "As mentioned earlier, we believe the suspect had a \"good\" credit score. Review the predictions made by the model, then display how many suspects were categorized under each credit score, and extract the `userID`s of those with a \"standard\" credit score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSL2d1K-KeKW"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK0J3mmALxUo"
      },
      "source": [
        "**Q5.Which of the following suspects have a \"good\" credit mix according to your model's predictions?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKadkpnabxi1"
      },
      "source": [
        "## Your investigation is progressing effectively, and the list of suspects is narrowing down.\n",
        "\n",
        "**Don't forget to answer the quiz and submit your code on Moodle before the end of the deadline.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}